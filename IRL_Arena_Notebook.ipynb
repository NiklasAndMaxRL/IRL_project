{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b95ef11a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "from typing import Any, List, Callable\n",
    "\n",
    "from GridWorld_environments import Grid_World\n",
    "from RL_agents import ValueIterationAgent, QLearningAgent\n",
    "from IRL_agents import IRL_from_sampled_trajectories\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812e149d",
   "metadata": {},
   "source": [
    "#### Train Value Iteration function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c7fa1e9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def train_value_iteration(gw_env: Grid_World, verbose=False):\n",
    "    vi_agent = ValueIterationAgent(states=gw_env.get_state_space(),\n",
    "                                   terminal_states=gw_env.get_terminal_states(),\n",
    "                                   reward_function=gw_env.get_reward_func(),\n",
    "                                   actions=gw_env.get_action_space(),\n",
    "                                   gamma=GAMMA)\n",
    "\n",
    "    iters = 0\n",
    "    while iters < VALUE_ITERATION_TRAINING_N and not vi_agent.converged:\n",
    "\n",
    "        for state in gw_env.get_state_space():\n",
    "\n",
    "            if state in gw_env.get_terminal_states():\n",
    "                continue\n",
    "\n",
    "            opt_act = vi_agent.get_optimal_action(action_state_pairs=gw_env.get_action_state_pairs(state=state))\n",
    "            next_state = gw_env.get_new_state_on_action(old_state=state, action=opt_act)\n",
    "            next_state_value = vi_agent.get_state_value(state=next_state)\n",
    "\n",
    "            vi_agent.set_state_value(state=state, new_value=(gw_env.get_state_reward(state=next_state) + GAMMA * next_state_value))\n",
    "\n",
    "        iters += 1\n",
    "\n",
    "    if verbose:\n",
    "        gw_env.display_value_function(value_func=vi_agent.get_value_function())\n",
    "\n",
    "    vi_agent.construct_policy(gw_env.get_action_state_pairs)\n",
    "\n",
    "    if verbose:\n",
    "        gw_env.display_policy(policy=vi_agent.get_policy())\n",
    "\n",
    "    return vi_agent.get_policy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c569f0",
   "metadata": {},
   "source": [
    "#### Train Q Learning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cbbc175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_q_learning(gw_env: Grid_World, n_episodes=1000, verbose=False, policy=\"eps_greedy\", eps=0.2, max_episode_len=100, gamma=0.95):\n",
    "    ql_agent = QLearningAgent(states=gw_env.get_state_space(),\n",
    "                              terminal_states=gw_env.get_terminal_states(),\n",
    "                              reward_function=gw_env.get_reward_func(),\n",
    "                              actions=gw_env.get_action_space(),\n",
    "                              gamma=gamma)\n",
    "    \n",
    "    # init episodes\n",
    "    episodes = []\n",
    "    \n",
    "    # Define state_space without terminal states for getting starting position\n",
    "    state_space = deepcopy(gw_env.get_state_space()) # all states\n",
    "    terminal_states = gw_env.get_terminal_states()\n",
    "    for terminal_state in terminal_states:\n",
    "        state_space.remove(terminal_state) # not non absorbing state_space\n",
    "    \n",
    "    for n in range(n_episodes):\n",
    "        \n",
    "        episode = []\n",
    "        \n",
    "        # random starting position\n",
    "        start_idx = (np.random.choice(len(state_space)))\n",
    "        start = state_space[start_idx]\n",
    "        \n",
    "        episode.append(start)\n",
    "        \n",
    "        i = 0\n",
    "        terminal = False\n",
    "        \n",
    "        while ( ( i < max_episode_len ) and ( not terminal ) ):\n",
    "            i += 1\n",
    "            \n",
    "            # Choose Action from S derived by given policy\n",
    "            if policy == \"eps_greedy\":\n",
    "                if np.random.uniform() < (1-eps):\n",
    "                    # Choose greedy action -> highest Q-Value\n",
    "                    chosen_action = ql_agent.get_greedy_action(episode[-1])\n",
    "                else:\n",
    "                    # Choose random action form action space\n",
    "                    action_space = gw_env.get_action_space()\n",
    "                    chosen_action = action_space[np.random.choice(len(action_space))]\n",
    "            \n",
    "            new_state = gw_env.get_new_state_on_action(episode[-1], chosen_action)\n",
    "            \n",
    "            # Reward is taken from Q_learning agent -> it knows the reward function from the environment\n",
    "            ql_agent.update_Q_value(episode[-1], new_state, chosen_action)\n",
    "            \n",
    "            episode.append(new_state)\n",
    "            \n",
    "            if new_state in terminal_states:\n",
    "                terminal = True\n",
    "        \n",
    "        episodes.append(episode)\n",
    "    \n",
    "    if verbose:\n",
    "        gw_env.display_q_function(q_func=ql_agent.get_Q_function())\n",
    "\n",
    "    ql_agent.construct_policy(gw_env.get_action_state_pairs)\n",
    "\n",
    "    if verbose:\n",
    "        gw_env.display_policy(policy=ql_agent.get_policy())\n",
    "\n",
    "    return ql_agent.get_policy()         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d336096",
   "metadata": {},
   "source": [
    "#### IRL Reward estimation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "793e4fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def irl_reward_estimation(env: Grid_World, optimal_trajectories: List[List[Any]], train_func: Callable):\n",
    "\n",
    "    # prepare reference reward function\n",
    "    reward_func_ref = deepcopy(env.get_board())\n",
    "    reward_func_preds = []\n",
    "    print('reward_func_ref\\n', reward_func_ref)\n",
    "\n",
    "    minmax_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "    reward_func_ref_shape = reward_func_ref.shape\n",
    "    reward_func_ref = minmax_scaler.fit_transform(reward_func_ref.reshape(-1, 1)).reshape(reward_func_ref_shape)\n",
    "    print('MinMax scaled reward_func_ref:\\n', reward_func_ref)\n",
    "\n",
    "    irl_agent = IRL_from_sampled_trajectories(d=(GW_SIZE[0] * 4, GW_SIZE[1] * 4),\n",
    "                                              env_ranges=((0, GW_SIZE[0]), (0, GW_SIZE[1])),\n",
    "                                              env_discrete_size=GW_SIZE,\n",
    "                                              penalty_factor=2,\n",
    "                                              gamma=GAMMA)\n",
    "\n",
    "    # step 2: given optimal trajectories, compute the value estimate\n",
    "    print(\"Computing value estimates for optimal trajectories...\")\n",
    "    optimal_value_estimate = irl_agent.compute_value_estimate(trajs=optimal_trajectories)\n",
    "\n",
    "    candidate_policies = [env.construct_random_policy()]\n",
    "    candidate_value_estimates = []\n",
    "\n",
    "    # while True:\n",
    "    for i in range(IRL_TRAINING_N):\n",
    "        print(f\"Iteration {i}...\")\n",
    "\n",
    "        # step 3: generate trajectories and compute the value estimate for a random policy\n",
    "        print(\"Generating trajectories for the candidate policy...\")\n",
    "        candidate_trajectories = env.generate_trajectories(policy=candidate_policies[-1],\n",
    "                                                           n_traj=NUMBER_OF_TRAJECTORIES,\n",
    "                                                           max_traj_length=MAXIMUM_TRAJECTORY_LENGTH)\n",
    "        print(\"Computing value estimates for condidate trajectories...\")\n",
    "        candidate_value_estimates.append(irl_agent.compute_value_estimate(trajs=candidate_trajectories))\n",
    "\n",
    "        # step 4: obtain new alphas\n",
    "        print(\"Solving linear programming...\")\n",
    "        irl_agent.solve_lp(optimal_value_estimate, candidate_value_estimates)\n",
    "\n",
    "        # step 5: construct new reward function from the alphas\n",
    "        reward_func = irl_agent.construct_reward_function(alphas=irl_agent.get_alphas())\n",
    "\n",
    "        # step 6: find optimal policy under new reward function and add to 'candidate_policies' list\n",
    "        env.set_reward_func(reward_func)\n",
    "        print(\"Latest non-Scaled reward func:\\n\", env.get_board())\n",
    "        scaled_board = minmax_scaler.fit_transform(env.get_board().reshape(-1, 1)).reshape(env.get_board().shape)\n",
    "        env.set_board(new_board=scaled_board)\n",
    "\n",
    "        reward_func_preds.append(env.get_board())\n",
    "\n",
    "        candidate_policies.append(train_func(gw_env=env, verbose=True))  # train_value_iteration(gw_env=env))\n",
    "\n",
    "        print(\"Latest Scaled reward func:\\n\", reward_func_preds[-1])\n",
    "        env.display_policy(policy=candidate_policies[-1])\n",
    "        print(\"============================================================\\n\" * 2)\n",
    "\n",
    "    return {'reference_reward_func': reward_func_ref, 'policy_pred': np.mean(np.array([ np_normalize(list(pol.values()), 1) for pol in candidate_policies ]), axis=0), 'avg_predicted_reward_func': np.mean(np.array(reward_func_preds), axis=0)}\n",
    "\n",
    "\n",
    "def calc_value_distance(value_estimates_ref, value_estimates_pred):\n",
    "    return np.linalg.norm(np.array(value_estimates_ref) - np.array(value_estimates_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8ea0f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.95\n",
    "VALUE_ITERATION_TRAINING_N = 25\n",
    "IRL_TRAINING_N = 10\n",
    "\n",
    "NUMBER_OF_TRAJECTORIES = 400\n",
    "MAXIMUM_TRAJECTORY_LENGTH = 40\n",
    "\n",
    "GW_SIZE = (5, 5)\n",
    "GW_SIZES = [(5, 5)]  # [(x, x) for x in np.arange(5,11, 5)]\n",
    "GW_TRAPS = [(2,2)]\n",
    "GW_GOALS = [(3, 4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46913987",
   "metadata": {},
   "source": [
    "### Step-by-step code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90070f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = Grid_World(size=GW_SIZE, traps=GW_TRAPS, goals=GW_GOALS, randomize_board=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "176c8196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function:\n",
      "[[0.73509189 0.77378094 0.81450625 0.857375   0.9025    ]\n",
      " [0.77378094 0.81450625 0.857375   0.9025     0.95      ]\n",
      " [0.81450625 0.857375   0.         0.95       1.        ]\n",
      " [0.857375   0.9025     0.95       1.         0.        ]\n",
      " [0.81450625 0.857375   0.9025     0.95       1.        ]]\n",
      "Policy:\n",
      "[['v' 'v' 'v' 'v' 'v']\n",
      " ['v' 'v' '>' 'v' 'v']\n",
      " ['v' 'v' 'x' 'v' 'v']\n",
      " ['>' '>' '>' '>' 'x']\n",
      " ['>' '>' '>' '>' '^']]\n"
     ]
    }
   ],
   "source": [
    "vi_greedy_policy = train_value_iteration(gw_env=environment, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "585078bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q function:\n",
      "[[[ 0.73509189  0.73509189  0.6983373   0.6983373 ]\n",
      "  [ 0.77378094  0.77378094  0.73509189  0.6983373 ]\n",
      "  [ 0.81450625  0.81450625  0.77378094  0.73509189]\n",
      "  [ 0.857375    0.857375    0.81450625  0.77378094]\n",
      "  [ 0.9025      0.857375    0.51987381  0.81450625]]\n",
      "\n",
      " [[ 0.77378094  0.77378094  0.6983373   0.73509189]\n",
      "  [ 0.81450625  0.81450625  0.73509189  0.73509189]\n",
      "  [-1.          0.857375    0.77378094  0.77378094]\n",
      "  [ 0.9025      0.9025      0.81450625  0.81450625]\n",
      "  [ 0.95        0.9025      0.857375    0.857375  ]]\n",
      "\n",
      " [[ 0.81450625  0.81450625  0.73509189  0.77378094]\n",
      "  [ 0.857375   -1.          0.77378094  0.77378094]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.95        0.95        0.857375   -1.        ]\n",
      "  [ 1.          0.95        0.9025      0.9025    ]]\n",
      "\n",
      " [[ 0.77378094  0.857375    0.77378094  0.81450625]\n",
      "  [ 0.81450625  0.9025      0.81450625  0.81450625]\n",
      "  [ 0.857375    0.95       -1.          0.857375  ]\n",
      "  [ 0.9025      1.          0.9025      0.9025    ]\n",
      "  [ 0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.77378094  0.81450625  0.73509189  0.77378094]\n",
      "  [ 0.81450625  0.857375    0.857375    0.77378094]\n",
      "  [ 0.857375    0.9025      0.9025      0.81450625]\n",
      "  [ 0.9025      0.95        0.95        0.857375  ]\n",
      "  [ 0.95        0.95        1.          0.9025    ]]]\n",
      "Policy:\n",
      "[['v' 'v' 'v' 'v' 'v']\n",
      " ['v' 'v' '>' 'v' 'v']\n",
      " ['v' 'v' 'x' 'v' 'v']\n",
      " ['>' '>' '>' '>' 'x']\n",
      " ['>' '>' '>' '>' '^']]\n"
     ]
    }
   ],
   "source": [
    "ql_greedy_policy = train_q_learning(gw_env=environment, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8810805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 400 trajectories...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Generating {NUMBER_OF_TRAJECTORIES} trajectories...\")\n",
    "\n",
    "greedy_policy = vi_greedy_policy\n",
    "# greedy_policy = ql_greedy_policy\n",
    "\n",
    "trajectories = environment.generate_trajectories(policy=greedy_policy,\n",
    "                                                 n_traj=NUMBER_OF_TRAJECTORIES,\n",
    "                                                 max_traj_length=MAXIMUM_TRAJECTORY_LENGTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e877c596",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IRL from samples...\n",
      "reward_func_ref\n",
      " [[ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "MinMax scaled reward_func_ref:\n",
      " [[ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "Computing value estimates for optimal trajectories...\n",
      "Iteration 0...\n",
      "Generating trajectories for the candidate policy...\n",
      "Computing value estimates for condidate trajectories...\n"
     ]
    }
   ],
   "source": [
    "print(\"IRL from samples...\")\n",
    "\n",
    "# restart the environment\n",
    "environment = Grid_World(size=GW_SIZE, traps=GW_TRAPS, goals=GW_GOALS, randomize_board=False)\n",
    "\n",
    "train_func = train_value_iteration\n",
    "# train_func = train_q_learning\n",
    "\n",
    "estimated_rewards = irl_reward_estimation(env=environment, optimal_trajectories=trajectories, train_func=train_func)\n",
    "ref_reward_funcs.append(estimated_rewards['reference_reward_func'])\n",
    "avg_pred_reward_funcs.append(estimated_rewards['avg_predicted_reward_func'])\n",
    "# Using default value for reward loss -> Frobenius for matrices and L2-loss for vectors\n",
    "reward_loss.append(np.linalg.norm(estimated_rewards['reference_reward_func'] - estimated_rewards['avg_predicted_reward_func']))\n",
    "# Using L1-Loss for policy loss as described by Ng and Russel in 2000\n",
    "policy_loss.append(np.linalg.norm(estimated_rewards['policy_pred'] - np.array(list(greedy_policy.values())), ord=1 ))\n",
    "\n",
    "print('**********************************************')\n",
    "print('*****************REWARD LOSS******************')\n",
    "print(reward_loss)\n",
    "print('**********************************************')\n",
    "print('*****************POLICY LOSS*************************')\n",
    "print(policy_loss)\n",
    "print('**********************************************')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed02e3e",
   "metadata": {},
   "source": [
    "### End-to-end loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0042f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configuration is:\n",
      "training: value iteration\n",
      "generating trajectories\n",
      "inverse reinforcment learning\n",
      "\n",
      "Training via value iteration...\n",
      "Value function:\n",
      "[[0.6983373  0.73509189 0.77378094 0.81450625 0.857375   0.9025    ]\n",
      " [0.73509189 0.77378094 0.         0.857375   0.9025     0.95      ]\n",
      " [0.77378094 0.81450625 0.857375   0.9025     0.95       1.        ]\n",
      " [0.81450625 0.857375   0.9025     0.95       1.         0.        ]]\n",
      "Policy:\n",
      "[['v' 'v' '>' 'v' 'v' 'v']\n",
      " ['v' 'v' 'x' 'v' 'v' 'v']\n",
      " ['v' 'v' 'v' 'v' 'v' 'v']\n",
      " ['>' '>' '>' '>' '>' 'x']]\n",
      "Generating 40 trajectories...\n",
      "IRL from samples...\n",
      "reward_func_ref\n",
      " [[ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.]]\n",
      "MinMax scaled reward_func_ref:\n",
      " [[ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.]]\n",
      "Iteration 0...\n",
      "Latest non-Scaled reward func:\n",
      " [[-3.98524428 -5.21657998 -6.05985019 -6.30720925 -5.89911348 -4.93425745]\n",
      " [-4.91908932 -6.43895859 -7.47982866 -7.78515029 -7.28142721 -6.09048064]\n",
      " [-5.2084129  -6.81767555 -7.91976594 -8.24304552 -7.70969522 -6.44870135]\n",
      " [-4.73488853 -6.1978446  -7.19973812 -7.49362664 -7.00876604 -5.8624158 ]]\n",
      "Value function:\n",
      "[[15.28210306 15.51799791 15.1637076  14.43102586 13.61878722 13.03885371]\n",
      " [15.51799791 15.3034467   0.         13.61878722 13.03885371 12.94113485]\n",
      " [15.3034467  14.96372025 13.88506759 12.34266704 11.83860816 12.30519382]\n",
      " [14.96372025 14.86340683 14.08092057 12.86694316 11.57561755  0.        ]]\n",
      "Policy:\n",
      "[['<' '<' '<' '<' '<' '<']\n",
      " ['^' '<' 'x' '^' '^' '^']\n",
      " ['^' '<' '<' '<' '^' '^']\n",
      " ['^' '<' '<' '<' '<' 'x']]\n",
      "Latest Scaled reward func:\n",
      " [[ 1.          0.42160959  0.02550364 -0.09068735  0.10100585  0.55422383]\n",
      " [ 0.56134869 -0.15257344 -0.64149719 -0.7849147  -0.54830287  0.01111572]\n",
      " [ 0.42544588 -0.33046665 -0.84814717 -1.         -0.74947149 -0.15714987]\n",
      " [ 0.6478726  -0.03931592 -0.50993138 -0.64797846 -0.42022682  0.11824371]]\n",
      "Policy:\n",
      "[['<' '<' '<' '<' '<' '<']\n",
      " ['^' '<' 'x' '^' '^' '^']\n",
      " ['^' '<' '<' '<' '^' '^']\n",
      " ['^' '<' '<' '<' '<' 'x']]\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "Iteration 1...\n",
      "Latest non-Scaled reward func:\n",
      " [[-3.97697504 -5.19443372 -6.0111669  -6.21937025 -5.76903551 -4.77616629]\n",
      " [-4.90385301 -6.3981925  -7.39029724 -7.62375402 -7.04262684 -5.80049322]\n",
      " [-5.18539328 -6.75614252 -7.78474922 -7.99986906 -7.35020021 -6.01250561]\n",
      " [-4.70637122 -6.12168636 -7.0327807  -7.1931839  -6.56498543 -5.32438624]]\n",
      "Value function:\n",
      "[[15.20321202 15.44305142 15.06563375 14.30104436 13.4711752  12.90668545]\n",
      " [15.44305142 15.21009727  0.         13.4711752  12.90668545 12.86402963]\n",
      " [15.21009727 14.8488218  13.724705   12.14541755 11.73724849 12.31425782]\n",
      " [14.8488218  14.74375809 13.94031723 12.72409374 11.48893624  0.        ]]\n",
      "Policy:\n",
      "[['<' '<' '<' '<' '<' '<']\n",
      " ['^' '<' 'x' '^' '^' '^']\n",
      " ['^' '<' '<' '<' '^' '^']\n",
      " ['^' '<' '<' '<' '<' 'x']]\n",
      "Latest Scaled reward func:\n",
      " [[ 1.          0.3947349  -0.0113077  -0.11481694  0.10906901  0.60267845]\n",
      " [ 0.53919842 -0.20371924 -0.69694861 -0.8130127  -0.52410269  0.09342967]\n",
      " [ 0.3992294  -0.38167571 -0.8930522  -1.         -0.67701419 -0.01197325]\n",
      " [ 0.63737738 -0.06625296 -0.51920764 -0.59895281 -0.2866411   0.33012841]]\n",
      "Policy:\n",
      "[['<' '<' '<' '<' '<' '<']\n",
      " ['^' '<' 'x' '^' '^' '^']\n",
      " ['^' '<' '<' '<' '^' '^']\n",
      " ['^' '<' '<' '<' '<' 'x']]\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "Iteration 2...\n",
      "Latest non-Scaled reward func:\n",
      " [[-3.92458397 -5.06668451 -5.75341484 -5.78913326 -5.17508791 -4.09828064]\n",
      " [-4.81525123 -6.18397821 -6.96175267 -6.91442678 -6.07141491 -4.70080977]\n",
      " [-5.06157758 -6.45925141 -7.19574037 -7.03303322 -6.03725943 -4.53779172]\n",
      " [-4.56346305 -5.78172301 -6.36376131 -6.10395577 -5.0978564  -3.68969373]]\n",
      "Value function:\n",
      "[[12.84878203 13.07235142 12.63323901 11.82434166 11.79345669 11.79345669]\n",
      " [13.07235142 12.77666738  0.         11.03551385 11.62699972 11.97070829]\n",
      " [12.77666738 12.35525238 11.15761478 10.48464307 11.56185812 11.79538874]\n",
      " [12.35525238 12.2390541  11.4337178  10.3366286  10.64461263  0.        ]]\n",
      "Policy:\n",
      "[['<' '<' '<' '<' '>' '>']\n",
      " ['^' '<' 'x' '^' '>' '^']\n",
      " ['^' '<' '<' '>' '>' '^']\n",
      " ['^' '<' '<' '<' '^' 'x']]\n",
      "Latest Scaled reward func:\n",
      " [[ 0.86600849  0.21450516 -0.1772354  -0.19761072  0.15266719  0.76692443]\n",
      " [ 0.35793352 -0.42284729 -0.86652334 -0.83952662 -0.35863634  0.42321586]\n",
      " [ 0.21741837 -0.57987498 -1.         -0.90718483 -0.33915258  0.51620838]\n",
      " [ 0.50156435 -0.1933836  -0.52540331 -0.37719904  0.19672336  1.        ]]\n",
      "Policy:\n",
      "[['<' '<' '<' '<' '>' '>']\n",
      " ['^' '<' 'x' '^' '>' '^']\n",
      " ['^' '<' '<' '>' '>' '^']\n",
      " ['^' '<' '<' '<' '^' 'x']]\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "Iteration 3...\n",
      "Latest non-Scaled reward func:\n",
      " [[-3.96830006 -5.17516579 -5.97621058 -6.16762922 -5.70664658 -4.71500534]\n",
      " [-4.88736618 -6.36153219 -7.32368193 -7.52493919 -6.92312814 -5.68288424]\n",
      " [-5.15974563 -6.69905673 -7.68087647 -7.8454971  -7.16303811 -5.82766904]\n",
      " [-4.67371066 -6.04893138 -6.90023954 -6.99588249 -6.32524127 -5.08690902]]\n",
      "Value function:\n",
      "[[15.15711617 15.39926037 15.00675188 14.22066062 13.3751332  12.80967384]\n",
      " [15.39926037 15.15520944  0.         13.3751332  12.80967384 12.78401226]\n",
      " [15.15520944 14.78285778 13.63509075 12.03825356 11.64498178 12.26036644]\n",
      " [14.78285778 14.67983832 13.87258069 12.66655007 11.47148485  0.        ]]\n",
      "Policy:\n",
      "[['<' '<' '<' '<' '<' '<']\n",
      " ['^' '<' 'x' '^' '^' '^']\n",
      " ['^' '<' '<' '<' '>' '^']\n",
      " ['^' '<' '<' '<' '<' 'x']]\n",
      "Latest Scaled reward func:\n",
      " [[ 1.          0.37745453 -0.03575366 -0.1344944   0.1032973   0.61482211]\n",
      " [ 0.52591209 -0.23451664 -0.73082866 -0.83464451 -0.52420836  0.11555479]\n",
      " [ 0.38540881 -0.40862414 -0.91508266 -1.         -0.6479627   0.0408695 ]\n",
      " [ 0.63612342 -0.07326571 -0.51240159 -0.56173772 -0.21579645  0.4229806 ]]\n",
      "Policy:\n",
      "[['<' '<' '<' '<' '<' '<']\n",
      " ['^' '<' 'x' '^' '^' '^']\n",
      " ['^' '<' '<' '<' '>' '^']\n",
      " ['^' '<' '<' '<' '<' 'x']]\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "Iteration 4...\n",
      "Latest non-Scaled reward func:\n",
      " [[-3.94914228 -5.12999245 -5.88792741 -6.02464501 -5.51478447 -4.50180492]\n",
      " [-4.8534198  -6.28189038 -7.16880708 -7.27530655 -6.58969558 -5.31396535]\n",
      " [-5.11037774 -6.58382014 -7.45790589 -7.48786294 -6.68759166 -5.30395822]\n",
      " [-4.61478547 -5.91208345 -6.63679355 -6.57542751 -5.76896294 -4.47695862]]\n",
      "Value function:\n",
      "[[15.03965345 15.28767077 14.85589905 14.01734927 13.14345746 12.6014212 ]\n",
      " [15.28767077 15.01221128  0.         13.14345746 12.6014212  12.65899847]\n",
      " [15.01221128 14.6052983  13.38597669 11.73360886 11.70303993 12.2546833 ]\n",
      " [14.6052983  14.49882783 13.66447919 12.46225923 11.35483288  0.        ]]\n",
      "Policy:\n",
      "[['<' '<' '<' '<' '<' '>']\n",
      " ['^' '<' 'x' '^' '^' '^']\n",
      " ['^' '<' '<' '<' '>' '^']\n",
      " ['^' '<' '<' '<' '<' 'x']]\n",
      "Latest Scaled reward func:\n",
      " [[ 1.          0.33261182 -0.09575483 -0.17302434  0.11513661  0.68764834]\n",
      " [ 0.48892404 -0.31841325 -0.81967728 -0.87986823 -0.49237736  0.22863475]\n",
      " [ 0.34369759 -0.4890567  -0.98306899 -1.         -0.54770588  0.23429054]\n",
      " [ 0.62379444 -0.10940725 -0.518996   -0.48431339 -0.02851897  0.70169087]]\n",
      "Policy:\n",
      "[['<' '<' '<' '<' '<' '>']\n",
      " ['^' '<' 'x' '^' '^' '^']\n",
      " ['^' '<' '<' '<' '>' '^']\n",
      " ['^' '<' '<' '<' '<' 'x']]\n",
      "============================================================\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np_normalize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 28>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m irl:\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIRL from samples...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 52\u001b[0m     estimated_rewards \u001b[38;5;241m=\u001b[39m \u001b[43mirl_reward_estimation\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimal_trajectories\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrajectories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m     ref_reward_funcs\u001b[38;5;241m.\u001b[39mappend(estimated_rewards[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreference_reward_func\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     54\u001b[0m     avg_pred_reward_funcs\u001b[38;5;241m.\u001b[39mappend(estimated_rewards[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_predicted_reward_func\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36mirl_reward_estimation\u001b[1;34m(env, optimal_trajectories, train_func)\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m============================================================\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# print('reward_func_pred \\n', [np.array(reward_func_pred).flatten() for reward_func_pred in reward_func_preds]) #[np.array(one_candidate_value_estimates).flatten().shape for one_candidate_value_estimates in candidate_value_estimates ] )\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# print('reward_func_ref \\n', np.array(reward_func_ref).flatten())\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# vec1 = [np.array(reward_func_pred).flatten() for reward_func_pred in reward_func_preds]\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# plt.plot(reward_loss)\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n\u001b[1;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreference_reward_func\u001b[39m\u001b[38;5;124m'\u001b[39m: reward_func_ref, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpolicy_pred\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39marray([ np_normalize(\u001b[38;5;28mlist\u001b[39m(pol\u001b[38;5;241m.\u001b[39mvalues()), \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m pol \u001b[38;5;129;01min\u001b[39;00m candidate_policies ]), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_predicted_reward_func\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39marray(reward_func_preds), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)}\n",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m============================================================\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# print('reward_func_pred \\n', [np.array(reward_func_pred).flatten() for reward_func_pred in reward_func_preds]) #[np.array(one_candidate_value_estimates).flatten().shape for one_candidate_value_estimates in candidate_value_estimates ] )\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# print('reward_func_ref \\n', np.array(reward_func_ref).flatten())\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# vec1 = [np.array(reward_func_pred).flatten() for reward_func_pred in reward_func_preds]\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# plt.plot(reward_loss)\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n\u001b[1;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreference_reward_func\u001b[39m\u001b[38;5;124m'\u001b[39m: reward_func_ref, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpolicy_pred\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39marray([ \u001b[43mnp_normalize\u001b[49m(\u001b[38;5;28mlist\u001b[39m(pol\u001b[38;5;241m.\u001b[39mvalues()), \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m pol \u001b[38;5;129;01min\u001b[39;00m candidate_policies ]), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_predicted_reward_func\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39marray(reward_func_preds), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)}\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np_normalize' is not defined"
     ]
    }
   ],
   "source": [
    "vi = True\n",
    "ql = False\n",
    "gt = True\n",
    "irl = True\n",
    "plt = False\n",
    "\n",
    "print(\"configuration is:\")\n",
    "if vi:\n",
    "    print(\"training: value iteration\")\n",
    "if ql:\n",
    "    print(\"training: Q-Learning\")\n",
    "if gt:\n",
    "    print(\"generating trajectories\")\n",
    "if irl:\n",
    "    print(\"inverse reinforcment learning\")\n",
    "if plt:\n",
    "    print(\"creating plots\")\n",
    "    \n",
    "print(\"\")\n",
    "\n",
    "ref_reward_funcs = []\n",
    "avg_pred_reward_funcs = []\n",
    "reward_loss = []\n",
    "policy_loss = []\n",
    "\n",
    "for GW_SIZE in GW_SIZES:\n",
    "    environment = Grid_World(size=GW_SIZE, traps=GW_TRAPS, goals=GW_GOALS, randomize_board=False)\n",
    "\n",
    "    train_func = train_value_iteration\n",
    "\n",
    "    if vi:\n",
    "        print(\"Training via value iteration...\")\n",
    "        greedy_policy = train_value_iteration(gw_env=environment, verbose=True)\n",
    "    elif ql:\n",
    "        print(\"Training via q-learning...\")\n",
    "        greedy_policy = train_q_learning(gw_env=environment, verbose=True)\n",
    "        train_func = train_q_learning\n",
    "    else:\n",
    "        # load from file (?)\n",
    "        greedy_policy = {}\n",
    "\n",
    "    if gt:\n",
    "        print(f\"Generating {NUMBER_OF_TRAJECTORIES} trajectories...\")\n",
    "        trajectories = environment.generate_trajectories(policy=greedy_policy,\n",
    "                                                         n_traj=NUMBER_OF_TRAJECTORIES,\n",
    "                                                         max_traj_length=MAXIMUM_TRAJECTORY_LENGTH)\n",
    "\n",
    "    if irl:\n",
    "        print(\"IRL from samples...\")\n",
    "        estimated_rewards = irl_reward_estimation(env=environment, optimal_trajectories=trajectories, train_func=train_func)\n",
    "        ref_reward_funcs.append(estimated_rewards['reference_reward_func'])\n",
    "        avg_pred_reward_funcs.append(estimated_rewards['avg_predicted_reward_func'])\n",
    "        # Using default value for reward loss -> Frobenius for matrices and L2-loss for vectors\n",
    "        reward_loss.append(np.linalg.norm(estimated_rewards['reference_reward_func'] - estimated_rewards['avg_predicted_reward_func']))\n",
    "        # Using L1-Loss for policy loss as described by Ng and Russel in 2000\n",
    "        policy_loss.append(np.linalg.norm(estimated_rewards['policy_pred'] - np.array(list(greedy_policy.values())), ord=1 ))\n",
    "\n",
    "        print('**********************************************')\n",
    "        print('*****************REWARD LOSS******************')\n",
    "        print(reward_loss)\n",
    "        print('**********************************************')\n",
    "        print('*****************POLICY LOSS*************************')\n",
    "        print(policy_loss)\n",
    "        print('**********************************************')\n",
    "\n",
    "print('reward_loss \\n', reward_loss)\n",
    "plt.plot(reward_loss)\n",
    "plt.savefig('reward_loss.png')\n",
    "\n",
    "print(\"Closing up the arena...\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
