{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "from typing import Any, List, Callable, Dict\n",
    "\n",
    "from GridWorld_environments import Grid_World\n",
    "from RL_agents import ValueIterationAgent, QLearningAgent\n",
    "from IRL_agents import IRL_from_sampled_trajectories\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Value Iteration function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def train_value_iteration(gw_env: Grid_World, verbose=False):\n",
    "    vi_agent = ValueIterationAgent(states=gw_env.get_state_space(),\n",
    "                                   terminal_states=gw_env.get_terminal_states(),\n",
    "                                   reward_function=gw_env.get_reward_func(),\n",
    "                                   actions=gw_env.get_action_space(),\n",
    "                                   gamma=GAMMA)\n",
    "\n",
    "    iters = 0\n",
    "    while iters < VALUE_ITERATION_TRAINING_N and not vi_agent.value_converged:\n",
    "\n",
    "        for state in gw_env.get_state_space():\n",
    "\n",
    "            if state in gw_env.get_terminal_states():\n",
    "                continue\n",
    "\n",
    "            opt_act = vi_agent.get_optimal_action(action_state_pairs=gw_env.get_action_state_pairs(state=state))\n",
    "            next_state = gw_env.get_new_state_on_action(old_state=state, action=opt_act)\n",
    "            next_state_value = vi_agent.get_state_value(state=next_state)\n",
    "\n",
    "            vi_agent.set_state_value(state=state, new_value=(gw_env.get_state_reward(state=next_state) + GAMMA * next_state_value))\n",
    "\n",
    "        iters += 1\n",
    "\n",
    "    if verbose:\n",
    "        gw_env.display_value_function(value_func=vi_agent.get_value_function())\n",
    "\n",
    "    vi_agent.construct_greedy_policy(gw_env.get_action_state_pairs)\n",
    "\n",
    "    if verbose:\n",
    "        gw_env.display_policy(policy=vi_agent.get_policy())\n",
    "\n",
    "    return vi_agent.get_policy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Q Learning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_q_learning(gw_env: Grid_World, n_episodes=5000, verbose=False, policy=\"eps_greedy\", eps=0.2, max_episode_len=100, gamma=0.95):\n",
    "    ql_agent = QLearningAgent(states=gw_env.get_state_space(),\n",
    "                              size=gw_env.get_board_shape(),\n",
    "                              terminal_states=gw_env.get_terminal_states(),\n",
    "                              reward_function=gw_env.get_reward_func(),\n",
    "                              actions=gw_env.get_action_space(),\n",
    "                              gamma=gamma)\n",
    "    \n",
    "    # init episodes\n",
    "    episodes = []\n",
    "    \n",
    "    # Define state_space without terminal states for getting starting position\n",
    "    state_space = deepcopy(gw_env.get_state_space()) # all states\n",
    "    \n",
    "    board_size = gw_env.get_board_shape()\n",
    "    total_states = board_size[0] * board_size[1]\n",
    "    n_episodes = total_states * 500\n",
    "    \n",
    "    # Number 15 is empirically determined.\n",
    "    # For a 3x3 Grid the total states are 9 and we checked, that at least 100 states are required to produce reasonably reliable results\n",
    "    # So 9 * x >= 100 yields that x >= 10\n",
    "    # Now we also added a buffer and therefore chose 15\n",
    "    convergence_criterion = total_states * 30\n",
    "    \n",
    "    terminal_states = gw_env.get_terminal_states()\n",
    "    for terminal_state in terminal_states:\n",
    "        state_space.remove(terminal_state) # not non absorbing state_space\n",
    "    \n",
    "    # init state_visited_counter\n",
    "    state_visited = {state: 4 for state in state_space}\n",
    "    \n",
    "    #action_value_converged = False\n",
    "    convergence_counter = 0\n",
    "    \n",
    "    for n in range(n_episodes):\n",
    "        \n",
    "        episode = []\n",
    "        \n",
    "        # reset if every state has been visited at least 4 times (for each action)\n",
    "        if ( (np.array(list(state_visited.values())) <= 0).all() ):\n",
    "            state_visited = {state: 4 for state in state_space}\n",
    "        \n",
    "        # random starting position\n",
    "        states_not_visited = [ state for state in state_visited if state_visited[state] > 0 ]\n",
    "        if len(states_not_visited) > 0:\n",
    "            start_idx = (np.random.choice(len(states_not_visited)))\n",
    "            start = states_not_visited[start_idx]\n",
    "        else:\n",
    "            start_idx = (np.random.choice(len(state_space)))\n",
    "            start = state_space[start_idx]\n",
    "        \n",
    "        state_visited[start] -= 1\n",
    "        \n",
    "        episode.append(start)\n",
    "        \n",
    "        i = 0\n",
    "        terminal = False\n",
    "        \n",
    "        old_q_val_func = ql_agent.get_Q_function(mat_repr=True)\n",
    "        \n",
    "        while ( ( i < max_episode_len ) and ( not terminal ) ):\n",
    "            i += 1\n",
    "            \n",
    "            # Choose Action from S derived by given policy\n",
    "            if policy == \"eps_greedy\":\n",
    "                if np.random.uniform() < (1-eps):\n",
    "                    # Choose greedy action -> highest Q-Value\n",
    "                    chosen_action = ql_agent.get_greedy_action(episode[-1])\n",
    "                else:\n",
    "                    # Choose random action form action space\n",
    "                    action_space = gw_env.get_action_space()\n",
    "                    chosen_action = action_space[np.random.choice(len(action_space))]\n",
    "            \n",
    "            new_state = gw_env.get_new_state_on_action(episode[-1], chosen_action)\n",
    "            \n",
    "            # Reward is taken from Q_learning agent -> it knows the reward function from the environment\n",
    "            ql_agent.update_Q_value(episode[-1], new_state, chosen_action)\n",
    "            \n",
    "            episode.append(new_state)\n",
    "            \n",
    "            if new_state in terminal_states:\n",
    "                terminal = True\n",
    "            else:\n",
    "                # add to state visited counter for the new state if it is not terminal\n",
    "                state_visited[new_state] -= 1\n",
    "                #if (state_visited[new_state] >= 5):\n",
    "                    #state_visited[new_state] = 0\n",
    "                    \n",
    "        episodes.append(episode)\n",
    "                    \n",
    "        # essentially works nicely, but to be used carefully. States that will not be visited by the current policy\n",
    "        # will only be visited, when the start is chosen by random choice in this state\n",
    "        # -> Fixed by: rarely visited states will be preferred for the choice of the start\n",
    "        \n",
    "        # Check if Q-function did is close to the Q-function from the last episode\n",
    "        if np.isclose( old_q_val_func, ql_agent.get_Q_function(mat_repr=True), atol=1e-08 ).all( ):\n",
    "            convergence_counter += 1\n",
    "            \n",
    "            # Comment in print statements to see how the episodes develop until convergence\n",
    "            #print(\"--------------\")\n",
    "            #print(f\"episode {n}\")\n",
    "            #print(\"convergence_counter\", convergence_counter)\n",
    "            #print(\"--------------\")\n",
    "            \n",
    "            if convergence_counter >= convergence_criterion:\n",
    "                break\n",
    "        else:\n",
    "            convergence_counter = 0\n",
    "\n",
    "        \n",
    "        \n",
    "    if verbose:\n",
    "        if n < n_episodes:\n",
    "            print(f\"It took {n} episodes to converge to the optimal Q-function\")\n",
    "        else:\n",
    "            print(f\"Did not converge to optimal Q-function in {n_episodes} episodes\")\n",
    "    \n",
    "    if verbose:\n",
    "        gw_env.display_q_function(q_func=ql_agent.get_Q_function())\n",
    "\n",
    "    ql_agent.construct_greedy_policy(gw_env.get_action_state_pairs)\n",
    "\n",
    "    if verbose:\n",
    "        gw_env.display_policy(policy=ql_agent.get_policy())\n",
    "\n",
    "    return ql_agent.get_policy()       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform Action-Value evaluation function (Q-Learning evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_value_evaluation(gw_env: Grid_World, policy: Dict[Any, Any], verbose=False):\n",
    "    \n",
    "    vi_agent = ValueIterationAgent(states=gw_env.get_state_space(),\n",
    "                                   terminal_states=gw_env.get_terminal_states(),\n",
    "                                   reward_function=gw_env.get_reward_func(),\n",
    "                                   actions=gw_env.get_action_space(),\n",
    "                                   gamma=GAMMA)\n",
    "    \n",
    "    while not vi_agent.value_converged:\n",
    "        \n",
    "        for state in gw_env.get_state_space():\n",
    "            \n",
    "            if state in gw_env.get_terminal_states():\n",
    "                continue\n",
    "                \n",
    "            policy_act = policy[state]\n",
    "            next_state = gw_env.get_new_state_on_action(old_state=state, action=policy_act)\n",
    "            next_state_value = vi_agent.get_state_value(state=next_state)\n",
    "            \n",
    "            vi_agent.set_state_value(state=state, new_value=(gw_env.get_state_reward(state=next_state) + GAMMA * next_state_value))\n",
    "        \n",
    "    if verbose:\n",
    "        gw_env.display_value_function(value_func=vi_agent.get_value_function())\n",
    "\n",
    "    return vi_agent.get_value_function()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IRL Reward estimation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def irl_reward_estimation(env: Grid_World, optimal_trajectories: List[List[Any]], train_func: Callable):\n",
    "\n",
    "    # store reference reward function\n",
    "    reward_func_ref = deepcopy(env.get_board())\n",
    "    print('Reference reward function:\\n', reward_func_ref)\n",
    "\n",
    "    irl_agent = IRL_from_sampled_trajectories(d=(GW_SIZE[0] * 4, GW_SIZE[1] * 4),\n",
    "                                              env_ranges=((0, GW_SIZE[0]), (0, GW_SIZE[1])),\n",
    "                                              env_discrete_size=GW_SIZE,\n",
    "                                              penalty_factor=2,\n",
    "                                              gamma=GAMMA)\n",
    "\n",
    "    # step 2: given optimal trajectories, compute the value estimate\n",
    "    print(\"Computing value estimates for optimal trajectories...\")\n",
    "    optimal_value_estimate = irl_agent.compute_value_estimate(trajs=optimal_trajectories)\n",
    "\n",
    "    candidate_policies = [env.construct_random_policy()]\n",
    "    candidate_value_estimates = []\n",
    "    reward_func_estimates = []\n",
    "\n",
    "    # while True:\n",
    "    for i in range(IRL_TRAINING_N):\n",
    "        print(f\"Iteration {i}...\")\n",
    "\n",
    "        # step 3: generate trajectories and compute the value estimate for a random policy\n",
    "        print(\"Generating trajectories for the candidate policy...\")\n",
    "        candidate_trajectories = env.generate_trajectories(policy=candidate_policies[-1],\n",
    "                                                           n_traj=NUMBER_OF_TRAJECTORIES,\n",
    "                                                           max_traj_length=MAXIMUM_TRAJECTORY_LENGTH)\n",
    "        print(\"Computing value estimates for candidate trajectories...\")\n",
    "        candidate_value_estimates.append(irl_agent.compute_value_estimate(trajs=candidate_trajectories))\n",
    "\n",
    "        # step 4: obtain new alphas\n",
    "        print(\"Solving linear programming...\")\n",
    "        irl_agent.solve_lp(optimal_value_estimate, candidate_value_estimates)\n",
    "\n",
    "        # step 5: construct new reward function from the alphas\n",
    "        reward_func = irl_agent.construct_reward_function(alphas=irl_agent.get_alphas())\n",
    "\n",
    "        # step 6: find optimal policy under new reward function and add to 'candidate_policies' list\n",
    "        env.set_reward_func(reward_func)\n",
    "        candidate_policies.append(train_func(gw_env=env, verbose=False))  # train_value_iteration(gw_env=env))\n",
    "        # store new reward function\n",
    "        reward_func_estimates.append(env.get_board())\n",
    "        \n",
    "        print(\"Latest estimated reward function:\\n\", reward_func_estimates[-1])\n",
    "        env.display_policy(policy=candidate_policies[-1])\n",
    "        print(\"============================================================\\n\" * 2)\n",
    "\n",
    "    return {'environment': env, 'reference_reward_func': reward_func_ref, 'policy_pred': candidate_policies[-1], 'predicted_reward_func': reward_func_estimates[-1], 'avg_predicted_reward_func': np.mean(np.array(reward_func_estimates), axis=0)} #'policy_pred': np.mean(np.array([list(pol.values()) for pol in candidate_policies]), axis=0), 'avg_predicted_reward_func': np.mean(np.array(reward_func_estimates), axis=0)}\n",
    "\n",
    "\n",
    "def calc_value_distance(value_estimates_ref, value_estimates_pred):\n",
    "    return np.linalg.norm(np.array(value_estimates_ref) - np.array(value_estimates_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.95\n",
    "VALUE_ITERATION_TRAINING_N = 1000\n",
    "IRL_TRAINING_N = 10\n",
    "\n",
    "GW_SIZE = (3, 3)\n",
    "GW_SIZES = [(3, 3)]  # [(x, x) for x in np.arange(5,11, 5)]\n",
    "GW_TRAPS = []\n",
    "GW_GOALS = [(0, 0)]\n",
    "\n",
    "NUMBER_OF_STATES = GW_SIZE[0] * GW_SIZE[1]\n",
    "\n",
    "NUMBER_OF_TRAJECTORIES = NUMBER_OF_STATES * 20 #400\n",
    "MAXIMUM_TRAJECTORY_LENGTH = NUMBER_OF_STATES * 4 #10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-by-step code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = Grid_World(size=GW_SIZE, traps=GW_TRAPS, goals=GW_GOALS, randomize_board=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *****************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function:\n",
      "[[0.       1.       0.95    ]\n",
      " [1.       0.95     0.9025  ]\n",
      " [0.95     0.9025   0.857375]]\n",
      "Policy:\n",
      "[['x' '<' '<']\n",
      " ['^' '<' '<']\n",
      " ['^' '<' '<']]\n"
     ]
    }
   ],
   "source": [
    "vi_greedy_policy = train_value_iteration(gw_env=environment, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *****************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 636 episodes to converge to the optimal Q-function\n",
      "Q function:\n",
      "[[[0.         0.         0.         0.        ]\n",
      "  [0.9025     0.9025     0.95       1.        ]\n",
      "  [0.857375   0.9025     0.9025     0.95      ]]\n",
      "\n",
      " [[0.9025     0.9025     1.         0.95      ]\n",
      "  [0.857375   0.857375   0.95       0.95      ]\n",
      "  [0.81450625 0.857375   0.9025     0.9025    ]]\n",
      "\n",
      " [[0.9025     0.857375   0.95       0.9025    ]\n",
      "  [0.857375   0.81450625 0.9025     0.9025    ]\n",
      "  [0.81450625 0.81450625 0.857375   0.857375  ]]]\n",
      "Policy:\n",
      "[['x' '<' '<']\n",
      " ['^' '^' '^']\n",
      " ['^' '^' '^']]\n"
     ]
    }
   ],
   "source": [
    "ql_greedy_policy = train_q_learning(gw_env=environment, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Code can be uncommented for testing multiple iterations of Q-Learning\n",
    "\n",
    "#counter = 0\n",
    "#for _ in range(100):\n",
    "#    ql_greedy_policy = train_q_learning(gw_env=environment, verbose=False)\n",
    "#    if ql_greedy_policy != optimal_policy:\n",
    "        #print(\"ql_greedy_policy \\n\", ql_greedy_policy)\n",
    "        #print(\"optimal_policy \\n\", optimal_policy)\n",
    "        #environment.display_policy(policy=ql_greedy_policy)\n",
    "        #print(\"-----------------------------\")\n",
    "#        counter += 1\n",
    "#print(f\"policy was wrong {counter} times.\" )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 180 trajectories...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Generating {NUMBER_OF_TRAJECTORIES} trajectories...\")\n",
    "\n",
    "greedy_policy = vi_greedy_policy\n",
    "# greedy_policy = ql_greedy_policy\n",
    "\n",
    "trajectories = environment.generate_trajectories(policy=greedy_policy,\n",
    "                                                 n_traj=NUMBER_OF_TRAJECTORIES,\n",
    "                                                 max_traj_length=MAXIMUM_TRAJECTORY_LENGTH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate data for IRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IRL from samples...\n",
      "Reference reward function:\n",
      " [[1. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "Computing value estimates for optimal trajectories...\n"
     ]
    }
   ],
   "source": [
    "print(\"IRL from samples...\")\n",
    "\n",
    "# Init configuration\n",
    "\n",
    "GAMMA = 0.95\n",
    "VALUE_ITERATION_TRAINING_N = 1000\n",
    "IRL_TRAINING_N = 2\n",
    "\n",
    "#GW_SIZE = (3, 3)\n",
    "GW_SIZES = [(3, 3), (4,5), (7,7), (10,10)]  # [(x, x) for x in np.arange(5,11, 5)]\n",
    "GW_TRAPS = []\n",
    "GW_GOALS = [(0, 0)]\n",
    "\n",
    "for GW_SIZE in GW_SIZES:\n",
    "\n",
    "    NUMBER_OF_STATES = GW_SIZE[0] * GW_SIZE[1]\n",
    "\n",
    "    NUMBER_OF_TRAJECTORIES = NUMBER_OF_STATES * 20 #400\n",
    "    MAXIMUM_TRAJECTORY_LENGTH = NUMBER_OF_STATES * 4 #10\n",
    "\n",
    "\n",
    "    # restart the environment\n",
    "    environment = Grid_World(size=GW_SIZE, traps=GW_TRAPS, goals=GW_GOALS, randomize_board=False)\n",
    "\n",
    "    target_reward = deepcopy(environment.get_board())\n",
    "\n",
    "    vi_greedy_policy = train_value_iteration(gw_env=environment, verbose=False)\n",
    "    vi_greedy_value_function = perform_value_evaluation(gw_env=environment, policy=vi_greedy_policy, verbose=False)\n",
    "\n",
    "    # train_func = train_value_iteration\n",
    "    train_func = train_q_learning\n",
    "\n",
    "    estimated_rewards = irl_reward_estimation(env=environment, optimal_trajectories=trajectories, train_func=train_func)\n",
    "\n",
    "\n",
    "    vi_predicted_val_func = perform_value_evaluation(gw_env=estimated_rewards['environment'], policy=estimated_rewards['policy_pred'], verbose=False)\n",
    "\n",
    "\n",
    "    #ref_reward_funcs.append(estimated_rewards['reference_reward_func'])\n",
    "    #avg_pred_reward_funcs.append(estimated_rewards['avg_predicted_reward_func'])\n",
    "    # Using default value for reward loss -> Frobenius for matrices and L2-loss for vectors\n",
    "\n",
    "    #np.linalg.norm()\n",
    "    #reward_loss.append(np.linalg.norm(estimated_rewards['reference_reward_func'] - estimated_rewards['avg_predicted_reward_func']))\n",
    "    # Using L1-Loss for policy loss as described by Ng and Russel in 2000\n",
    "    #policy_loss.append(np.linalg.norm(estimated_rewards['policy_pred'] - np.array(list(greedy_policy.values())), ord=1 ))\n",
    "\n",
    "    # STORING\n",
    "\n",
    "    # add data to dictionary for storing\n",
    "\n",
    "    dict_to_store = {\n",
    "        \"metadata\": {\n",
    "            \"algorithm\": \"IRL - Q Learning\",\n",
    "            \"environment\": \"Grid_World\",\n",
    "            \"env_n_of_states\": NUMBER_OF_STATES,\n",
    "            \"env_size\": GW_SIZE,\n",
    "            \"env_traps\": GW_TRAPS,\n",
    "            \"env_goals\": GW_GOALS,\n",
    "            \"gamma\": GAMMA,\n",
    "            \"expert_n_of_trajs\": NUMBER_OF_TRAJECTORIES,\n",
    "            \"expert_max_traj_length\": MAXIMUM_TRAJECTORY_LENGTH\n",
    "        },\n",
    "        \"data\": {\n",
    "            \"target_reward\": target_reward,\n",
    "            \"predicted_rewards\": estimated_rewards['predicted_reward_func'],\n",
    "            \"expert_greedy_policy\": vi_greedy_policy,\n",
    "            \"expert_greedy_val_func\" : vi_greedy_value_function,\n",
    "            \"predicted_policy\": estimated_rewards['policy_pred'],\n",
    "            \"predicted_policy_val_func\": vi_predicted_val_func\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "    IRL_file_suffix = \"IRL_\"\n",
    "\n",
    "    # find largest file number of BIRL file in \"data\" folder\n",
    "    largest_file_number = 0\n",
    "\n",
    "    for file in os.listdir(\"data/IRL\"):\n",
    "        if IRL_file_suffix in file:\n",
    "            after_suffix = (file[len(IRL_file_suffix):])\n",
    "            file_number = int(after_suffix.split(\".\")[0])\n",
    "            if file_number > largest_file_number:\n",
    "                largest_file_number = file_number\n",
    "\n",
    "    print(f\"Found largest file number of IRL data file: {largest_file_number}\")\n",
    "\n",
    "\n",
    "    print(f\"Writing next IRL data file with number: {largest_file_number + 1}\")\n",
    "    # write dict to next file\n",
    "    with open(os.path.join(\"data/IRL\", f\"{IRL_file_suffix}{largest_file_number + 1}.pkl\"), \"wb\") as file:\n",
    "        pickle.dump(dict_to_store, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'metadata': {'algorithm': 'IRL - Q Learning', 'environment': 'Grid_World', 'env_n_of_states': 4, 'env_size': (2, 2), 'env_traps': [], 'env_goals': [(0, 0)], 'gamma': 0.95, 'expert_n_of_trajs': 80, 'expert_max_traj_length': 16}, 'data': {'target_reward': array([[1., 0.],\n",
      "       [0., 0.]]), 'predicted_rewards': array([[-1.34821153, -1.44839011],\n",
      "       [-1.44839011, -1.55262414]]), 'expert_greedy_policy': {(0, 0): (1, 0), (0, 1): (0, -1), (1, 0): (-1, 0), (1, 1): (0, -1)}, 'expert_greedy_val_func': {(0, 0): 0.0, (0, 1): 1.0, (1, 0): 1.0, (1, 1): 0.95}, 'predicted_policy': {(0, 0): (1, 0), (0, 1): (0, -1), (1, 0): (-1, 0), (1, 1): (-1, 0)}, 'predicted_policy_val_func': {(0, 0): 0.0, (0, 1): -1.3482115255684346, (1, 0): -1.3482115255684346, (1, 1): -2.7291910562390473}}}\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(\"data/IRL\", f\"{IRL_file_suffix}{7}.pkl\"), 'rb') as file:\n",
    "    \n",
    "    loaded_dict = pickle.load(file)\n",
    "\n",
    "    print(loaded_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original IRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IRL from samples...\n",
      "Reference reward function:\n",
      " [[1. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "Computing value estimates for optimal trajectories...\n",
      "Iteration 0...\n",
      "Generating trajectories for the candidate policy...\n",
      "Computing value estimates for condidate trajectories...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m train_func \u001b[38;5;241m=\u001b[39m train_value_iteration\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# train_func = train_q_learning\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m estimated_rewards \u001b[38;5;241m=\u001b[39m \u001b[43mirl_reward_estimation\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimal_trajectories\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrajectories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m ref_reward_funcs\u001b[38;5;241m.\u001b[39mappend(estimated_rewards[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreference_reward_func\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     11\u001b[0m avg_pred_reward_funcs\u001b[38;5;241m.\u001b[39mappend(estimated_rewards[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_predicted_reward_func\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mirl_reward_estimation\u001b[1;34m(env, optimal_trajectories, train_func)\u001b[0m\n\u001b[0;32m     27\u001b[0m candidate_trajectories \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mgenerate_trajectories(policy\u001b[38;5;241m=\u001b[39mcandidate_policies[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m     28\u001b[0m                                                    n_traj\u001b[38;5;241m=\u001b[39mNUMBER_OF_TRAJECTORIES,\n\u001b[0;32m     29\u001b[0m                                                    max_traj_length\u001b[38;5;241m=\u001b[39mMAXIMUM_TRAJECTORY_LENGTH)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComputing value estimates for condidate trajectories...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 31\u001b[0m candidate_value_estimates\u001b[38;5;241m.\u001b[39mappend(\u001b[43mirl_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_value_estimate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrajs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcandidate_trajectories\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# step 4: obtain new alphas\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSolving linear programming...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Niklas\\Studies\\Data_Science\\11_Reinforcement_Learning\\Project\\Coding\\IRL_project\\IRL_agents.py:74\u001b[0m, in \u001b[0;36mIRL_from_sampled_trajectories.compute_value_estimate\u001b[1;34m(self, trajs)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m traj \u001b[38;5;129;01min\u001b[39;00m trajs:\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, state \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(traj):\n\u001b[1;32m---> 74\u001b[0m         temp_value \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mi \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_approx_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapprox_center\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m temp_value \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(trajs)\n\u001b[0;32m     77\u001b[0m value_estimates\u001b[38;5;241m.\u001b[39mappend(temp_value)\n",
      "File \u001b[1;32mC:\\Niklas\\Studies\\Data_Science\\11_Reinforcement_Learning\\Project\\Coding\\IRL_project\\IRL_agents.py:61\u001b[0m, in \u001b[0;36mIRL_from_sampled_trajectories._approx_func\u001b[1;34m(self, x, center)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_approx_func\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, center):\n\u001b[1;32m---> 61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmultivariate_normal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapprox_func_cov\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\dev\\anaconda3\\lib\\site-packages\\scipy\\stats\\_multivariate.py:517\u001b[0m, in \u001b[0;36mmultivariate_normal_gen.pdf\u001b[1;34m(self, x, mean, cov, allow_singular)\u001b[0m\n\u001b[0;32m    515\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_quantiles(x, dim)\n\u001b[0;32m    516\u001b[0m psd \u001b[38;5;241m=\u001b[39m _PSD(cov, allow_singular\u001b[38;5;241m=\u001b[39mallow_singular)\n\u001b[1;32m--> 517\u001b[0m out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_logpdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpsd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mU\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpsd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_pdet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpsd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrank\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _squeeze_output(out)\n",
      "File \u001b[1;32mC:\\dev\\anaconda3\\lib\\site-packages\\scipy\\stats\\_multivariate.py:467\u001b[0m, in \u001b[0;36mmultivariate_normal_gen._logpdf\u001b[1;34m(self, x, mean, prec_U, log_det_cov, rank)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;124;03m\"\"\"Log of the multivariate normal probability density function.\u001b[39;00m\n\u001b[0;32m    444\u001b[0m \n\u001b[0;32m    445\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    464\u001b[0m \n\u001b[0;32m    465\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    466\u001b[0m dev \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m-\u001b[39m mean\n\u001b[1;32m--> 467\u001b[0m maha \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39msquare(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprec_U\u001b[49m\u001b[43m)\u001b[49m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m (rank \u001b[38;5;241m*\u001b[39m _LOG_2PI \u001b[38;5;241m+\u001b[39m log_det_cov \u001b[38;5;241m+\u001b[39m maha)\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"IRL from samples...\")\n",
    "\n",
    "# restart the environment\n",
    "environment = Grid_World(size=GW_SIZE, traps=GW_TRAPS, goals=GW_GOALS, randomize_board=False)\n",
    "\n",
    "train_func = train_value_iteration\n",
    "# train_func = train_q_learning\n",
    "\n",
    "estimated_rewards = irl_reward_estimation(env=environment, optimal_trajectories=trajectories, train_func=train_func)\n",
    "ref_reward_funcs.append(estimated_rewards['reference_reward_func'])\n",
    "avg_pred_reward_funcs.append(estimated_rewards['avg_predicted_reward_func'])\n",
    "# Using default value for reward loss -> Frobenius for matrices and L2-loss for vectors\n",
    "reward_loss.append(np.linalg.norm(estimated_rewards['reference_reward_func'] - estimated_rewards['avg_predicted_reward_func']))\n",
    "# Using L1-Loss for policy loss as described by Ng and Russel in 2000\n",
    "policy_loss.append(np.linalg.norm(estimated_rewards['policy_pred'] - np.array(list(greedy_policy.values())), ord=1 ))\n",
    "\n",
    "print('**********************************************')\n",
    "print('*****************REWARD LOSS******************')\n",
    "print(reward_loss)\n",
    "print('**********************************************')\n",
    "print('*****************POLICY LOSS*************************')\n",
    "print(policy_loss)\n",
    "print('**********************************************')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End-to-end loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi = True\n",
    "ql = False\n",
    "gt = True\n",
    "irl = True\n",
    "plt = False\n",
    "\n",
    "print(\"configuration is:\")\n",
    "if vi:\n",
    "    print(\"training: value iteration\")\n",
    "if ql:\n",
    "    print(\"training: Q-Learning\")\n",
    "if gt:\n",
    "    print(\"generating trajectories\")\n",
    "if irl:\n",
    "    print(\"inverse reinforcment learning\")\n",
    "if plt:\n",
    "    print(\"creating plots\")\n",
    "    \n",
    "print(\"\")\n",
    "\n",
    "ref_reward_funcs = []\n",
    "avg_pred_reward_funcs = []\n",
    "reward_loss = []\n",
    "policy_loss = []\n",
    "\n",
    "for GW_SIZE in GW_SIZES:\n",
    "    environment = Grid_World(size=GW_SIZE, traps=GW_TRAPS, goals=GW_GOALS, randomize_board=False)\n",
    "\n",
    "    train_func = train_value_iteration\n",
    "\n",
    "    if vi:\n",
    "        print(\"Training via value iteration...\")\n",
    "        greedy_policy = train_value_iteration(gw_env=environment, verbose=True)\n",
    "    elif ql:\n",
    "        print(\"Training via q-learning...\")\n",
    "        greedy_policy = train_q_learning(gw_env=environment, verbose=True)\n",
    "        train_func = train_q_learning\n",
    "    else:\n",
    "        # load from file (?)\n",
    "        greedy_policy = {}\n",
    "\n",
    "    if gt:\n",
    "        print(f\"Generating {NUMBER_OF_TRAJECTORIES} trajectories...\")\n",
    "        trajectories = environment.generate_trajectories(policy=greedy_policy,\n",
    "                                                         n_traj=NUMBER_OF_TRAJECTORIES,\n",
    "                                                         max_traj_length=MAXIMUM_TRAJECTORY_LENGTH)\n",
    "\n",
    "    if irl:\n",
    "        print(\"IRL from samples...\")\n",
    "        estimated_rewards = irl_reward_estimation(env=environment, optimal_trajectories=trajectories, train_func=train_func)\n",
    "        ref_reward_funcs.append(estimated_rewards['reference_reward_func'])\n",
    "        avg_pred_reward_funcs.append(estimated_rewards['avg_predicted_reward_func'])\n",
    "        # Using default value for reward loss -> Frobenius for matrices and L2-loss for vectors\n",
    "        reward_loss.append(np.linalg.norm(estimated_rewards['reference_reward_func'] - estimated_rewards['avg_predicted_reward_func']))\n",
    "        # Using L1-Loss for policy loss as described by Ng and Russel in 2000\n",
    "        policy_loss.append(np.linalg.norm(estimated_rewards['policy_pred'] - np.array(list(greedy_policy.values())), ord=1 ))\n",
    "\n",
    "        print('**********************************************')\n",
    "        print('*****************REWARD LOSS******************')\n",
    "        print(reward_loss)\n",
    "        print('**********************************************')\n",
    "        print('*****************POLICY LOSS*************************')\n",
    "        print(policy_loss)\n",
    "        print('**********************************************')\n",
    "\n",
    "print('reward_loss \\n', reward_loss)\n",
    "plt.plot(reward_loss)\n",
    "plt.savefig('reward_loss.png')\n",
    "\n",
    "print(\"Closing up the arena...\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "9650cb4e16cdd4a8e8e2d128bf38d875813998db22a3c986335f89e0cb4d7bb2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
