{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b95ef11a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "from typing import Any, List, Callable\n",
    "\n",
    "from GridWorld_environments import Grid_World\n",
    "from RL_agents import ValueIterationAgent, QLearningAgent\n",
    "from IRL_agents import IRL_from_sampled_trajectories\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812e149d",
   "metadata": {},
   "source": [
    "#### Train Value Iteration function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6c7fa1e9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def train_value_iteration(gw_env: Grid_World, verbose=False):\n",
    "    vi_agent = ValueIterationAgent(states=gw_env.get_state_space(),\n",
    "                                   terminal_states=gw_env.get_terminal_states(),\n",
    "                                   reward_function=gw_env.get_reward_func(),\n",
    "                                   actions=gw_env.get_action_space(),\n",
    "                                   gamma=GAMMA)\n",
    "\n",
    "    iters = 0\n",
    "    while iters < VALUE_ITERATION_TRAINING_N and not vi_agent.value_converged:\n",
    "\n",
    "        for state in gw_env.get_state_space():\n",
    "\n",
    "            if state in gw_env.get_terminal_states():\n",
    "                continue\n",
    "\n",
    "            opt_act = vi_agent.get_optimal_action(action_state_pairs=gw_env.get_action_state_pairs(state=state))\n",
    "            next_state = gw_env.get_new_state_on_action(old_state=state, action=opt_act)\n",
    "            next_state_value = vi_agent.get_state_value(state=next_state)\n",
    "\n",
    "            vi_agent.set_state_value(state=state, new_value=(gw_env.get_state_reward(state=next_state) + GAMMA * next_state_value))\n",
    "\n",
    "        iters += 1\n",
    "\n",
    "    if verbose:\n",
    "        gw_env.display_value_function(value_func=vi_agent.get_value_function())\n",
    "\n",
    "    vi_agent.construct_greedy_policy(gw_env.get_action_state_pairs)\n",
    "\n",
    "    if verbose:\n",
    "        gw_env.display_policy(policy=vi_agent.get_policy())\n",
    "\n",
    "    return vi_agent.get_policy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c569f0",
   "metadata": {},
   "source": [
    "#### Train Q Learning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9cbbc175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_q_learning(gw_env: Grid_World, n_episodes=1000, verbose=False, policy=\"eps_greedy\", eps=0.2, max_episode_len=100, gamma=0.95):\n",
    "    ql_agent = QLearningAgent(states=gw_env.get_state_space(),\n",
    "                              size=gw_env.get_board_shape(),\n",
    "                              terminal_states=gw_env.get_terminal_states(),\n",
    "                              reward_function=gw_env.get_reward_func(),\n",
    "                              actions=gw_env.get_action_space(),\n",
    "                              gamma=gamma)\n",
    "    \n",
    "    # init episodes\n",
    "    episodes = []\n",
    "    \n",
    "    # Define state_space without terminal states for getting starting position\n",
    "    state_space = deepcopy(gw_env.get_state_space()) # all states\n",
    "    terminal_states = gw_env.get_terminal_states()\n",
    "    for terminal_state in terminal_states:\n",
    "        state_space.remove(terminal_state) # not non absorbing state_space\n",
    "    \n",
    "    # init state_visited_counter\n",
    "    state_visited = {state: 0 for state in state_space} #np.zeros(gw_env.get_board_shape())\n",
    "    \n",
    "    #action_value_converged = False\n",
    "    convergence_counter = 0\n",
    "    \n",
    "    for n in range(n_episodes):\n",
    "        \n",
    "        episode = []\n",
    "        \n",
    "        # random starting position\n",
    "        states_not_visited = [ state for state in state_visited if state_visited[state] == 0 ]\n",
    "        #print(\"||||||\")\n",
    "        #print(\"states_not_visited\\n\", states_not_visited)\n",
    "        #print(\"^^^^^\")\n",
    "        if len(states_not_visited) > 0:\n",
    "            start_idx = (np.random.choice(len(states_not_visited)))\n",
    "            start = states_not_visited[start_idx]\n",
    "        else:\n",
    "            start_idx = (np.random.choice(len(state_space)))\n",
    "            start = state_space[start_idx]\n",
    "        \n",
    "        state_visited[start] += 1\n",
    "        \n",
    "        episode.append(start)\n",
    "        \n",
    "        i = 0\n",
    "        terminal = False\n",
    "        \n",
    "        old_q_val_func = ql_agent.get_Q_function(mat_repr=True)\n",
    "        \n",
    "        while ( ( i < max_episode_len ) and ( not terminal ) ):\n",
    "            i += 1\n",
    "            \n",
    "            # Choose Action from S derived by given policy\n",
    "            if policy == \"eps_greedy\":\n",
    "                if np.random.uniform() < (1-eps):\n",
    "                    # Choose greedy action -> highest Q-Value\n",
    "                    chosen_action = ql_agent.get_greedy_action(episode[-1])\n",
    "                else:\n",
    "                    # Choose random action form action space\n",
    "                    action_space = gw_env.get_action_space()\n",
    "                    chosen_action = action_space[np.random.choice(len(action_space))]\n",
    "            \n",
    "            new_state = gw_env.get_new_state_on_action(episode[-1], chosen_action)\n",
    "            \n",
    "            # Reward is taken from Q_learning agent -> it knows the reward function from the environment\n",
    "            ql_agent.update_Q_value(episode[-1], new_state, chosen_action)\n",
    "            \n",
    "            episode.append(new_state)\n",
    "            \n",
    "            if new_state in terminal_states:\n",
    "                terminal = True\n",
    "            else:\n",
    "                # add to state visited counter for the new state if it is not terminal\n",
    "                state_visited[new_state] += 1\n",
    "                if (state_visited[new_state] >= 5):\n",
    "                    state_visited[new_state] = 0\n",
    "                    \n",
    "        episodes.append(episode)\n",
    "                    \n",
    "        # essentially works nicely, but to be used carefully. States that will not be visited by the current policy\n",
    "        # will only be visited, when the start is chosen by random choice in this state\n",
    "        # TODO: Add logic, that chooses starts in states that have not been visited for X episodes\n",
    "        # Check if Q-function did is close to the Q-function from the last episode\n",
    "        if np.isclose( old_q_val_func, ql_agent.get_Q_function(mat_repr=True), atol=1e-08 ).all( ):\n",
    "            #print(\"***\")\n",
    "            #print(old_q_val_func)\n",
    "            #print(ql_agent.get_Q_function(mat_repr=True))\n",
    "            #print(\"***\")\n",
    "            convergence_counter += 1\n",
    "            #print(\"--------------\")\n",
    "            #print(f\"episode {n}\")\n",
    "            #print(\"convergence_counter\", convergence_counter)\n",
    "            #print(\"--------------\")\n",
    "            if convergence_counter >= 50:\n",
    "                break\n",
    "        else:\n",
    "            convergence_counter = 0\n",
    "\n",
    "        \n",
    "        \n",
    "    if verbose:\n",
    "        if n < n_episodes:\n",
    "            print(f\"It took {n} episodes to converge to the optimal Q-function\")\n",
    "        else:\n",
    "            print(f\"Did not converge to optimal Q-function in {n_episodes} episodes\")\n",
    "    \n",
    "    if verbose:\n",
    "        gw_env.display_q_function(q_func=ql_agent.get_Q_function())\n",
    "\n",
    "    ql_agent.construct_greedy_policy(gw_env.get_action_state_pairs)\n",
    "\n",
    "    if verbose:\n",
    "        gw_env.display_policy(policy=ql_agent.get_policy())\n",
    "\n",
    "    return ql_agent.get_policy()         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d945385a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(environment.get_board_shape())[(0,0)]\n",
    "d = {'a':1, 'b':2, 'c':3, 'e': 5}\n",
    "[ state for state in d if d[state] == 0 ]\n",
    "#for x in d:\n",
    "#    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d336096",
   "metadata": {},
   "source": [
    "#### IRL Reward estimation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "793e4fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def irl_reward_estimation(env: Grid_World, optimal_trajectories: List[List[Any]], train_func: Callable):\n",
    "\n",
    "    # store reference reward function\n",
    "    reward_func_ref = deepcopy(env.get_board())\n",
    "    print('Reference reward function:\\n', reward_func_ref)\n",
    "\n",
    "    irl_agent = IRL_from_sampled_trajectories(d=(GW_SIZE[0] * 4, GW_SIZE[1] * 4),\n",
    "                                              env_ranges=((0, GW_SIZE[0]), (0, GW_SIZE[1])),\n",
    "                                              env_discrete_size=GW_SIZE,\n",
    "                                              penalty_factor=2,\n",
    "                                              gamma=GAMMA)\n",
    "\n",
    "    # step 2: given optimal trajectories, compute the value estimate\n",
    "    print(\"Computing value estimates for optimal trajectories...\")\n",
    "    optimal_value_estimate = irl_agent.compute_value_estimate(trajs=optimal_trajectories)\n",
    "\n",
    "    candidate_policies = [env.construct_random_policy()]\n",
    "    candidate_value_estimates = []\n",
    "    reward_func_estimates = []\n",
    "\n",
    "    # while True:\n",
    "    for i in range(IRL_TRAINING_N):\n",
    "        print(f\"Iteration {i}...\")\n",
    "\n",
    "        # step 3: generate trajectories and compute the value estimate for a random policy\n",
    "        print(\"Generating trajectories for the candidate policy...\")\n",
    "        candidate_trajectories = env.generate_trajectories(policy=candidate_policies[-1],\n",
    "                                                           n_traj=NUMBER_OF_TRAJECTORIES,\n",
    "                                                           max_traj_length=MAXIMUM_TRAJECTORY_LENGTH)\n",
    "        print(\"Computing value estimates for condidate trajectories...\")\n",
    "        candidate_value_estimates.append(irl_agent.compute_value_estimate(trajs=candidate_trajectories))\n",
    "\n",
    "        # step 4: obtain new alphas\n",
    "        print(\"Solving linear programming...\")\n",
    "        irl_agent.solve_lp(optimal_value_estimate, candidate_value_estimates)\n",
    "\n",
    "        # step 5: construct new reward function from the alphas\n",
    "        reward_func = irl_agent.construct_reward_function(alphas=irl_agent.get_alphas())\n",
    "\n",
    "        # step 6: find optimal policy under new reward function and add to 'candidate_policies' list\n",
    "        env.set_reward_func(reward_func)\n",
    "        candidate_policies.append(train_func(gw_env=env, verbose=True))  # train_value_iteration(gw_env=env))\n",
    "        # store new reward function\n",
    "        reward_func_estimates.append(env.get_board())\n",
    "        \n",
    "        print(\"Latest estimated reward function:\\n\", reward_func_estimates[-1])\n",
    "        env.display_policy(policy=candidate_policies[-1])\n",
    "        print(\"============================================================\\n\" * 2)\n",
    "\n",
    "    return {'reference_reward_func': reward_func_ref, 'policy_pred': np.mean(np.array([list(pol.values()) for pol in candidate_policies]), axis=0), 'avg_predicted_reward_func': np.mean(np.array(reward_func_estimates), axis=0)}\n",
    "\n",
    "\n",
    "def calc_value_distance(value_estimates_ref, value_estimates_pred):\n",
    "    return np.linalg.norm(np.array(value_estimates_ref) - np.array(value_estimates_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a8ea0f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.95\n",
    "VALUE_ITERATION_TRAINING_N = 25\n",
    "IRL_TRAINING_N = 10\n",
    "\n",
    "NUMBER_OF_TRAJECTORIES = 400\n",
    "MAXIMUM_TRAJECTORY_LENGTH = 10\n",
    "\n",
    "GW_SIZE = (3, 3)\n",
    "GW_SIZES = [(3, 3)]  # [(x, x) for x in np.arange(5,11, 5)]\n",
    "GW_TRAPS = []\n",
    "GW_GOALS = [(0, 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46913987",
   "metadata": {},
   "source": [
    "### Step-by-step code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "90070f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = Grid_World(size=GW_SIZE, traps=GW_TRAPS, goals=GW_GOALS, randomize_board=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a76a67a",
   "metadata": {},
   "source": [
    "## *****************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "176c8196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function:\n",
      "[[0.       1.       0.95    ]\n",
      " [1.       0.95     0.9025  ]\n",
      " [0.95     0.9025   0.857375]]\n",
      "Policy:\n",
      "[['x' '<' '<']\n",
      " ['^' '<' '<']\n",
      " ['^' '<' '<']]\n"
     ]
    }
   ],
   "source": [
    "vi_greedy_policy = train_value_iteration(gw_env=environment, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850e7cc5",
   "metadata": {},
   "source": [
    "## *****************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "585078bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "episode 4\n",
      "convergence_counter 1\n",
      "--------------\n",
      "--------------\n",
      "episode 5\n",
      "convergence_counter 2\n",
      "--------------\n",
      "--------------\n",
      "episode 10\n",
      "convergence_counter 1\n",
      "--------------\n",
      "--------------\n",
      "episode 11\n",
      "convergence_counter 2\n",
      "--------------\n",
      "--------------\n",
      "episode 12\n",
      "convergence_counter 3\n",
      "--------------\n",
      "--------------\n",
      "episode 15\n",
      "convergence_counter 1\n",
      "--------------\n",
      "--------------\n",
      "episode 18\n",
      "convergence_counter 1\n",
      "--------------\n",
      "--------------\n",
      "episode 19\n",
      "convergence_counter 2\n",
      "--------------\n",
      "--------------\n",
      "episode 20\n",
      "convergence_counter 3\n",
      "--------------\n",
      "--------------\n",
      "episode 21\n",
      "convergence_counter 4\n",
      "--------------\n",
      "--------------\n",
      "episode 22\n",
      "convergence_counter 5\n",
      "--------------\n",
      "--------------\n",
      "episode 23\n",
      "convergence_counter 6\n",
      "--------------\n",
      "--------------\n",
      "episode 24\n",
      "convergence_counter 7\n",
      "--------------\n",
      "--------------\n",
      "episode 26\n",
      "convergence_counter 1\n",
      "--------------\n",
      "--------------\n",
      "episode 27\n",
      "convergence_counter 2\n",
      "--------------\n",
      "--------------\n",
      "episode 28\n",
      "convergence_counter 3\n",
      "--------------\n",
      "--------------\n",
      "episode 29\n",
      "convergence_counter 4\n",
      "--------------\n",
      "--------------\n",
      "episode 30\n",
      "convergence_counter 5\n",
      "--------------\n",
      "--------------\n",
      "episode 31\n",
      "convergence_counter 6\n",
      "--------------\n",
      "--------------\n",
      "episode 32\n",
      "convergence_counter 7\n",
      "--------------\n",
      "--------------\n",
      "episode 33\n",
      "convergence_counter 8\n",
      "--------------\n",
      "--------------\n",
      "episode 34\n",
      "convergence_counter 9\n",
      "--------------\n",
      "--------------\n",
      "episode 36\n",
      "convergence_counter 1\n",
      "--------------\n",
      "--------------\n",
      "episode 37\n",
      "convergence_counter 2\n",
      "--------------\n",
      "--------------\n",
      "episode 38\n",
      "convergence_counter 3\n",
      "--------------\n",
      "--------------\n",
      "episode 39\n",
      "convergence_counter 4\n",
      "--------------\n",
      "--------------\n",
      "episode 40\n",
      "convergence_counter 5\n",
      "--------------\n",
      "--------------\n",
      "episode 41\n",
      "convergence_counter 6\n",
      "--------------\n",
      "--------------\n",
      "episode 42\n",
      "convergence_counter 7\n",
      "--------------\n",
      "--------------\n",
      "episode 43\n",
      "convergence_counter 8\n",
      "--------------\n",
      "--------------\n",
      "episode 44\n",
      "convergence_counter 9\n",
      "--------------\n",
      "--------------\n",
      "episode 45\n",
      "convergence_counter 10\n",
      "--------------\n",
      "--------------\n",
      "episode 46\n",
      "convergence_counter 11\n",
      "--------------\n",
      "--------------\n",
      "episode 47\n",
      "convergence_counter 12\n",
      "--------------\n",
      "--------------\n",
      "episode 48\n",
      "convergence_counter 13\n",
      "--------------\n",
      "--------------\n",
      "episode 49\n",
      "convergence_counter 14\n",
      "--------------\n",
      "--------------\n",
      "episode 51\n",
      "convergence_counter 1\n",
      "--------------\n",
      "--------------\n",
      "episode 52\n",
      "convergence_counter 2\n",
      "--------------\n",
      "--------------\n",
      "episode 53\n",
      "convergence_counter 3\n",
      "--------------\n",
      "--------------\n",
      "episode 54\n",
      "convergence_counter 4\n",
      "--------------\n",
      "--------------\n",
      "episode 55\n",
      "convergence_counter 5\n",
      "--------------\n",
      "--------------\n",
      "episode 56\n",
      "convergence_counter 6\n",
      "--------------\n",
      "--------------\n",
      "episode 57\n",
      "convergence_counter 7\n",
      "--------------\n",
      "--------------\n",
      "episode 58\n",
      "convergence_counter 8\n",
      "--------------\n",
      "--------------\n",
      "episode 59\n",
      "convergence_counter 9\n",
      "--------------\n",
      "--------------\n",
      "episode 60\n",
      "convergence_counter 10\n",
      "--------------\n",
      "--------------\n",
      "episode 61\n",
      "convergence_counter 11\n",
      "--------------\n",
      "--------------\n",
      "episode 62\n",
      "convergence_counter 12\n",
      "--------------\n",
      "--------------\n",
      "episode 63\n",
      "convergence_counter 13\n",
      "--------------\n",
      "--------------\n",
      "episode 64\n",
      "convergence_counter 14\n",
      "--------------\n",
      "--------------\n",
      "episode 65\n",
      "convergence_counter 15\n",
      "--------------\n",
      "--------------\n",
      "episode 66\n",
      "convergence_counter 16\n",
      "--------------\n",
      "--------------\n",
      "episode 67\n",
      "convergence_counter 17\n",
      "--------------\n",
      "--------------\n",
      "episode 68\n",
      "convergence_counter 18\n",
      "--------------\n",
      "--------------\n",
      "episode 69\n",
      "convergence_counter 19\n",
      "--------------\n",
      "--------------\n",
      "episode 70\n",
      "convergence_counter 20\n",
      "--------------\n",
      "--------------\n",
      "episode 71\n",
      "convergence_counter 21\n",
      "--------------\n",
      "--------------\n",
      "episode 72\n",
      "convergence_counter 22\n",
      "--------------\n",
      "--------------\n",
      "episode 73\n",
      "convergence_counter 23\n",
      "--------------\n",
      "--------------\n",
      "episode 74\n",
      "convergence_counter 24\n",
      "--------------\n",
      "--------------\n",
      "episode 75\n",
      "convergence_counter 25\n",
      "--------------\n",
      "--------------\n",
      "episode 76\n",
      "convergence_counter 26\n",
      "--------------\n",
      "--------------\n",
      "episode 77\n",
      "convergence_counter 27\n",
      "--------------\n",
      "--------------\n",
      "episode 78\n",
      "convergence_counter 28\n",
      "--------------\n",
      "--------------\n",
      "episode 79\n",
      "convergence_counter 29\n",
      "--------------\n",
      "--------------\n",
      "episode 80\n",
      "convergence_counter 30\n",
      "--------------\n",
      "--------------\n",
      "episode 81\n",
      "convergence_counter 31\n",
      "--------------\n",
      "--------------\n",
      "episode 82\n",
      "convergence_counter 32\n",
      "--------------\n",
      "--------------\n",
      "episode 83\n",
      "convergence_counter 33\n",
      "--------------\n",
      "--------------\n",
      "episode 84\n",
      "convergence_counter 34\n",
      "--------------\n",
      "--------------\n",
      "episode 85\n",
      "convergence_counter 35\n",
      "--------------\n",
      "--------------\n",
      "episode 86\n",
      "convergence_counter 36\n",
      "--------------\n",
      "--------------\n",
      "episode 87\n",
      "convergence_counter 37\n",
      "--------------\n",
      "--------------\n",
      "episode 88\n",
      "convergence_counter 38\n",
      "--------------\n",
      "--------------\n",
      "episode 90\n",
      "convergence_counter 1\n",
      "--------------\n",
      "--------------\n",
      "episode 91\n",
      "convergence_counter 2\n",
      "--------------\n",
      "--------------\n",
      "episode 92\n",
      "convergence_counter 3\n",
      "--------------\n",
      "--------------\n",
      "episode 93\n",
      "convergence_counter 4\n",
      "--------------\n",
      "--------------\n",
      "episode 94\n",
      "convergence_counter 5\n",
      "--------------\n",
      "--------------\n",
      "episode 95\n",
      "convergence_counter 6\n",
      "--------------\n",
      "--------------\n",
      "episode 96\n",
      "convergence_counter 7\n",
      "--------------\n",
      "--------------\n",
      "episode 98\n",
      "convergence_counter 1\n",
      "--------------\n",
      "--------------\n",
      "episode 99\n",
      "convergence_counter 2\n",
      "--------------\n",
      "--------------\n",
      "episode 100\n",
      "convergence_counter 3\n",
      "--------------\n",
      "--------------\n",
      "episode 101\n",
      "convergence_counter 4\n",
      "--------------\n",
      "--------------\n",
      "episode 102\n",
      "convergence_counter 5\n",
      "--------------\n",
      "--------------\n",
      "episode 103\n",
      "convergence_counter 6\n",
      "--------------\n",
      "--------------\n",
      "episode 104\n",
      "convergence_counter 7\n",
      "--------------\n",
      "--------------\n",
      "episode 105\n",
      "convergence_counter 8\n",
      "--------------\n",
      "--------------\n",
      "episode 107\n",
      "convergence_counter 1\n",
      "--------------\n",
      "--------------\n",
      "episode 108\n",
      "convergence_counter 2\n",
      "--------------\n",
      "--------------\n",
      "episode 109\n",
      "convergence_counter 3\n",
      "--------------\n",
      "--------------\n",
      "episode 110\n",
      "convergence_counter 4\n",
      "--------------\n",
      "--------------\n",
      "episode 111\n",
      "convergence_counter 5\n",
      "--------------\n",
      "--------------\n",
      "episode 112\n",
      "convergence_counter 6\n",
      "--------------\n",
      "--------------\n",
      "episode 113\n",
      "convergence_counter 7\n",
      "--------------\n",
      "--------------\n",
      "episode 114\n",
      "convergence_counter 8\n",
      "--------------\n",
      "--------------\n",
      "episode 115\n",
      "convergence_counter 9\n",
      "--------------\n",
      "--------------\n",
      "episode 117\n",
      "convergence_counter 1\n",
      "--------------\n",
      "--------------\n",
      "episode 118\n",
      "convergence_counter 2\n",
      "--------------\n",
      "--------------\n",
      "episode 119\n",
      "convergence_counter 3\n",
      "--------------\n",
      "--------------\n",
      "episode 120\n",
      "convergence_counter 4\n",
      "--------------\n",
      "--------------\n",
      "episode 121\n",
      "convergence_counter 5\n",
      "--------------\n",
      "--------------\n",
      "episode 122\n",
      "convergence_counter 6\n",
      "--------------\n",
      "--------------\n",
      "episode 123\n",
      "convergence_counter 7\n",
      "--------------\n",
      "--------------\n",
      "episode 124\n",
      "convergence_counter 8\n",
      "--------------\n",
      "--------------\n",
      "episode 125\n",
      "convergence_counter 9\n",
      "--------------\n",
      "--------------\n",
      "episode 126\n",
      "convergence_counter 10\n",
      "--------------\n",
      "--------------\n",
      "episode 127\n",
      "convergence_counter 11\n",
      "--------------\n",
      "--------------\n",
      "episode 128\n",
      "convergence_counter 12\n",
      "--------------\n",
      "--------------\n",
      "episode 129\n",
      "convergence_counter 13\n",
      "--------------\n",
      "--------------\n",
      "episode 131\n",
      "convergence_counter 1\n",
      "--------------\n",
      "--------------\n",
      "episode 133\n",
      "convergence_counter 1\n",
      "--------------\n",
      "--------------\n",
      "episode 134\n",
      "convergence_counter 2\n",
      "--------------\n",
      "--------------\n",
      "episode 135\n",
      "convergence_counter 3\n",
      "--------------\n",
      "--------------\n",
      "episode 136\n",
      "convergence_counter 4\n",
      "--------------\n",
      "--------------\n",
      "episode 137\n",
      "convergence_counter 5\n",
      "--------------\n",
      "--------------\n",
      "episode 138\n",
      "convergence_counter 6\n",
      "--------------\n",
      "--------------\n",
      "episode 139\n",
      "convergence_counter 7\n",
      "--------------\n",
      "--------------\n",
      "episode 140\n",
      "convergence_counter 8\n",
      "--------------\n",
      "--------------\n",
      "episode 141\n",
      "convergence_counter 9\n",
      "--------------\n",
      "--------------\n",
      "episode 142\n",
      "convergence_counter 10\n",
      "--------------\n",
      "--------------\n",
      "episode 143\n",
      "convergence_counter 11\n",
      "--------------\n",
      "--------------\n",
      "episode 144\n",
      "convergence_counter 12\n",
      "--------------\n",
      "--------------\n",
      "episode 145\n",
      "convergence_counter 13\n",
      "--------------\n",
      "--------------\n",
      "episode 146\n",
      "convergence_counter 14\n",
      "--------------\n",
      "--------------\n",
      "episode 147\n",
      "convergence_counter 15\n",
      "--------------\n",
      "--------------\n",
      "episode 148\n",
      "convergence_counter 16\n",
      "--------------\n",
      "--------------\n",
      "episode 150\n",
      "convergence_counter 1\n",
      "--------------\n",
      "--------------\n",
      "episode 151\n",
      "convergence_counter 2\n",
      "--------------\n",
      "--------------\n",
      "episode 152\n",
      "convergence_counter 3\n",
      "--------------\n",
      "--------------\n",
      "episode 153\n",
      "convergence_counter 4\n",
      "--------------\n",
      "--------------\n",
      "episode 154\n",
      "convergence_counter 5\n",
      "--------------\n",
      "--------------\n",
      "episode 155\n",
      "convergence_counter 6\n",
      "--------------\n",
      "--------------\n",
      "episode 157\n",
      "convergence_counter 1\n",
      "--------------\n",
      "--------------\n",
      "episode 158\n",
      "convergence_counter 2\n",
      "--------------\n",
      "--------------\n",
      "episode 160\n",
      "convergence_counter 1\n",
      "--------------\n",
      "--------------\n",
      "episode 161\n",
      "convergence_counter 2\n",
      "--------------\n",
      "--------------\n",
      "episode 162\n",
      "convergence_counter 3\n",
      "--------------\n",
      "--------------\n",
      "episode 163\n",
      "convergence_counter 4\n",
      "--------------\n",
      "--------------\n",
      "episode 164\n",
      "convergence_counter 5\n",
      "--------------\n",
      "--------------\n",
      "episode 165\n",
      "convergence_counter 6\n",
      "--------------\n",
      "--------------\n",
      "episode 166\n",
      "convergence_counter 7\n",
      "--------------\n",
      "--------------\n",
      "episode 167\n",
      "convergence_counter 8\n",
      "--------------\n",
      "--------------\n",
      "episode 168\n",
      "convergence_counter 9\n",
      "--------------\n",
      "--------------\n",
      "episode 169\n",
      "convergence_counter 10\n",
      "--------------\n",
      "--------------\n",
      "episode 170\n",
      "convergence_counter 11\n",
      "--------------\n",
      "--------------\n",
      "episode 171\n",
      "convergence_counter 12\n",
      "--------------\n",
      "--------------\n",
      "episode 172\n",
      "convergence_counter 13\n",
      "--------------\n",
      "--------------\n",
      "episode 173\n",
      "convergence_counter 14\n",
      "--------------\n",
      "--------------\n",
      "episode 174\n",
      "convergence_counter 15\n",
      "--------------\n",
      "--------------\n",
      "episode 175\n",
      "convergence_counter 16\n",
      "--------------\n",
      "--------------\n",
      "episode 176\n",
      "convergence_counter 17\n",
      "--------------\n",
      "--------------\n",
      "episode 177\n",
      "convergence_counter 18\n",
      "--------------\n",
      "--------------\n",
      "episode 178\n",
      "convergence_counter 19\n",
      "--------------\n",
      "--------------\n",
      "episode 179\n",
      "convergence_counter 20\n",
      "--------------\n",
      "--------------\n",
      "episode 180\n",
      "convergence_counter 21\n",
      "--------------\n",
      "--------------\n",
      "episode 181\n",
      "convergence_counter 22\n",
      "--------------\n",
      "--------------\n",
      "episode 182\n",
      "convergence_counter 23\n",
      "--------------\n",
      "--------------\n",
      "episode 183\n",
      "convergence_counter 24\n",
      "--------------\n",
      "--------------\n",
      "episode 184\n",
      "convergence_counter 25\n",
      "--------------\n",
      "--------------\n",
      "episode 185\n",
      "convergence_counter 26\n",
      "--------------\n",
      "--------------\n",
      "episode 186\n",
      "convergence_counter 27\n",
      "--------------\n",
      "--------------\n",
      "episode 187\n",
      "convergence_counter 28\n",
      "--------------\n",
      "--------------\n",
      "episode 188\n",
      "convergence_counter 29\n",
      "--------------\n",
      "--------------\n",
      "episode 189\n",
      "convergence_counter 30\n",
      "--------------\n",
      "--------------\n",
      "episode 190\n",
      "convergence_counter 31\n",
      "--------------\n",
      "--------------\n",
      "episode 191\n",
      "convergence_counter 32\n",
      "--------------\n",
      "--------------\n",
      "episode 192\n",
      "convergence_counter 33\n",
      "--------------\n",
      "--------------\n",
      "episode 193\n",
      "convergence_counter 34\n",
      "--------------\n",
      "--------------\n",
      "episode 194\n",
      "convergence_counter 35\n",
      "--------------\n",
      "--------------\n",
      "episode 195\n",
      "convergence_counter 36\n",
      "--------------\n",
      "--------------\n",
      "episode 196\n",
      "convergence_counter 37\n",
      "--------------\n",
      "--------------\n",
      "episode 197\n",
      "convergence_counter 38\n",
      "--------------\n",
      "--------------\n",
      "episode 198\n",
      "convergence_counter 39\n",
      "--------------\n",
      "--------------\n",
      "episode 199\n",
      "convergence_counter 40\n",
      "--------------\n",
      "--------------\n",
      "episode 200\n",
      "convergence_counter 41\n",
      "--------------\n",
      "--------------\n",
      "episode 201\n",
      "convergence_counter 42\n",
      "--------------\n",
      "--------------\n",
      "episode 202\n",
      "convergence_counter 43\n",
      "--------------\n",
      "--------------\n",
      "episode 203\n",
      "convergence_counter 44\n",
      "--------------\n",
      "--------------\n",
      "episode 204\n",
      "convergence_counter 45\n",
      "--------------\n",
      "--------------\n",
      "episode 205\n",
      "convergence_counter 46\n",
      "--------------\n",
      "--------------\n",
      "episode 206\n",
      "convergence_counter 47\n",
      "--------------\n",
      "--------------\n",
      "episode 207\n",
      "convergence_counter 48\n",
      "--------------\n",
      "--------------\n",
      "episode 208\n",
      "convergence_counter 49\n",
      "--------------\n",
      "--------------\n",
      "episode 209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence_counter 50\n",
      "--------------\n",
      "It took 209 episodes to converge to the optimal Q-function\n",
      "Q function:\n",
      "[[[0.         0.         0.         0.        ]\n",
      "  [0.9025     0.9025     0.95       1.        ]\n",
      "  [0.857375   0.9025     0.9025     0.95      ]]\n",
      "\n",
      " [[0.9025     0.9025     1.         0.95      ]\n",
      "  [0.857375   0.857375   0.95       0.95      ]\n",
      "  [0.81450625 0.857375   0.9025     0.9025    ]]\n",
      "\n",
      " [[0.67310681 0.857375   0.95       0.9025    ]\n",
      "  [0.857375   0.81450625 0.9025     0.9025    ]\n",
      "  [0.81450625 0.67095911 0.857375   0.857375  ]]]\n",
      "Policy:\n",
      "[['x' '<' '<']\n",
      " ['^' '^' '^']\n",
      " ['^' '^' '^']]\n"
     ]
    }
   ],
   "source": [
    "ql_greedy_policy = train_q_learning(gw_env=environment, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b8810805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 400 trajectories...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Generating {NUMBER_OF_TRAJECTORIES} trajectories...\")\n",
    "\n",
    "greedy_policy = vi_greedy_policy\n",
    "# greedy_policy = ql_greedy_policy\n",
    "\n",
    "trajectories = environment.generate_trajectories(policy=greedy_policy,\n",
    "                                                 n_traj=NUMBER_OF_TRAJECTORIES,\n",
    "                                                 max_traj_length=MAXIMUM_TRAJECTORY_LENGTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e877c596",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"IRL from samples...\")\n",
    "\n",
    "# restart the environment\n",
    "environment = Grid_World(size=GW_SIZE, traps=GW_TRAPS, goals=GW_GOALS, randomize_board=False)\n",
    "\n",
    "train_func = train_value_iteration\n",
    "# train_func = train_q_learning\n",
    "\n",
    "estimated_rewards = irl_reward_estimation(env=environment, optimal_trajectories=trajectories, train_func=train_func)\n",
    "ref_reward_funcs.append(estimated_rewards['reference_reward_func'])\n",
    "avg_pred_reward_funcs.append(estimated_rewards['avg_predicted_reward_func'])\n",
    "# Using default value for reward loss -> Frobenius for matrices and L2-loss for vectors\n",
    "reward_loss.append(np.linalg.norm(estimated_rewards['reference_reward_func'] - estimated_rewards['avg_predicted_reward_func']))\n",
    "# Using L1-Loss for policy loss as described by Ng and Russel in 2000\n",
    "policy_loss.append(np.linalg.norm(estimated_rewards['policy_pred'] - np.array(list(greedy_policy.values())), ord=1 ))\n",
    "\n",
    "print('**********************************************')\n",
    "print('*****************REWARD LOSS******************')\n",
    "print(reward_loss)\n",
    "print('**********************************************')\n",
    "print('*****************POLICY LOSS*************************')\n",
    "print(policy_loss)\n",
    "print('**********************************************')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed02e3e",
   "metadata": {},
   "source": [
    "### End-to-end loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0042f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "vi = True\n",
    "ql = False\n",
    "gt = True\n",
    "irl = True\n",
    "plt = False\n",
    "\n",
    "print(\"configuration is:\")\n",
    "if vi:\n",
    "    print(\"training: value iteration\")\n",
    "if ql:\n",
    "    print(\"training: Q-Learning\")\n",
    "if gt:\n",
    "    print(\"generating trajectories\")\n",
    "if irl:\n",
    "    print(\"inverse reinforcment learning\")\n",
    "if plt:\n",
    "    print(\"creating plots\")\n",
    "    \n",
    "print(\"\")\n",
    "\n",
    "ref_reward_funcs = []\n",
    "avg_pred_reward_funcs = []\n",
    "reward_loss = []\n",
    "policy_loss = []\n",
    "\n",
    "for GW_SIZE in GW_SIZES:\n",
    "    environment = Grid_World(size=GW_SIZE, traps=GW_TRAPS, goals=GW_GOALS, randomize_board=False)\n",
    "\n",
    "    train_func = train_value_iteration\n",
    "\n",
    "    if vi:\n",
    "        print(\"Training via value iteration...\")\n",
    "        greedy_policy = train_value_iteration(gw_env=environment, verbose=True)\n",
    "    elif ql:\n",
    "        print(\"Training via q-learning...\")\n",
    "        greedy_policy = train_q_learning(gw_env=environment, verbose=True)\n",
    "        train_func = train_q_learning\n",
    "    else:\n",
    "        # load from file (?)\n",
    "        greedy_policy = {}\n",
    "\n",
    "    if gt:\n",
    "        print(f\"Generating {NUMBER_OF_TRAJECTORIES} trajectories...\")\n",
    "        trajectories = environment.generate_trajectories(policy=greedy_policy,\n",
    "                                                         n_traj=NUMBER_OF_TRAJECTORIES,\n",
    "                                                         max_traj_length=MAXIMUM_TRAJECTORY_LENGTH)\n",
    "\n",
    "    if irl:\n",
    "        print(\"IRL from samples...\")\n",
    "        estimated_rewards = irl_reward_estimation(env=environment, optimal_trajectories=trajectories, train_func=train_func)\n",
    "        ref_reward_funcs.append(estimated_rewards['reference_reward_func'])\n",
    "        avg_pred_reward_funcs.append(estimated_rewards['avg_predicted_reward_func'])\n",
    "        # Using default value for reward loss -> Frobenius for matrices and L2-loss for vectors\n",
    "        reward_loss.append(np.linalg.norm(estimated_rewards['reference_reward_func'] - estimated_rewards['avg_predicted_reward_func']))\n",
    "        # Using L1-Loss for policy loss as described by Ng and Russel in 2000\n",
    "        policy_loss.append(np.linalg.norm(estimated_rewards['policy_pred'] - np.array(list(greedy_policy.values())), ord=1 ))\n",
    "\n",
    "        print('**********************************************')\n",
    "        print('*****************REWARD LOSS******************')\n",
    "        print(reward_loss)\n",
    "        print('**********************************************')\n",
    "        print('*****************POLICY LOSS*************************')\n",
    "        print(policy_loss)\n",
    "        print('**********************************************')\n",
    "\n",
    "print('reward_loss \\n', reward_loss)\n",
    "plt.plot(reward_loss)\n",
    "plt.savefig('reward_loss.png')\n",
    "\n",
    "print(\"Closing up the arena...\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "9650cb4e16cdd4a8e8e2d128bf38d875813998db22a3c986335f89e0cb4d7bb2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
