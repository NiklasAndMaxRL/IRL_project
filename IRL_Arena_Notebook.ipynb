{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b95ef11a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "from typing import Any, List, Callable\n",
    "\n",
    "from GridWorld_environments import Grid_World\n",
    "from RL_agents import ValueIterationAgent, QLearningAgent\n",
    "from IRL_agents import IRL_from_sampled_trajectories\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c7fa1e9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def train_value_iteration(gw_env: Grid_World, verbose=False):\n",
    "    vi_agent = ValueIterationAgent(states=gw_env.get_state_space(),\n",
    "                                   terminal_states=gw_env.get_terminal_states(),\n",
    "                                   reward_function=gw_env.get_reward_func(),\n",
    "                                   actions=gw_env.get_action_space(),\n",
    "                                   gamma=GAMMA)\n",
    "\n",
    "    iters = 0\n",
    "    while iters < VALUE_ITERATION_TRAINING_N and not vi_agent.converged:\n",
    "\n",
    "        for state in gw_env.get_state_space():\n",
    "\n",
    "            if state in gw_env.get_terminal_states():\n",
    "                continue\n",
    "\n",
    "            opt_act = vi_agent.get_optimal_action(action_state_pairs=gw_env.get_action_state_pairs(state=state))\n",
    "            next_state = gw_env.get_new_state_on_action(old_state=state, action=opt_act)\n",
    "            next_state_value = vi_agent.get_state_value(state=next_state)\n",
    "\n",
    "            vi_agent.set_state_value(state=state, new_value=(gw_env.get_state_reward(state=next_state) + GAMMA * next_state_value))\n",
    "\n",
    "        iters += 1\n",
    "        # print(f\"Iteration {iters}\")\n",
    "        # print(vi_agent.get_value_function())\n",
    "\n",
    "    # print(\"Board:\")\n",
    "    # print(gw_env.get_board())\n",
    "\n",
    "    if verbose:\n",
    "        gw_env.display_value_function(value_func=vi_agent.get_value_function())\n",
    "\n",
    "    vi_agent.construct_policy(gw_env.get_action_state_pairs)\n",
    "\n",
    "    if verbose:\n",
    "        gw_env.display_policy(policy=vi_agent.get_policy())\n",
    "\n",
    "    return vi_agent.get_policy()\n",
    "\n",
    "\n",
    "def train_q_learning(gw_env: Grid_World, verbose=False):\n",
    "    ql_agent = QLearningAgent(states=gw_env.get_state_space(),\n",
    "                              terminal_states=gw_env.get_terminal_states(),\n",
    "                              reward_function=gw_env.get_reward_func(),\n",
    "                              actions=gw_env.get_action_space(),\n",
    "                              gamma=GAMMA)\n",
    "\n",
    "    iters = 0\n",
    "    while iters < VALUE_ITERATION_TRAINING_N and not ql_agent.converged:\n",
    "\n",
    "        for state in gw_env.get_state_space():\n",
    "\n",
    "            if state in gw_env.get_terminal_states():\n",
    "                continue\n",
    "\n",
    "            opt_act = ql_agent.get_optimal_action(state, action_state_pairs=gw_env.get_action_state_pairs(state=state))\n",
    "            next_state = gw_env.get_new_state_on_action(old_state=state, action=opt_act)\n",
    "            next_q_value = ql_agent.get_state_action_value(state=next_state, action=opt_act)\n",
    "\n",
    "            ql_agent.set_state_action_value(state=state, action=opt_act, new_value=(gw_env.get_state_reward(state=next_state) + GAMMA * next_q_value))\n",
    "\n",
    "        iters += 1\n",
    "        # print(f\"Iteration {iters}\")\n",
    "        # print(vi_agent.get_value_function())\n",
    "\n",
    "    # print(\"Board:\")\n",
    "    # print(gw_env.get_board())\n",
    "\n",
    "    if verbose:\n",
    "        gw_env.display_q_function(q_func=ql_agent.get_Q_function())\n",
    "\n",
    "    ql_agent.construct_policy(gw_env.get_action_state_pairs)\n",
    "\n",
    "    if verbose:\n",
    "        gw_env.display_policy(policy=ql_agent.get_policy())\n",
    "\n",
    "    return ql_agent.get_policy()\n",
    "\n",
    "\n",
    "def irl_reward_estimation(env: Grid_World, optimal_trajectories: List[List[Any]], train_func: Callable):\n",
    "\n",
    "    # prepare reference reward function\n",
    "    reward_func_ref = deepcopy(env.get_board())\n",
    "    reward_func_preds = []\n",
    "    print('reward_func_ref\\n', reward_func_ref)\n",
    "\n",
    "    minmax_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "    reward_func_ref_shape = reward_func_ref.shape\n",
    "    reward_func_ref = minmax_scaler.fit_transform(reward_func_ref.reshape(-1, 1)).reshape(reward_func_ref_shape)\n",
    "    print('MinMax scaled reward_func_ref:\\n', reward_func_ref)\n",
    "\n",
    "    irl_agent = IRL_from_sampled_trajectories(d=(GW_SIZE[0] * 4, GW_SIZE[1] * 4),\n",
    "                                              env_ranges=((0, GW_SIZE[0]), (0, GW_SIZE[1])),\n",
    "                                              env_discrete_size=GW_SIZE,\n",
    "                                              penalty_factor=2,\n",
    "                                              gamma=GAMMA)\n",
    "\n",
    "    # step 2: given optimal trajectories, compute the value estimate\n",
    "    optimal_value_estimate = irl_agent.compute_value_estimate(trajs=optimal_trajectories)\n",
    "\n",
    "    candidate_policies = [env.construct_random_policy()]\n",
    "    candidate_value_estimates = []\n",
    "\n",
    "    # while True:\n",
    "    for i in range(IRL_TRAINING_N):\n",
    "        print(f\"Iteration {i}...\")\n",
    "\n",
    "        # step 3: generate trajectories and compute the value estimate for a random policy\n",
    "        candidate_trajectories = env.generate_trajectories(policy=candidate_policies[-1],\n",
    "                                                           n_traj=NUMBER_OF_TRAJECTORIES,\n",
    "                                                           max_traj_length=MAXIMUM_TRAJECTORY_LENGTH)\n",
    "        candidate_value_estimates.append(irl_agent.compute_value_estimate(trajs=candidate_trajectories))\n",
    "\n",
    "        # step 4: obtain new alphas\n",
    "        irl_agent.solve_lp(optimal_value_estimate, candidate_value_estimates)\n",
    "\n",
    "        # step 5: construct new reward function from the alphas\n",
    "        reward_func = irl_agent.construct_reward_function(alphas=irl_agent.get_alphas())\n",
    "\n",
    "        # step 6: find optimal policy under new reward function and add to 'candidate_policies' list\n",
    "        env.set_reward_func(reward_func)\n",
    "        print(\"Latest non-Scaled reward func:\\n\", env.get_board())\n",
    "        scaled_board = minmax_scaler.fit_transform(env.get_board().reshape(-1, 1)).reshape(env.get_board().shape)\n",
    "        env.set_board(new_board=scaled_board)\n",
    "\n",
    "        reward_func_preds.append(env.get_board())\n",
    "\n",
    "        candidate_policies.append(train_func(gw_env=env, verbose=True))  # train_value_iteration(gw_env=env))\n",
    "\n",
    "        print(\"Latest Scaled reward func:\\n\", reward_func_preds[-1])\n",
    "        env.display_policy(policy=candidate_policies[-1])\n",
    "        print(\"============================================================\\n\" * 2)\n",
    "\n",
    "    # print('reward_func_pred \\n', [np.array(reward_func_pred).flatten() for reward_func_pred in reward_func_preds]) #[np.array(one_candidate_value_estimates).flatten().shape for one_candidate_value_estimates in candidate_value_estimates ] )\n",
    "    # print('reward_func_ref \\n', np.array(reward_func_ref).flatten())\n",
    "    # vec1 = [np.array(reward_func_pred).flatten() for reward_func_pred in reward_func_preds]\n",
    "    # vec2 = np.array(reward_func_ref).flatten()\n",
    "    # print('l2-loss', np.linalg.norm(vec1[0] - vec2))\n",
    "    #reward_loss = [ np.linalg.norm(np.array(reward_func_ref).flatten() - np.array(reward_func_pred).flatten()) for reward_func_pred in reward_func_preds ]\n",
    "\n",
    "    #value_loss = [ calc_value_distance(optimal_value_estimate, one_candidate_value_estimates) for one_candidate_value_estimates in candidate_value_estimates ]\n",
    "    # plt.plot(reward_loss)\n",
    "    # plt.show()\n",
    "\n",
    "    return {'reference_reward_func': reward_func_ref, 'policy_pred': np.mean(np.array([ np_normalize(list(pol.values()), 1) for pol in candidate_policies ]), axis=0), 'avg_predicted_reward_func': np.mean(np.array(reward_func_preds), axis=0)}\n",
    "\n",
    "\n",
    "def calc_value_distance(value_estimates_ref, value_estimates_pred):\n",
    "    return np.linalg.norm(np.array(value_estimates_ref) - np.array(value_estimates_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8ea0f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.95\n",
    "VALUE_ITERATION_TRAINING_N = 25\n",
    "IRL_TRAINING_N = 5\n",
    "\n",
    "NUMBER_OF_TRAJECTORIES = 40\n",
    "MAXIMUM_TRAJECTORY_LENGTH = 50\n",
    "\n",
    "GW_SIZE = (4, 6)\n",
    "GW_SIZES = [(4, 6)]  # [(x, x) for x in np.arange(5,11, 5)]\n",
    "GW_TRAPS = [(1, 2)]\n",
    "GW_GOALS = [(3, 5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46913987",
   "metadata": {},
   "source": [
    "### Step-by-step code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90070f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = Grid_World(size=GW_SIZE, traps=GW_TRAPS, goals=GW_GOALS, randomize_board=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "176c8196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function:\n",
      "[[0.6983373  0.73509189 0.77378094 0.81450625 0.857375   0.9025    ]\n",
      " [0.73509189 0.77378094 0.         0.857375   0.9025     0.95      ]\n",
      " [0.77378094 0.81450625 0.857375   0.9025     0.95       1.        ]\n",
      " [0.81450625 0.857375   0.9025     0.95       1.         0.        ]]\n",
      "Policy:\n",
      "[['v' 'v' '>' 'v' 'v' 'v']\n",
      " ['v' 'v' 'x' 'v' 'v' 'v']\n",
      " ['v' 'v' 'v' 'v' 'v' 'v']\n",
      " ['>' '>' '>' '>' '>' 'x']]\n"
     ]
    }
   ],
   "source": [
    "vi_greedy_policy = train_value_iteration(gw_env=environment, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "585078bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q function:\n",
      "[[[ 0.66342043  0.6983373   0.66342043  0.66342043]\n",
      "  [ 0.6983373   0.73509189  0.6983373   0.66342043]\n",
      "  [-1.          0.77378094  0.73509189  0.6983373 ]\n",
      "  [ 0.77378094  0.81450625  0.77378094  0.73509189]\n",
      "  [ 0.81450625  0.857375    0.81450625  0.77378094]\n",
      "  [ 0.9025      0.857375    0.857375    0.81450625]]\n",
      "\n",
      " [[ 0.6983373   0.6983373   0.66342043  0.6983373 ]\n",
      "  [ 0.73509189 -1.          0.6983373   0.66342043]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.81450625  0.857375    0.77378094 -1.        ]\n",
      "  [ 0.857375    0.9025      0.81450625  0.81450625]\n",
      "  [ 0.95        0.9025      0.857375    0.857375  ]]\n",
      "\n",
      " [[ 0.73509189  0.77378094  0.66342043  0.73509189]\n",
      "  [ 0.77378094  0.81450625  0.6983373   0.73509189]\n",
      "  [ 0.81450625  0.857375   -1.          0.77378094]\n",
      "  [ 0.857375    0.9025      0.81450625  0.81450625]\n",
      "  [ 0.9025      0.95        0.857375    0.857375  ]\n",
      "  [ 1.          0.95        0.9025      0.9025    ]]\n",
      "\n",
      " [[ 0.77378094  0.81450625  0.73509189  0.77378094]\n",
      "  [ 0.81450625  0.857375    0.77378094  0.77378094]\n",
      "  [ 0.857375    0.9025      0.81450625  0.81450625]\n",
      "  [ 0.9025      0.95        0.857375    0.857375  ]\n",
      "  [ 0.95        1.          0.9025      0.9025    ]\n",
      "  [ 0.          0.          0.          0.        ]]]\n",
      "Policy:\n",
      "[['>' '>' '>' 'v' 'v' 'v']\n",
      " ['v' 'v' 'x' 'v' 'v' 'v']\n",
      " ['v' 'v' 'v' 'v' 'v' 'v']\n",
      " ['>' '>' '>' '>' '>' 'x']]\n"
     ]
    }
   ],
   "source": [
    "ql_greedy_policy = train_q_learning(gw_env=environment, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8810805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 40 trajectories...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Generating {NUMBER_OF_TRAJECTORIES} trajectories...\")\n",
    "\n",
    "greedy_policy = vi_greedy_policy\n",
    "# greedy_policy = ql_greedy_policy\n",
    "\n",
    "trajectories = environment.generate_trajectories(policy=greedy_policy,\n",
    "                                                 n_traj=NUMBER_OF_TRAJECTORIES,\n",
    "                                                 max_traj_length=MAXIMUM_TRAJECTORY_LENGTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e877c596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IRL from samples...\n",
      "reward_func_ref\n",
      " [[ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.]]\n",
      "MinMax scaled reward_func_ref:\n",
      " [[ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.]]\n",
      "Iteration 0...\n",
      "Latest non-Scaled reward func:\n",
      " [[-3.98524428 -5.21657998 -6.05985019 -6.30720925 -5.89911348 -4.93425745]\n",
      " [-4.91908932 -6.43895859 -7.47982866 -7.78515029 -7.28142721 -6.09048064]\n",
      " [-5.2084129  -6.81767555 -7.91976594 -8.24304552 -7.70969522 -6.44870135]\n",
      " [-4.73488853 -6.1978446  -7.19973812 -7.49362664 -7.00876604 -5.8624158 ]]\n",
      "Value function:\n",
      "[[15.28210306 15.51799791 15.1637076  14.43102586 13.61878722 13.03885371]\n",
      " [15.51799791 15.3034467   0.         13.61878722 13.03885371 12.94113485]\n",
      " [15.3034467  14.96372025 13.88506759 12.34266704 11.83860816 12.30519382]\n",
      " [14.96372025 14.86340683 14.08092057 12.86694316 11.57561755  0.        ]]\n",
      "Policy:\n",
      "[['<' '<' '<' '<' '<' '<']\n",
      " ['^' '<' 'x' '^' '^' '^']\n",
      " ['^' '<' '<' '<' '^' '^']\n",
      " ['^' '<' '<' '<' '<' 'x']]\n",
      "Latest Scaled reward func:\n",
      " [[ 1.          0.42160959  0.02550364 -0.09068735  0.10100585  0.55422383]\n",
      " [ 0.56134869 -0.15257344 -0.64149719 -0.7849147  -0.54830287  0.01111572]\n",
      " [ 0.42544588 -0.33046665 -0.84814717 -1.         -0.74947149 -0.15714987]\n",
      " [ 0.6478726  -0.03931592 -0.50993138 -0.64797846 -0.42022682  0.11824371]]\n",
      "Policy:\n",
      "[['<' '<' '<' '<' '<' '<']\n",
      " ['^' '<' 'x' '^' '^' '^']\n",
      " ['^' '<' '<' '<' '^' '^']\n",
      " ['^' '<' '<' '<' '<' 'x']]\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "Iteration 1...\n",
      "Latest non-Scaled reward func:\n",
      " [[-3.97073259 -5.17776365 -5.97459484 -6.15345745 -5.67145338 -4.657503  ]\n",
      " [-4.89326395 -6.36999778 -7.32861978 -7.51290851 -6.8789753  -5.60201955]\n",
      " [-5.17064682 -6.71699386 -7.69935912 -7.8468482  -7.1249165  -5.74003414]\n",
      " [-4.68951173 -6.07706037 -6.93572966 -7.01977188 -6.3104109  -5.01735003]]\n",
      "Value function:\n",
      "[[15.15095456 15.39340683 15.00093203 14.21693162 13.37984161 12.8333108 ]\n",
      " [15.39340683 15.14772832  0.         13.37984161 12.8333108  12.83728513]\n",
      " [15.14772832 14.77120958 13.61563188 12.01095177 11.69104878 12.35370864]\n",
      " [14.77120958 14.66177309 13.84186041 12.61988677 11.41564768  0.        ]]\n",
      "Policy:\n",
      "[['<' '<' '<' '<' '<' '>']\n",
      " ['^' '<' 'x' '^' '^' '^']\n",
      " ['^' '<' '<' '<' '>' '^']\n",
      " ['^' '<' '<' '<' '<' 'x']]\n",
      "Latest Scaled reward func:\n",
      " [[ 1.          0.37719553 -0.03395381 -0.12624342  0.12246127  0.64563987]\n",
      " [ 0.52399182 -0.23797401 -0.73260425 -0.82769364 -0.50059648  0.15828777]\n",
      " [ 0.38086768 -0.41701721 -0.92389852 -1.         -0.62749733  0.08707493]\n",
      " [ 0.62912399 -0.08682402 -0.52988062 -0.57324476 -0.20722834  0.45996583]]\n",
      "Policy:\n",
      "[['<' '<' '<' '<' '<' '>']\n",
      " ['^' '<' 'x' '^' '^' '^']\n",
      " ['^' '<' '<' '<' '>' '^']\n",
      " ['^' '<' '<' '<' '<' 'x']]\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "Iteration 2...\n",
      "Latest non-Scaled reward func:\n",
      " [[-3.96161884 -5.15605305 -5.9321399  -6.08530517 -5.58164048 -4.56033794]\n",
      " [-4.87675919 -6.33078762 -7.25215816 -7.3905141  -6.71814527 -5.42853087]\n",
      " [-5.14609753 -6.65882641 -7.58623719 -7.66627222 -6.88830425 -5.4855309 ]\n",
      " [-4.65952394 -6.00618778 -6.79826061 -6.80092143 -6.02443692 -4.71061718]]\n",
      "Value function:\n",
      "[[15.09306623 15.33841292 14.92666314 14.11652139 13.26419867 12.72640131]\n",
      " [15.33841292 15.07744316  0.         13.26419867 12.72640131 12.7668558 ]\n",
      " [15.07744316 14.68411648 13.49379193 11.86231017 11.72571809 12.33658357]\n",
      " [14.68411648 14.57313851 13.74069737 12.52226861 11.36332482  0.        ]]\n",
      "Policy:\n",
      "[['<' '<' '<' '<' '<' '>']\n",
      " ['^' '<' 'x' '^' '^' '^']\n",
      " ['^' '<' '<' '<' '>' '^']\n",
      " ['^' '<' '<' '<' '<' 'x']]\n",
      "Latest Scaled reward func:\n",
      " [[ 1.          0.35517087 -0.0638086  -0.14649664  0.12541257  0.67677456]\n",
      " [ 0.50595089 -0.27902319 -0.77643574 -0.85112879 -0.4881427   0.20807056]\n",
      " [ 0.36054547 -0.45611872 -0.95679216 -1.         -0.58000499  0.17729843]\n",
      " [ 0.62322785 -0.10378421 -0.53139389 -0.53283036 -0.11363621  0.59564458]]\n",
      "Policy:\n",
      "[['<' '<' '<' '<' '<' '>']\n",
      " ['^' '<' 'x' '^' '^' '^']\n",
      " ['^' '<' '<' '<' '>' '^']\n",
      " ['^' '<' '<' '<' '<' 'x']]\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "Iteration 3...\n",
      "Latest non-Scaled reward func:\n",
      " [[-3.95069301 -5.13053842 -5.88303175 -6.00738305 -5.47968606 -4.45032018]\n",
      " [-4.85755439 -6.2861795  -7.16677515 -7.25580082 -6.54290119 -5.24052846]\n",
      " [-5.11833448 -6.59467683 -7.46411755 -7.47467782 -6.64049701 -5.22123491]\n",
      " [-4.62651845 -5.93031273 -6.65458671 -6.57675339 -5.73614786 -4.40493542]]\n",
      "Value function:\n",
      "[[15.02933195 15.27786535 14.8443634  14.00546698 13.13794113 12.61328037]\n",
      " [15.27786535 14.99929258  0.         13.13794113 12.61328037 12.69905828]\n",
      " [14.99929258 14.58664548 13.35674839 11.69490434 11.82082671 12.33207301]\n",
      " [14.58664548 14.47375569 13.62655584 12.41066201 11.29973634  0.        ]]\n",
      "Policy:\n",
      "[['<' '<' '<' '<' '<' '>']\n",
      " ['^' '<' 'x' '^' '^' '^']\n",
      " ['^' '<' '<' '<' '>' '^']\n",
      " ['^' '<' '<' '<' '<' 'x']]\n",
      "Latest Scaled reward func:\n",
      " [[ 1.          0.33039132 -0.09667825 -0.1672525   0.1322363   0.71644193]\n",
      " [ 0.4853205  -0.32548045 -0.82525312 -0.87577869 -0.47118011  0.26796764]\n",
      " [ 0.33731753 -0.50056482 -0.99400663 -1.         -0.52656958  0.27891749]\n",
      " [ 0.61644248 -0.12351206 -0.53456603 -0.49039257 -0.01331586  0.74219956]]\n",
      "Policy:\n",
      "[['<' '<' '<' '<' '<' '>']\n",
      " ['^' '<' 'x' '^' '^' '^']\n",
      " ['^' '<' '<' '<' '>' '^']\n",
      " ['^' '<' '<' '<' '<' 'x']]\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "Iteration 4...\n",
      "Latest non-Scaled reward func:\n",
      " [[-3.94592231 -5.12163659 -5.87008067 -5.99326665 -5.46936354 -4.44767206]\n",
      " [-4.84819904 -6.26836558 -7.13997164 -7.22470039 -6.51657822 -5.2269869 ]\n",
      " [-5.10344354 -6.56588675 -7.41973825 -7.42100017 -6.59116509 -5.18946631]\n",
      " [-4.60724089 -5.89260375 -6.59540668 -6.50305538 -5.6647831  -4.35349175]]\n",
      "Value function:\n",
      "[[15.01614254 15.26533541 14.82541371 13.97673857 13.09960036 12.56783928]\n",
      " [15.26533541 14.98278432  0.         13.09960036 12.56783928 12.65067695]\n",
      " [14.98278432 14.56746076 13.3312283  11.66539315 11.77710003 12.28085626]\n",
      " [14.56746076 14.45848124 13.61519008 12.40958165 11.31740427  0.        ]]\n",
      "Policy:\n",
      "[['<' '<' '<' '<' '<' '>']\n",
      " ['^' '<' 'x' '^' '^' '^']\n",
      " ['^' '<' '<' '<' '>' '^']\n",
      " ['^' '<' '<' '<' '<' 'x']]\n",
      "Latest Scaled reward func:\n",
      " [[ 1.          0.32334507 -0.10740446 -0.17830128  0.12321893  0.71122963]\n",
      " [ 0.48071567 -0.33662805 -0.83826059 -0.88702424 -0.47948104  0.26271316]\n",
      " [ 0.33381566 -0.50785942 -0.99927373 -1.         -0.52240778  0.28430726]\n",
      " [ 0.61939352 -0.12036709 -0.52484893 -0.47169829  0.01074977  0.76543291]]\n",
      "Policy:\n",
      "[['<' '<' '<' '<' '<' '>']\n",
      " ['^' '<' 'x' '^' '^' '^']\n",
      " ['^' '<' '<' '<' '>' '^']\n",
      " ['^' '<' '<' '<' '<' 'x']]\n",
      "============================================================\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np_normalize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m train_func \u001b[38;5;241m=\u001b[39m train_value_iteration\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# train_func = train_q_learning\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m estimated_rewards \u001b[38;5;241m=\u001b[39m \u001b[43mirl_reward_estimation\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimal_trajectories\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrajectories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m ref_reward_funcs\u001b[38;5;241m.\u001b[39mappend(estimated_rewards[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreference_reward_func\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      8\u001b[0m avg_pred_reward_funcs\u001b[38;5;241m.\u001b[39mappend(estimated_rewards[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_predicted_reward_func\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mirl_reward_estimation\u001b[0;34m(env, optimal_trajectories, train_func)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m============================================================\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# print('reward_func_pred \\n', [np.array(reward_func_pred).flatten() for reward_func_pred in reward_func_preds]) #[np.array(one_candidate_value_estimates).flatten().shape for one_candidate_value_estimates in candidate_value_estimates ] )\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# print('reward_func_ref \\n', np.array(reward_func_ref).flatten())\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# vec1 = [np.array(reward_func_pred).flatten() for reward_func_pred in reward_func_preds]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# plt.plot(reward_loss)\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreference_reward_func\u001b[39m\u001b[38;5;124m'\u001b[39m: reward_func_ref, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpolicy_pred\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39marray([ np_normalize(\u001b[38;5;28mlist\u001b[39m(pol\u001b[38;5;241m.\u001b[39mvalues()), \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m pol \u001b[38;5;129;01min\u001b[39;00m candidate_policies ]), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_predicted_reward_func\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39marray(reward_func_preds), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)}\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m============================================================\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# print('reward_func_pred \\n', [np.array(reward_func_pred).flatten() for reward_func_pred in reward_func_preds]) #[np.array(one_candidate_value_estimates).flatten().shape for one_candidate_value_estimates in candidate_value_estimates ] )\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# print('reward_func_ref \\n', np.array(reward_func_ref).flatten())\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# vec1 = [np.array(reward_func_pred).flatten() for reward_func_pred in reward_func_preds]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# plt.plot(reward_loss)\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreference_reward_func\u001b[39m\u001b[38;5;124m'\u001b[39m: reward_func_ref, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpolicy_pred\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39marray([ \u001b[43mnp_normalize\u001b[49m(\u001b[38;5;28mlist\u001b[39m(pol\u001b[38;5;241m.\u001b[39mvalues()), \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m pol \u001b[38;5;129;01min\u001b[39;00m candidate_policies ]), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_predicted_reward_func\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39marray(reward_func_preds), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np_normalize' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"IRL from samples...\")\n",
    "\n",
    "train_func = train_value_iteration\n",
    "# train_func = train_q_learning\n",
    "\n",
    "estimated_rewards = irl_reward_estimation(env=environment, optimal_trajectories=trajectories, train_func=train_func)\n",
    "ref_reward_funcs.append(estimated_rewards['reference_reward_func'])\n",
    "avg_pred_reward_funcs.append(estimated_rewards['avg_predicted_reward_func'])\n",
    "# Using default value for reward loss -> Frobenius for matrices and L2-loss for vectors\n",
    "reward_loss.append(np.linalg.norm(estimated_rewards['reference_reward_func'] - estimated_rewards['avg_predicted_reward_func']))\n",
    "# Using L1-Loss for policy loss as described by Ng and Russel in 2000\n",
    "policy_loss.append(np.linalg.norm(estimated_rewards['policy_pred'] - np.array(list(greedy_policy.values())), ord=1 ))\n",
    "\n",
    "print('**********************************************')\n",
    "print('*****************REWARD LOSS******************')\n",
    "print(reward_loss)\n",
    "print('**********************************************')\n",
    "print('*****************POLICY LOSS*************************')\n",
    "print(policy_loss)\n",
    "print('**********************************************')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58d7f3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bed02e3e",
   "metadata": {},
   "source": [
    "### End-to-end loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0042f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configuration is:\n",
      "training: value iteration\n",
      "generating trajectories\n",
      "inverse reinforcment learning\n",
      "\n",
      "Training via value iteration...\n",
      "Value function:\n",
      "[[0.6983373  0.73509189 0.77378094 0.81450625 0.857375   0.9025    ]\n",
      " [0.73509189 0.77378094 0.         0.857375   0.9025     0.95      ]\n",
      " [0.77378094 0.81450625 0.857375   0.9025     0.95       1.        ]\n",
      " [0.81450625 0.857375   0.9025     0.95       1.         0.        ]]\n",
      "Policy:\n",
      "[['v' 'v' '>' 'v' 'v' 'v']\n",
      " ['v' 'v' 'x' 'v' 'v' 'v']\n",
      " ['v' 'v' 'v' 'v' 'v' 'v']\n",
      " ['>' '>' '>' '>' '>' 'x']]\n",
      "Generating 40 trajectories...\n",
      "IRL from samples...\n",
      "reward_func_ref\n",
      " [[ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.]]\n",
      "MinMax scaled reward_func_ref:\n",
      " [[ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0. -1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.]]\n",
      "Iteration 0...\n",
      "Latest non-Scaled reward func:\n",
      " [[-3.98524428 -5.21657998 -6.05985019 -6.30720925 -5.89911348 -4.93425745]\n",
      " [-4.91908932 -6.43895859 -7.47982866 -7.78515029 -7.28142721 -6.09048064]\n",
      " [-5.2084129  -6.81767555 -7.91976594 -8.24304552 -7.70969522 -6.44870135]\n",
      " [-4.73488853 -6.1978446  -7.19973812 -7.49362664 -7.00876604 -5.8624158 ]]\n",
      "Value function:\n",
      "[[15.28210306 15.51799791 15.1637076  14.43102586 13.61878722 13.03885371]\n",
      " [15.51799791 15.3034467   0.         13.61878722 13.03885371 12.94113485]\n",
      " [15.3034467  14.96372025 13.88506759 12.34266704 11.83860816 12.30519382]\n",
      " [14.96372025 14.86340683 14.08092057 12.86694316 11.57561755  0.        ]]\n",
      "Policy:\n",
      "[['<' '<' '<' '<' '<' '<']\n",
      " ['^' '<' 'x' '^' '^' '^']\n",
      " ['^' '<' '<' '<' '^' '^']\n",
      " ['^' '<' '<' '<' '<' 'x']]\n",
      "Latest Scaled reward func:\n",
      " [[ 1.          0.42160959  0.02550364 -0.09068735  0.10100585  0.55422383]\n",
      " [ 0.56134869 -0.15257344 -0.64149719 -0.7849147  -0.54830287  0.01111572]\n",
      " [ 0.42544588 -0.33046665 -0.84814717 -1.         -0.74947149 -0.15714987]\n",
      " [ 0.6478726  -0.03931592 -0.50993138 -0.64797846 -0.42022682  0.11824371]]\n",
      "Policy:\n",
      "[['<' '<' '<' '<' '<' '<']\n",
      " ['^' '<' 'x' '^' '^' '^']\n",
      " ['^' '<' '<' '<' '^' '^']\n",
      " ['^' '<' '<' '<' '<' 'x']]\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "Iteration 1...\n",
      "Latest non-Scaled reward func:\n",
      " [[-3.97697504 -5.19443372 -6.0111669  -6.21937025 -5.76903551 -4.77616629]\n",
      " [-4.90385301 -6.3981925  -7.39029724 -7.62375402 -7.04262684 -5.80049322]\n",
      " [-5.18539328 -6.75614252 -7.78474922 -7.99986906 -7.35020021 -6.01250561]\n",
      " [-4.70637122 -6.12168636 -7.0327807  -7.1931839  -6.56498543 -5.32438624]]\n",
      "Value function:\n",
      "[[15.20321202 15.44305142 15.06563375 14.30104436 13.4711752  12.90668545]\n",
      " [15.44305142 15.21009727  0.         13.4711752  12.90668545 12.86402963]\n",
      " [15.21009727 14.8488218  13.724705   12.14541755 11.73724849 12.31425782]\n",
      " [14.8488218  14.74375809 13.94031723 12.72409374 11.48893624  0.        ]]\n",
      "Policy:\n",
      "[['<' '<' '<' '<' '<' '<']\n",
      " ['^' '<' 'x' '^' '^' '^']\n",
      " ['^' '<' '<' '<' '^' '^']\n",
      " ['^' '<' '<' '<' '<' 'x']]\n",
      "Latest Scaled reward func:\n",
      " [[ 1.          0.3947349  -0.0113077  -0.11481694  0.10906901  0.60267845]\n",
      " [ 0.53919842 -0.20371924 -0.69694861 -0.8130127  -0.52410269  0.09342967]\n",
      " [ 0.3992294  -0.38167571 -0.8930522  -1.         -0.67701419 -0.01197325]\n",
      " [ 0.63737738 -0.06625296 -0.51920764 -0.59895281 -0.2866411   0.33012841]]\n",
      "Policy:\n",
      "[['<' '<' '<' '<' '<' '<']\n",
      " ['^' '<' 'x' '^' '^' '^']\n",
      " ['^' '<' '<' '<' '^' '^']\n",
      " ['^' '<' '<' '<' '<' 'x']]\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "Iteration 2...\n",
      "Latest non-Scaled reward func:\n",
      " [[-3.92458397 -5.06668451 -5.75341484 -5.78913326 -5.17508791 -4.09828064]\n",
      " [-4.81525123 -6.18397821 -6.96175267 -6.91442678 -6.07141491 -4.70080977]\n",
      " [-5.06157758 -6.45925141 -7.19574037 -7.03303322 -6.03725943 -4.53779172]\n",
      " [-4.56346305 -5.78172301 -6.36376131 -6.10395577 -5.0978564  -3.68969373]]\n",
      "Value function:\n",
      "[[12.84878203 13.07235142 12.63323901 11.82434166 11.79345669 11.79345669]\n",
      " [13.07235142 12.77666738  0.         11.03551385 11.62699972 11.97070829]\n",
      " [12.77666738 12.35525238 11.15761478 10.48464307 11.56185812 11.79538874]\n",
      " [12.35525238 12.2390541  11.4337178  10.3366286  10.64461263  0.        ]]\n",
      "Policy:\n",
      "[['<' '<' '<' '<' '>' '>']\n",
      " ['^' '<' 'x' '^' '>' '^']\n",
      " ['^' '<' '<' '>' '>' '^']\n",
      " ['^' '<' '<' '<' '^' 'x']]\n",
      "Latest Scaled reward func:\n",
      " [[ 0.86600849  0.21450516 -0.1772354  -0.19761072  0.15266719  0.76692443]\n",
      " [ 0.35793352 -0.42284729 -0.86652334 -0.83952662 -0.35863634  0.42321586]\n",
      " [ 0.21741837 -0.57987498 -1.         -0.90718483 -0.33915258  0.51620838]\n",
      " [ 0.50156435 -0.1933836  -0.52540331 -0.37719904  0.19672336  1.        ]]\n",
      "Policy:\n",
      "[['<' '<' '<' '<' '>' '>']\n",
      " ['^' '<' 'x' '^' '>' '^']\n",
      " ['^' '<' '<' '>' '>' '^']\n",
      " ['^' '<' '<' '<' '^' 'x']]\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "Iteration 3...\n",
      "Latest non-Scaled reward func:\n",
      " [[-3.96830006 -5.17516579 -5.97621058 -6.16762922 -5.70664658 -4.71500534]\n",
      " [-4.88736618 -6.36153219 -7.32368193 -7.52493919 -6.92312814 -5.68288424]\n",
      " [-5.15974563 -6.69905673 -7.68087647 -7.8454971  -7.16303811 -5.82766904]\n",
      " [-4.67371066 -6.04893138 -6.90023954 -6.99588249 -6.32524127 -5.08690902]]\n",
      "Value function:\n",
      "[[15.15711617 15.39926037 15.00675188 14.22066062 13.3751332  12.80967384]\n",
      " [15.39926037 15.15520944  0.         13.3751332  12.80967384 12.78401226]\n",
      " [15.15520944 14.78285778 13.63509075 12.03825356 11.64498178 12.26036644]\n",
      " [14.78285778 14.67983832 13.87258069 12.66655007 11.47148485  0.        ]]\n",
      "Policy:\n",
      "[['<' '<' '<' '<' '<' '<']\n",
      " ['^' '<' 'x' '^' '^' '^']\n",
      " ['^' '<' '<' '<' '>' '^']\n",
      " ['^' '<' '<' '<' '<' 'x']]\n",
      "Latest Scaled reward func:\n",
      " [[ 1.          0.37745453 -0.03575366 -0.1344944   0.1032973   0.61482211]\n",
      " [ 0.52591209 -0.23451664 -0.73082866 -0.83464451 -0.52420836  0.11555479]\n",
      " [ 0.38540881 -0.40862414 -0.91508266 -1.         -0.6479627   0.0408695 ]\n",
      " [ 0.63612342 -0.07326571 -0.51240159 -0.56173772 -0.21579645  0.4229806 ]]\n",
      "Policy:\n",
      "[['<' '<' '<' '<' '<' '<']\n",
      " ['^' '<' 'x' '^' '^' '^']\n",
      " ['^' '<' '<' '<' '>' '^']\n",
      " ['^' '<' '<' '<' '<' 'x']]\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "Iteration 4...\n",
      "Latest non-Scaled reward func:\n",
      " [[-3.94914228 -5.12999245 -5.88792741 -6.02464501 -5.51478447 -4.50180492]\n",
      " [-4.8534198  -6.28189038 -7.16880708 -7.27530655 -6.58969558 -5.31396535]\n",
      " [-5.11037774 -6.58382014 -7.45790589 -7.48786294 -6.68759166 -5.30395822]\n",
      " [-4.61478547 -5.91208345 -6.63679355 -6.57542751 -5.76896294 -4.47695862]]\n",
      "Value function:\n",
      "[[15.03965345 15.28767077 14.85589905 14.01734927 13.14345746 12.6014212 ]\n",
      " [15.28767077 15.01221128  0.         13.14345746 12.6014212  12.65899847]\n",
      " [15.01221128 14.6052983  13.38597669 11.73360886 11.70303993 12.2546833 ]\n",
      " [14.6052983  14.49882783 13.66447919 12.46225923 11.35483288  0.        ]]\n",
      "Policy:\n",
      "[['<' '<' '<' '<' '<' '>']\n",
      " ['^' '<' 'x' '^' '^' '^']\n",
      " ['^' '<' '<' '<' '>' '^']\n",
      " ['^' '<' '<' '<' '<' 'x']]\n",
      "Latest Scaled reward func:\n",
      " [[ 1.          0.33261182 -0.09575483 -0.17302434  0.11513661  0.68764834]\n",
      " [ 0.48892404 -0.31841325 -0.81967728 -0.87986823 -0.49237736  0.22863475]\n",
      " [ 0.34369759 -0.4890567  -0.98306899 -1.         -0.54770588  0.23429054]\n",
      " [ 0.62379444 -0.10940725 -0.518996   -0.48431339 -0.02851897  0.70169087]]\n",
      "Policy:\n",
      "[['<' '<' '<' '<' '<' '>']\n",
      " ['^' '<' 'x' '^' '^' '^']\n",
      " ['^' '<' '<' '<' '>' '^']\n",
      " ['^' '<' '<' '<' '<' 'x']]\n",
      "============================================================\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np_normalize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 28>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m irl:\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIRL from samples...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 52\u001b[0m     estimated_rewards \u001b[38;5;241m=\u001b[39m \u001b[43mirl_reward_estimation\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimal_trajectories\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrajectories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m     ref_reward_funcs\u001b[38;5;241m.\u001b[39mappend(estimated_rewards[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreference_reward_func\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     54\u001b[0m     avg_pred_reward_funcs\u001b[38;5;241m.\u001b[39mappend(estimated_rewards[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_predicted_reward_func\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36mirl_reward_estimation\u001b[1;34m(env, optimal_trajectories, train_func)\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m============================================================\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# print('reward_func_pred \\n', [np.array(reward_func_pred).flatten() for reward_func_pred in reward_func_preds]) #[np.array(one_candidate_value_estimates).flatten().shape for one_candidate_value_estimates in candidate_value_estimates ] )\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# print('reward_func_ref \\n', np.array(reward_func_ref).flatten())\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# vec1 = [np.array(reward_func_pred).flatten() for reward_func_pred in reward_func_preds]\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# plt.plot(reward_loss)\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n\u001b[1;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreference_reward_func\u001b[39m\u001b[38;5;124m'\u001b[39m: reward_func_ref, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpolicy_pred\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39marray([ np_normalize(\u001b[38;5;28mlist\u001b[39m(pol\u001b[38;5;241m.\u001b[39mvalues()), \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m pol \u001b[38;5;129;01min\u001b[39;00m candidate_policies ]), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_predicted_reward_func\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39marray(reward_func_preds), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)}\n",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m============================================================\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# print('reward_func_pred \\n', [np.array(reward_func_pred).flatten() for reward_func_pred in reward_func_preds]) #[np.array(one_candidate_value_estimates).flatten().shape for one_candidate_value_estimates in candidate_value_estimates ] )\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# print('reward_func_ref \\n', np.array(reward_func_ref).flatten())\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# vec1 = [np.array(reward_func_pred).flatten() for reward_func_pred in reward_func_preds]\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# plt.plot(reward_loss)\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n\u001b[1;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreference_reward_func\u001b[39m\u001b[38;5;124m'\u001b[39m: reward_func_ref, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpolicy_pred\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39marray([ \u001b[43mnp_normalize\u001b[49m(\u001b[38;5;28mlist\u001b[39m(pol\u001b[38;5;241m.\u001b[39mvalues()), \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m pol \u001b[38;5;129;01min\u001b[39;00m candidate_policies ]), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_predicted_reward_func\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39marray(reward_func_preds), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)}\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np_normalize' is not defined"
     ]
    }
   ],
   "source": [
    "vi = True\n",
    "ql = False\n",
    "gt = True\n",
    "irl = True\n",
    "plt = False\n",
    "\n",
    "print(\"configuration is:\")\n",
    "if vi:\n",
    "    print(\"training: value iteration\")\n",
    "if ql:\n",
    "    print(\"training: Q-Learning\")\n",
    "if gt:\n",
    "    print(\"generating trajectories\")\n",
    "if irl:\n",
    "    print(\"inverse reinforcment learning\")\n",
    "if plt:\n",
    "    print(\"creating plots\")\n",
    "    \n",
    "print(\"\")\n",
    "\n",
    "#print(f\"Passed args: {args}\")\n",
    "\n",
    "ref_reward_funcs = []\n",
    "avg_pred_reward_funcs = []\n",
    "reward_loss = []\n",
    "policy_loss = []\n",
    "\n",
    "for GW_SIZE in GW_SIZES:\n",
    "    environment = Grid_World(size=GW_SIZE, traps=GW_TRAPS, goals=GW_GOALS, randomize_board=False)\n",
    "\n",
    "    train_func = train_value_iteration\n",
    "\n",
    "    if vi:\n",
    "        print(\"Training via value iteration...\")\n",
    "        greedy_policy = train_value_iteration(gw_env=environment, verbose=True)\n",
    "    elif ql:\n",
    "        print(\"Training via q-learning...\")\n",
    "        greedy_policy = train_q_learning(gw_env=environment, verbose=True)\n",
    "        train_func = train_q_learning\n",
    "    else:\n",
    "        # load from file (?)\n",
    "        greedy_policy = {}\n",
    "\n",
    "    if gt:\n",
    "        print(f\"Generating {NUMBER_OF_TRAJECTORIES} trajectories...\")\n",
    "        trajectories = environment.generate_trajectories(policy=greedy_policy,\n",
    "                                                         n_traj=NUMBER_OF_TRAJECTORIES,\n",
    "                                                         max_traj_length=MAXIMUM_TRAJECTORY_LENGTH)\n",
    "\n",
    "    if irl:\n",
    "        print(\"IRL from samples...\")\n",
    "        estimated_rewards = irl_reward_estimation(env=environment, optimal_trajectories=trajectories, train_func=train_func)\n",
    "        ref_reward_funcs.append(estimated_rewards['reference_reward_func'])\n",
    "        avg_pred_reward_funcs.append(estimated_rewards['avg_predicted_reward_func'])\n",
    "        # Using default value for reward loss -> Frobenius for matrices and L2-loss for vectors\n",
    "        reward_loss.append(np.linalg.norm(estimated_rewards['reference_reward_func'] - estimated_rewards['avg_predicted_reward_func']))\n",
    "        # Using L1-Loss for policy loss as described by Ng and Russel in 2000\n",
    "        policy_loss.append(np.linalg.norm(estimated_rewards['policy_pred'] - np.array(list(greedy_policy.values())), ord=1 ))\n",
    "\n",
    "        print('**********************************************')\n",
    "        print('*****************REWARD LOSS******************')\n",
    "        print(reward_loss)\n",
    "        print('**********************************************')\n",
    "        print('*****************POLICY LOSS*************************')\n",
    "        print(policy_loss)\n",
    "        print('**********************************************')\n",
    "\n",
    "print('reward_loss \\n', reward_loss)\n",
    "plt.plot(reward_loss)\n",
    "plt.savefig('reward_loss.png')\n",
    "\n",
    "print(\"Closing up the arena...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35114570",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
