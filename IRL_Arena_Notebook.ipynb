{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95ef11a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "from typing import Any, List, Callable\n",
    "\n",
    "from GridWorld_environments import Grid_World\n",
    "from RL_agents import ValueIterationAgent, QLearningAgent\n",
    "from IRL_agents import IRL_from_sampled_trajectories\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812e149d",
   "metadata": {},
   "source": [
    "#### Train Value Iteration function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7fa1e9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def train_value_iteration(gw_env: Grid_World, verbose=False):\n",
    "    vi_agent = ValueIterationAgent(states=gw_env.get_state_space(),\n",
    "                                   terminal_states=gw_env.get_terminal_states(),\n",
    "                                   reward_function=gw_env.get_reward_func(),\n",
    "                                   actions=gw_env.get_action_space(),\n",
    "                                   gamma=GAMMA)\n",
    "\n",
    "    iters = 0\n",
    "    while iters < VALUE_ITERATION_TRAINING_N and not vi_agent.value_converged:\n",
    "\n",
    "        for state in gw_env.get_state_space():\n",
    "\n",
    "            if state in gw_env.get_terminal_states():\n",
    "                continue\n",
    "\n",
    "            opt_act = vi_agent.get_optimal_action(action_state_pairs=gw_env.get_action_state_pairs(state=state))\n",
    "            next_state = gw_env.get_new_state_on_action(old_state=state, action=opt_act)\n",
    "            next_state_value = vi_agent.get_state_value(state=next_state)\n",
    "\n",
    "            vi_agent.set_state_value(state=state, new_value=(gw_env.get_state_reward(state=next_state) + GAMMA * next_state_value))\n",
    "\n",
    "        iters += 1\n",
    "\n",
    "    if verbose:\n",
    "        gw_env.display_value_function(value_func=vi_agent.get_value_function())\n",
    "\n",
    "    vi_agent.construct_greedy_policy(gw_env.get_action_state_pairs)\n",
    "\n",
    "    if verbose:\n",
    "        gw_env.display_policy(policy=vi_agent.get_policy())\n",
    "\n",
    "    return vi_agent.get_policy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c569f0",
   "metadata": {},
   "source": [
    "#### Train Q Learning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbbc175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_q_learning(gw_env: Grid_World, n_episodes=1000, verbose=False, policy=\"eps_greedy\", eps=0.2, max_episode_len=100, gamma=0.95):\n",
    "    ql_agent = QLearningAgent(states=gw_env.get_state_space(),\n",
    "                              terminal_states=gw_env.get_terminal_states(),\n",
    "                              reward_function=gw_env.get_reward_func(),\n",
    "                              actions=gw_env.get_action_space(),\n",
    "                              gamma=gamma)\n",
    "    \n",
    "    # init episodes\n",
    "    episodes = []\n",
    "    \n",
    "    # Define state_space without terminal states for getting starting position\n",
    "    state_space = deepcopy(gw_env.get_state_space()) # all states\n",
    "    terminal_states = gw_env.get_terminal_states()\n",
    "    for terminal_state in terminal_states:\n",
    "        state_space.remove(terminal_state) # not non absorbing state_space\n",
    "    \n",
    "    for n in range(n_episodes):\n",
    "        \n",
    "        episode = []\n",
    "        \n",
    "        # random starting position\n",
    "        start_idx = (np.random.choice(len(state_space)))\n",
    "        start = state_space[start_idx]\n",
    "        \n",
    "        episode.append(start)\n",
    "        \n",
    "        i = 0\n",
    "        terminal = False\n",
    "        \n",
    "        while ( ( i < max_episode_len ) and ( not terminal ) ):\n",
    "            i += 1\n",
    "            \n",
    "            # Choose Action from S derived by given policy\n",
    "            if policy == \"eps_greedy\":\n",
    "                if np.random.uniform() < (1-eps):\n",
    "                    # Choose greedy action -> highest Q-Value\n",
    "                    chosen_action = ql_agent.get_greedy_action(episode[-1])\n",
    "                else:\n",
    "                    # Choose random action form action space\n",
    "                    action_space = gw_env.get_action_space()\n",
    "                    chosen_action = action_space[np.random.choice(len(action_space))]\n",
    "            \n",
    "            new_state = gw_env.get_new_state_on_action(episode[-1], chosen_action)\n",
    "            \n",
    "            # Reward is taken from Q_learning agent -> it knows the reward function from the environment\n",
    "            ql_agent.update_Q_value(episode[-1], new_state, chosen_action)\n",
    "            \n",
    "            episode.append(new_state)\n",
    "            \n",
    "            if new_state in terminal_states:\n",
    "                terminal = True\n",
    "        \n",
    "        episodes.append(episode)\n",
    "    \n",
    "    if verbose:\n",
    "        gw_env.display_q_function(q_func=ql_agent.get_Q_function())\n",
    "\n",
    "    ql_agent.construct_greedy_policy(gw_env.get_action_state_pairs)\n",
    "\n",
    "    if verbose:\n",
    "        gw_env.display_policy(policy=ql_agent.get_policy())\n",
    "\n",
    "    return ql_agent.get_policy()         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d336096",
   "metadata": {},
   "source": [
    "#### IRL Reward estimation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793e4fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def irl_reward_estimation(env: Grid_World, optimal_trajectories: List[List[Any]], train_func: Callable):\n",
    "\n",
    "    # store reference reward function\n",
    "    reward_func_ref = deepcopy(env.get_board())\n",
    "    print('Reference reward function:\\n', reward_func_ref)\n",
    "\n",
    "    irl_agent = IRL_from_sampled_trajectories(d=(GW_SIZE[0] * 4, GW_SIZE[1] * 4),\n",
    "                                              env_ranges=((0, GW_SIZE[0]), (0, GW_SIZE[1])),\n",
    "                                              env_discrete_size=GW_SIZE,\n",
    "                                              penalty_factor=2,\n",
    "                                              gamma=GAMMA)\n",
    "\n",
    "    # step 2: given optimal trajectories, compute the value estimate\n",
    "    print(\"Computing value estimates for optimal trajectories...\")\n",
    "    optimal_value_estimate = irl_agent.compute_value_estimate(trajs=optimal_trajectories)\n",
    "\n",
    "    candidate_policies = [env.construct_random_policy()]\n",
    "    candidate_value_estimates = []\n",
    "    reward_func_estimates = []\n",
    "\n",
    "    # while True:\n",
    "    for i in range(IRL_TRAINING_N):\n",
    "        print(f\"Iteration {i}...\")\n",
    "\n",
    "        # step 3: generate trajectories and compute the value estimate for a random policy\n",
    "        print(\"Generating trajectories for the candidate policy...\")\n",
    "        candidate_trajectories = env.generate_trajectories(policy=candidate_policies[-1],\n",
    "                                                           n_traj=NUMBER_OF_TRAJECTORIES,\n",
    "                                                           max_traj_length=MAXIMUM_TRAJECTORY_LENGTH)\n",
    "        print(\"Computing value estimates for condidate trajectories...\")\n",
    "        candidate_value_estimates.append(irl_agent.compute_value_estimate(trajs=candidate_trajectories))\n",
    "\n",
    "        # step 4: obtain new alphas\n",
    "        print(\"Solving linear programming...\")\n",
    "        irl_agent.solve_lp(optimal_value_estimate, candidate_value_estimates)\n",
    "\n",
    "        # step 5: construct new reward function from the alphas\n",
    "        reward_func = irl_agent.construct_reward_function(alphas=irl_agent.get_alphas())\n",
    "\n",
    "        # step 6: find optimal policy under new reward function and add to 'candidate_policies' list\n",
    "        env.set_reward_func(reward_func)\n",
    "        candidate_policies.append(train_func(gw_env=env, verbose=True))  # train_value_iteration(gw_env=env))\n",
    "        # store new reward function\n",
    "        reward_func_estimates.append(env.get_board())\n",
    "        \n",
    "        print(\"Latest estimated reward function:\\n\", reward_func_estimates[-1])\n",
    "        env.display_policy(policy=candidate_policies[-1])\n",
    "        print(\"============================================================\\n\" * 2)\n",
    "\n",
    "    return {'reference_reward_func': reward_func_ref, 'policy_pred': np.mean(np.array([list(pol.values()) for pol in candidate_policies]), axis=0), 'avg_predicted_reward_func': np.mean(np.array(reward_func_estimates), axis=0)}\n",
    "\n",
    "\n",
    "def calc_value_distance(value_estimates_ref, value_estimates_pred):\n",
    "    return np.linalg.norm(np.array(value_estimates_ref) - np.array(value_estimates_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ea0f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.95\n",
    "VALUE_ITERATION_TRAINING_N = 25\n",
    "IRL_TRAINING_N = 10\n",
    "\n",
    "NUMBER_OF_TRAJECTORIES = 400\n",
    "MAXIMUM_TRAJECTORY_LENGTH = 10\n",
    "\n",
    "GW_SIZE = (3, 3)\n",
    "GW_SIZES = [(3, 3)]  # [(x, x) for x in np.arange(5,11, 5)]\n",
    "GW_TRAPS = []\n",
    "GW_GOALS = [(0, 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46913987",
   "metadata": {},
   "source": [
    "### Step-by-step code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90070f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = Grid_World(size=GW_SIZE, traps=GW_TRAPS, goals=GW_GOALS, randomize_board=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176c8196",
   "metadata": {},
   "outputs": [],
   "source": [
    "vi_greedy_policy = train_value_iteration(gw_env=environment, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585078bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ql_greedy_policy = train_q_learning(gw_env=environment, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8810805",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Generating {NUMBER_OF_TRAJECTORIES} trajectories...\")\n",
    "\n",
    "greedy_policy = vi_greedy_policy\n",
    "# greedy_policy = ql_greedy_policy\n",
    "\n",
    "trajectories = environment.generate_trajectories(policy=greedy_policy,\n",
    "                                                 n_traj=NUMBER_OF_TRAJECTORIES,\n",
    "                                                 max_traj_length=MAXIMUM_TRAJECTORY_LENGTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e877c596",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"IRL from samples...\")\n",
    "\n",
    "# restart the environment\n",
    "environment = Grid_World(size=GW_SIZE, traps=GW_TRAPS, goals=GW_GOALS, randomize_board=False)\n",
    "\n",
    "train_func = train_value_iteration\n",
    "# train_func = train_q_learning\n",
    "\n",
    "estimated_rewards = irl_reward_estimation(env=environment, optimal_trajectories=trajectories, train_func=train_func)\n",
    "ref_reward_funcs.append(estimated_rewards['reference_reward_func'])\n",
    "avg_pred_reward_funcs.append(estimated_rewards['avg_predicted_reward_func'])\n",
    "# Using default value for reward loss -> Frobenius for matrices and L2-loss for vectors\n",
    "reward_loss.append(np.linalg.norm(estimated_rewards['reference_reward_func'] - estimated_rewards['avg_predicted_reward_func']))\n",
    "# Using L1-Loss for policy loss as described by Ng and Russel in 2000\n",
    "policy_loss.append(np.linalg.norm(estimated_rewards['policy_pred'] - np.array(list(greedy_policy.values())), ord=1 ))\n",
    "\n",
    "print('**********************************************')\n",
    "print('*****************REWARD LOSS******************')\n",
    "print(reward_loss)\n",
    "print('**********************************************')\n",
    "print('*****************POLICY LOSS*************************')\n",
    "print(policy_loss)\n",
    "print('**********************************************')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed02e3e",
   "metadata": {},
   "source": [
    "### End-to-end loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0042f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "vi = True\n",
    "ql = False\n",
    "gt = True\n",
    "irl = True\n",
    "plt = False\n",
    "\n",
    "print(\"configuration is:\")\n",
    "if vi:\n",
    "    print(\"training: value iteration\")\n",
    "if ql:\n",
    "    print(\"training: Q-Learning\")\n",
    "if gt:\n",
    "    print(\"generating trajectories\")\n",
    "if irl:\n",
    "    print(\"inverse reinforcment learning\")\n",
    "if plt:\n",
    "    print(\"creating plots\")\n",
    "    \n",
    "print(\"\")\n",
    "\n",
    "ref_reward_funcs = []\n",
    "avg_pred_reward_funcs = []\n",
    "reward_loss = []\n",
    "policy_loss = []\n",
    "\n",
    "for GW_SIZE in GW_SIZES:\n",
    "    environment = Grid_World(size=GW_SIZE, traps=GW_TRAPS, goals=GW_GOALS, randomize_board=False)\n",
    "\n",
    "    train_func = train_value_iteration\n",
    "\n",
    "    if vi:\n",
    "        print(\"Training via value iteration...\")\n",
    "        greedy_policy = train_value_iteration(gw_env=environment, verbose=True)\n",
    "    elif ql:\n",
    "        print(\"Training via q-learning...\")\n",
    "        greedy_policy = train_q_learning(gw_env=environment, verbose=True)\n",
    "        train_func = train_q_learning\n",
    "    else:\n",
    "        # load from file (?)\n",
    "        greedy_policy = {}\n",
    "\n",
    "    if gt:\n",
    "        print(f\"Generating {NUMBER_OF_TRAJECTORIES} trajectories...\")\n",
    "        trajectories = environment.generate_trajectories(policy=greedy_policy,\n",
    "                                                         n_traj=NUMBER_OF_TRAJECTORIES,\n",
    "                                                         max_traj_length=MAXIMUM_TRAJECTORY_LENGTH)\n",
    "\n",
    "    if irl:\n",
    "        print(\"IRL from samples...\")\n",
    "        estimated_rewards = irl_reward_estimation(env=environment, optimal_trajectories=trajectories, train_func=train_func)\n",
    "        ref_reward_funcs.append(estimated_rewards['reference_reward_func'])\n",
    "        avg_pred_reward_funcs.append(estimated_rewards['avg_predicted_reward_func'])\n",
    "        # Using default value for reward loss -> Frobenius for matrices and L2-loss for vectors\n",
    "        reward_loss.append(np.linalg.norm(estimated_rewards['reference_reward_func'] - estimated_rewards['avg_predicted_reward_func']))\n",
    "        # Using L1-Loss for policy loss as described by Ng and Russel in 2000\n",
    "        policy_loss.append(np.linalg.norm(estimated_rewards['policy_pred'] - np.array(list(greedy_policy.values())), ord=1 ))\n",
    "\n",
    "        print('**********************************************')\n",
    "        print('*****************REWARD LOSS******************')\n",
    "        print(reward_loss)\n",
    "        print('**********************************************')\n",
    "        print('*****************POLICY LOSS*************************')\n",
    "        print(policy_loss)\n",
    "        print('**********************************************')\n",
    "\n",
    "print('reward_loss \\n', reward_loss)\n",
    "plt.plot(reward_loss)\n",
    "plt.savefig('reward_loss.png')\n",
    "\n",
    "print(\"Closing up the arena...\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
