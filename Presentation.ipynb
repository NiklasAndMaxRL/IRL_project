{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f40172a0",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from ipycanvas import Canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "5d2c092c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "from typing import Any, List, Callable, Dict, Tuple\n",
    "\n",
    "from GridWorld_environments import Grid_World\n",
    "from RL_agents import ValueIterationAgent, QLearningAgent\n",
    "from IRL_agents import IRL_from_sampled_trajectories\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0e1d65",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8af4c4",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Show Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5c0299fd",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def get_display_policy(policy: Dict[Tuple[int], Tuple[int]], actions_to_str_map: Dict[Tuple[int], str], size: Tuple[int], terminal_states: Tuple[int] ):\n",
    "\n",
    "    policy_str = {state: actions_to_str_map[action] for state, action in policy.items()}\n",
    "    policy_arr = np.zeros(size, str)\n",
    "\n",
    "    for state in policy_str:\n",
    "        policy_arr[state] = policy_str[state]\n",
    "\n",
    "    for state in terminal_states:\n",
    "        policy_arr[state] = \"x\"\n",
    "\n",
    "    return policy_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd00041",
   "metadata": {},
   "source": [
    "#### Show Value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "dae666ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_display_value_func(val_func: Dict[Tuple[int], Tuple[int]], size: Tuple[int], terminal_states: Tuple[int] ):\n",
    "\n",
    "    val_func_disp = np.zeros(size)\n",
    "    #policy_str = {state: actions_to_str_map[action] for state, action in policy.items()}\n",
    "    #policy_arr = np.zeros(size, str)\n",
    "\n",
    "    for state in val_func:\n",
    "        val_func_disp[state] = val_func[state]\n",
    "\n",
    "    for state in terminal_states:\n",
    "        val_func_disp[state] = 0\n",
    "\n",
    "    return val_func_disp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb348844",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Perform Value evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9fd6eb29",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def perform_value_evaluation(gw_env: Grid_World, policy: Dict[Any, Any], verbose=False):\n",
    "    \n",
    "    vi_agent = ValueIterationAgent(states=gw_env.get_state_space(),\n",
    "                                   terminal_states=gw_env.get_terminal_states(),\n",
    "                                   reward_function=gw_env.get_reward_func(),\n",
    "                                   actions=gw_env.get_action_space(),\n",
    "                                   gamma=GAMMA)\n",
    "    \n",
    "    while not vi_agent.value_converged:\n",
    "        \n",
    "        for state in gw_env.get_state_space():\n",
    "            \n",
    "            if state in gw_env.get_terminal_states():\n",
    "                continue\n",
    "                \n",
    "            policy_act = policy[state]\n",
    "            next_state = gw_env.get_new_state_on_action(old_state=state, action=policy_act)\n",
    "            next_state_value = vi_agent.get_state_value(state=next_state)\n",
    "            \n",
    "            vi_agent.set_state_value(state=state, new_value=(gw_env.get_state_reward(state=next_state) + GAMMA * next_state_value))\n",
    "        \n",
    "    if verbose:\n",
    "        gw_env.display_value_function(value_func=vi_agent.get_value_function())\n",
    "\n",
    "    return vi_agent.get_value_function()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b402f4ae",
   "metadata": {},
   "source": [
    "#### Perform Action-Value evaluation function (Q-Learning evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ed05df50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_q_function_evaluation(gw_env: Grid_World, policy: Dict[Any, Any], rounding: int = 3):\n",
    "\n",
    "    value_func = perform_value_evaluation(gw_env, policy)\n",
    "    rewards = gw_env.get_reward_func()\n",
    "\n",
    "    q_function = {}\n",
    "    for state in gw_env.get_state_space():\n",
    "\n",
    "        action_values = {}\n",
    "        for action in gw_env.get_action_space():\n",
    "\n",
    "            next_state = gw_env.get_new_state_on_action(old_state=state, action=action)\n",
    "            action_values[action] = np.round(rewards[next_state] + GAMMA * value_func[next_state], rounding)\n",
    "            \n",
    "        q_function[state] = action_values\n",
    "\n",
    "    return q_function\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79b057d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### IRL Reward estimation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75e4b144",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def irl_reward_estimation(env: Grid_World, optimal_trajectories: List[List[Any]], train_func: Callable):\n",
    "\n",
    "    # store reference reward function\n",
    "    reward_func_ref = deepcopy(env.get_board())\n",
    "    print('Reference reward function:\\n', reward_func_ref)\n",
    "\n",
    "    irl_agent = IRL_from_sampled_trajectories(d=(GW_SIZE[0] * 4, GW_SIZE[1] * 4),\n",
    "                                              env_ranges=((0, GW_SIZE[0]), (0, GW_SIZE[1])),\n",
    "                                              env_discrete_size=GW_SIZE,\n",
    "                                              penalty_factor=2,\n",
    "                                              gamma=GAMMA)\n",
    "\n",
    "    # step 2: given optimal trajectories, compute the value estimate\n",
    "    print(\"Computing value estimates for optimal trajectories...\")\n",
    "    optimal_value_estimate = irl_agent.compute_value_estimate(trajs=optimal_trajectories)\n",
    "\n",
    "    candidate_policies = [env.construct_random_policy()]\n",
    "    candidate_value_estimates = []\n",
    "    reward_func_estimates = []\n",
    "\n",
    "    # while True:\n",
    "    for i in range(IRL_TRAINING_N):\n",
    "        print(f\"Iteration {i}...\")\n",
    "\n",
    "        # step 3: generate trajectories and compute the value estimate for a random policy\n",
    "        print(\"Generating trajectories for the candidate policy...\")\n",
    "        candidate_trajectories = env.generate_trajectories(policy=candidate_policies[-1],\n",
    "                                                           n_traj=NUMBER_OF_TRAJECTORIES,\n",
    "                                                           max_traj_length=MAXIMUM_TRAJECTORY_LENGTH)\n",
    "        print(\"Computing value estimates for condidate trajectories...\")\n",
    "        candidate_value_estimates.append(irl_agent.compute_value_estimate(trajs=candidate_trajectories))\n",
    "\n",
    "        # step 4: obtain new alphas\n",
    "        print(\"Solving linear programming...\")\n",
    "        irl_agent.solve_lp(optimal_value_estimate, candidate_value_estimates)\n",
    "\n",
    "        # step 5: construct new reward function from the alphas\n",
    "        reward_func = irl_agent.construct_reward_function(alphas=irl_agent.get_alphas())\n",
    "\n",
    "        # step 6: find optimal policy under new reward function and add to 'candidate_policies' list\n",
    "        env.set_reward_func(reward_func)\n",
    "        candidate_policies.append(train_func(gw_env=env, verbose=True))  # train_value_iteration(gw_env=env))\n",
    "        # store new reward function\n",
    "        reward_func_estimates.append(env.get_board())\n",
    "        \n",
    "        print(\"Latest estimated reward function:\\n\", reward_func_estimates[-1])\n",
    "        env.display_policy(policy=candidate_policies[-1])\n",
    "        print(\"============================================================\\n\" * 2)\n",
    "\n",
    "    return {'reference_reward_func': reward_func_ref, 'policy_pred': np.mean(np.array([list(pol.values()) for pol in candidate_policies]), axis=0), 'avg_predicted_reward_func': np.mean(np.array(reward_func_estimates), axis=0)}\n",
    "\n",
    "\n",
    "def calc_value_distance(value_estimates_ref, value_estimates_pred):\n",
    "    return np.linalg.norm(np.array(value_estimates_ref) - np.array(value_estimates_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83aac645",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bayesian Inverse Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7758314",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Agenda\n",
    "\n",
    "1. Motivation\n",
    "2. Reference work\n",
    "    1. Inverse Reinforcement Learning (IRL)\n",
    "    2. Bayesian Inverse Reinforcement Learning (BIRL)\n",
    "3. Implementation\n",
    "4. Results\n",
    "5. Outlook\n",
    "    - Add Priors\n",
    "        - Gaussian, ...\n",
    "    - Add different Policies\n",
    "        - Boltzman\n",
    "        - Epsilon Greedy\n",
    "        - Greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1ee7b3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a17055",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The problem of Inverse Reinforcement Learning has been defined in Learning agents for uncertain environments (Russel, 1998).\n",
    "\n",
    "- **Determine**: reward function being optimized.\n",
    "- **Given**: 1) Measurements of an agent's behavior over time, in a variety of circumstances 2) Measurements of the sensory inputs to that agent; 3) a model of the environment. ( Ng and Russel, 2000 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e319230",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Bee hive\n",
    "<img src=\"presentation/Bee_Hive_IRL.png\" alt=\"bee hive\" width=\"100%\"/>\n",
    "source: Niklas Kaspareit\n",
    "\n",
    "The problem here: How do we actually properly set rewards for different attributes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fc7c0f",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "motivation for this problem arises from e.g. animal and human learning.\n",
    "example: \n",
    "Bee foraging -> \n",
    "bee searches for nectar. \n",
    "reward -> nectar in flower\n",
    "possibly multiattribute: weight nectar ingestion against flight distance, time, risk from wind and predators.\n",
    "\n",
    "Conclusion: hard to determine relative weights here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44f31d1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Second idea: \n",
    "Often we want to learn a policy based on observation of an expert. However it is assumed that the reward function is known, which often is not the case. So we take the policy from the expert as given and infer the reward function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09d98ba",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Our goal\n",
    "Compare different IRL algorithms. To be precise compare the IRL algorithm provided by Ng and Russel in 2000 to the Bayesian Inverse Reinforcement Learning algorithm provided by Ramachandran et al. in 2007.\n",
    "Ramachandran et al. showed a diagram which proved that the BIRL algorithm performs better than the IRL algorithm by Ng and Russel comparing the reward loss and the policy loss.\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "\n",
    "<div style=\"display: flex;   flex: 33.33%; padding: 1px;\">\n",
    "    <img src=\"presentation/BIRL_Paper_Reward_Loss.jpg\" width=50%/>\n",
    "    <img src=\"presentation/BIRL_Paper_Policy_Loss.jpg\" width=50%/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a70691",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b70be154",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "GAMMA = 0.95\n",
    "VALUE_ITERATION_TRAINING_N = 25\n",
    "IRL_TRAINING_N = 10\n",
    "\n",
    "NUMBER_OF_TRAJECTORIES = 400\n",
    "MAXIMUM_TRAJECTORY_LENGTH = 10\n",
    "\n",
    "GW_SIZE = (6, 6)\n",
    "GW_SIZES = [(3, 3)]  # [(x, x) for x in np.arange(5,11, 5)]\n",
    "GW_TRAPS = [(3,3)]\n",
    "GW_GOALS = [(0, 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5deefac",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#environment = Grid_World(size=GW_SIZE, traps=GW_TRAPS, goals=GW_GOALS, randomize_board=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4d64718",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#vi_greedy_policy = train_value_iteration(gw_env=environment, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3751979e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Inverse Reinforcement Learning (IRL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0faef5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Given \n",
    "- finite State space $\\mathcal{S}$\n",
    "- a set of $k$ actions $A = \\{a_1, \\cdots , a_k\\}$\n",
    "- transition probabilities $\\{P_{sa}\\}$\n",
    "- discount factor $\\gamma$ \n",
    "- policy $\\pi$ in Markov Decision Process $(S, A, \\{P_{sa}\\}, \\gamma, R)$\n",
    "\n",
    "Constraint:\n",
    "- only finite-state MDPs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18737f38",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Required \"tools\"\n",
    "- value iteration\n",
    "- q learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9f69a1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Linear function approximation\n",
    "\n",
    "- For a general solution of $R$ we require all function to map $\\mathbb{R}^n \\rightarrow \\mathbb{R}$\n",
    "    - Hard to work with this, therefore we choose a linear approximation\n",
    "    \n",
    "$ \\quad R(s) = \\alpha_1 \\phi_1(s) + \\alpha_2 \\phi_2(s) + \\cdots + \\alpha_d \\phi_d(s) $\n",
    "\n",
    "with the $\\phi$s being fixed basis functions mapping from state $S$ into $R$ and the $\\alpha_i$s a are the parameters that can be fit and optimized for the reward function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da373f05",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The idea:\n",
    "- sample trajectories\n",
    "<img src=\"presentation/IRL_trajectories.png\" width=\"50%\" height=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186afda5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Using linearity of expectations value function $V_i^\\pi$ for the policy $\\pi$ is given by\n",
    "\n",
    "$ \\quad V^\\pi = \\alpha_1 V_1^\\pi + \\cdots + \\alpha_d V_d^\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6302bf8f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### IRL from Sampled Trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2329d57c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Access to policy $\\pi$ only given by a set of trajectories.\n",
    "- Assume that optimal policy can be found by choosing a reward function\n",
    "- No explicit model of MDP required\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6cdf5a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"display: flex;   flex: 33.33%; padding: 1px; align-items: center; justify-content: center;\">\n",
    "    <ul style=\"line-height: normal;\">\n",
    "        <li style=\"font-size:1.5em;\">sample $m$ (optimal) Monte Carlo trajectories under $\\pi$</li>\n",
    "    </ul>\n",
    "    <img src=\"presentation/IRL_Algo_Sampled_Trajectories_initial.png\" width=65% />\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf2881a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"display: flex;   flex: 33.33%; padding: 1px; align-items: center; justify-content: center;\">\n",
    "    <ul style=\"line-height: normal;\">\n",
    "        <li style=\"font-size:1.5em\">\n",
    "            For each $i = 1, \\cdots , d$ define $V_i^\\pi(s_0)$.</li>\n",
    "        <li style=\"font-size:1.5em; margin: 1em 0em; list-style-type:none;\">\n",
    "            <ul>\n",
    "                <li style=\"font-size:0.85em; margin: 0.5em 0em;\">Being the average empirical return on the samples trajectories with reward $R$</li>\n",
    "                <li style=\"font-size:0.85em; margin: 0.5em 0em;\">For any setting of $\\alpha_i$s the natural estimate of $\\hat{V}^\\pi(s_0)$ is:</li>\n",
    "                <li style=\"font-size:0.85em; margin: 0.5em 0em;\">$\\hat{V}^\\pi(s_0) = \\alpha_1 \\hat{V}_1^\\pi(s_0) + \\cdots + \\alpha_d \\hat{V}_d^\\pi(s_0) $</li>\n",
    "                <li style=\"font-size:0.85em; margin: 0.5em 0em;\">With: $\\hat{V}_i^\\pi(s_0) = \\phi_i(s_0) + \\gamma \\phi_i(s_1) + \\gamma^2 \\phi_i(s_2) + \\cdots $ </li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "    <img src=\"presentation/IRL_Algo_Sampled_Trajectories_1.png\" width=65% />\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89379246",
   "metadata": {},
   "source": [
    "<div style=\"display: flex;   flex: 33.33%; padding: 1px; align-items: center; justify-content: center;\">\n",
    "    <img src=\"presentation/IRL_Algo_Sampled_Trajectories_2.png\" width=80% />\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f31765",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "execute $m$ Monte Carlo trajectories under $\\pi$\n",
    "\n",
    "Then for each $i = 1, \\cdots , d$ define $V_i^\\pi(s_0)$ to be what the average empirical return would have been on these on these $m$ trajectories if the reward $R$ had been $\\phi_i$.\n",
    "\n",
    "For any setting of $\\alpha_i$s the natural estimate of $\\hat{V}^\\pi(s_0)$ is:\n",
    "\n",
    "$ \\quad \\hat{V}^\\pi(s_0) = \\alpha_1 \\hat{V}_1^\\pi(s_0) + \\cdots + \\alpha_d \\hat{V}_d^\\pi(s_0) $\n",
    "\n",
    "With:\n",
    "\n",
    "$ \\quad \\hat{V}_i^\\pi(s_0) = \\phi_i(s_0) + \\gamma \\phi_i(s_1) + \\gamma^2 \\phi_i(s_2) + \\cdots $ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9b2f41",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So first we determine the optimal policy $\\pi^*$ we are given and its value function.\n",
    "Then we do the same for our randomly generated initial policy $\\pi_1$.\n",
    "Now the idea is to use some \"inductive step\" to fit the $\\alpha_i$s and we want to achieve this:\n",
    "\n",
    "$ \\quad V^{\\pi^*} \\geq V^{\\pi_i}(s_0), \\qquad i = 1, \\cdots , k$\n",
    "\n",
    "We have to modify this slightly to get the following optimization:\n",
    "\n",
    "$ \\quad \\text{maximize} \\sum_{i=1}^{k} p \\left ( \\hat{V}^{\\pi^*}(s_0) - \\hat{V}^{\\pi_i}(s_0) \\right ) $\n",
    "\n",
    "$ \\quad s.t. |\\alpha_i| \\leq 1, \\qquad i = 1, \\cdots , d $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9029c592",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Both $\\hat{V}^{\\pi^*}(s_0)$ and $\\hat{V}^{\\pi_i}(s_0)$ are linear function. Hence we can easily solve this via linear programming. We use the simplex method.\n",
    "As described in the paper we choose $p(x) = x$ if $x \\geq 0$ and $p(x) = 2x$ if $x < 0$. These values were determined heuristically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc49ad4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "With this we get a new reward function $R = \\alpha_1 \\phi_1 + \\cdots + \\alpha_d \\phi_d$.\n",
    "Generate the new policy $\\pi_{k+1}$, that maximises $V^{\\pi}(s_0)$ with the new reward function and we add the policy to the set of policies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446632cb",
   "metadata": {},
   "source": [
    "<div style=\"display: flex;   flex: 33.33%; padding: 1px;\">\n",
    "    <p>some tet</p>\n",
    "    <img src=\"presentation/IRL_Algo_Sampled_Trajectories.jpg\" width=65% />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81df0f03",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hands On - Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a67af6b",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Train Value Iteration function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "eeed2691",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def train_value_iteration(gw_env: Grid_World, verbose=False):\n",
    "    vi_agent = ValueIterationAgent(states=gw_env.get_state_space(),\n",
    "                                   terminal_states=gw_env.get_terminal_states(),\n",
    "                                   reward_function=gw_env.get_reward_func(),\n",
    "                                   actions=gw_env.get_action_space(),\n",
    "                                   gamma=GAMMA)\n",
    "\n",
    "    iters = 0\n",
    "    while iters < VALUE_ITERATION_TRAINING_N and not vi_agent.value_converged:\n",
    "\n",
    "        for state in gw_env.get_state_space():\n",
    "\n",
    "            if state in gw_env.get_terminal_states():\n",
    "                continue\n",
    "\n",
    "            opt_act = vi_agent.get_optimal_action(action_state_pairs=gw_env.get_action_state_pairs(state=state))\n",
    "            next_state = gw_env.get_new_state_on_action(old_state=state, action=opt_act)\n",
    "            next_state_value = vi_agent.get_state_value(state=next_state)\n",
    "\n",
    "            vi_agent.set_state_value(state=state, new_value=(gw_env.get_state_reward(state=next_state) + GAMMA * next_state_value))\n",
    "\n",
    "        iters += 1\n",
    "\n",
    "    if verbose:\n",
    "        gw_env.display_value_function(value_func=vi_agent.get_value_function())\n",
    "\n",
    "    vi_agent.construct_greedy_policy(gw_env.get_action_state_pairs)\n",
    "\n",
    "    if verbose:\n",
    "        gw_env.display_policy(policy=vi_agent.get_policy())\n",
    "\n",
    "    return vi_agent.get_policy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19b75b1",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Train Q Learning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "099c4cb5",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def train_q_learning(gw_env: Grid_World, n_episodes=5000, verbose=False, policy=\"eps_greedy\", eps=0.2, max_episode_len=100, gamma=0.95):\n",
    "    ql_agent = QLearningAgent(states=gw_env.get_state_space(),\n",
    "                              size=gw_env.get_board_shape(),\n",
    "                              terminal_states=gw_env.get_terminal_states(),\n",
    "                              reward_function=gw_env.get_reward_func(),\n",
    "                              actions=gw_env.get_action_space(),\n",
    "                              gamma=gamma)\n",
    "    \n",
    "    # init episodes\n",
    "    episodes = []\n",
    "    \n",
    "    # Define state_space without terminal states for getting starting position\n",
    "    state_space = deepcopy(gw_env.get_state_space()) # all states\n",
    "    \n",
    "    board_size = gw_env.get_board_shape()\n",
    "    total_states = board_size[0] * board_size[1]\n",
    "    \n",
    "    # Number 15 is empirically determined.\n",
    "    # For a 3x3 Grid the total states are 9 and we checked, that at least 100 states are required to produce reasonably reliable results\n",
    "    # So 9 * x >= 100 yields that x >= 10\n",
    "    # Now we also added a buffer and therefore chose 15\n",
    "    convergence_criterion = total_states * 30\n",
    "    \n",
    "    terminal_states = gw_env.get_terminal_states()\n",
    "    for terminal_state in terminal_states:\n",
    "        state_space.remove(terminal_state) # not non absorbing state_space\n",
    "    \n",
    "    # init state_visited_counter\n",
    "    state_visited = {state: 4 for state in state_space}\n",
    "    \n",
    "    #action_value_converged = False\n",
    "    convergence_counter = 0\n",
    "    \n",
    "    for n in range(n_episodes):\n",
    "        \n",
    "        episode = []\n",
    "        \n",
    "        # reset if every state has been visited at least 4 times (for each action)\n",
    "        if ( (np.array(list(state_visited.values())) <= 0).all() ):\n",
    "            state_visited = {state: 4 for state in state_space}\n",
    "        \n",
    "        # random starting position\n",
    "        states_not_visited = [ state for state in state_visited if state_visited[state] > 0 ]\n",
    "        if len(states_not_visited) > 0:\n",
    "            start_idx = (np.random.choice(len(states_not_visited)))\n",
    "            start = states_not_visited[start_idx]\n",
    "        else:\n",
    "            start_idx = (np.random.choice(len(state_space)))\n",
    "            start = state_space[start_idx]\n",
    "        \n",
    "        state_visited[start] -= 1\n",
    "        \n",
    "        episode.append(start)\n",
    "        \n",
    "        i = 0\n",
    "        terminal = False\n",
    "        \n",
    "        old_q_val_func = ql_agent.get_Q_function(mat_repr=True)\n",
    "        \n",
    "        while ( ( i < max_episode_len ) and ( not terminal ) ):\n",
    "            i += 1\n",
    "            \n",
    "            # Choose Action from S derived by given policy\n",
    "            if policy == \"eps_greedy\":\n",
    "                if np.random.uniform() < (1-eps):\n",
    "                    # Choose greedy action -> highest Q-Value\n",
    "                    chosen_action = ql_agent.get_greedy_action(episode[-1])\n",
    "                else:\n",
    "                    # Choose random action form action space\n",
    "                    action_space = gw_env.get_action_space()\n",
    "                    chosen_action = action_space[np.random.choice(len(action_space))]\n",
    "            \n",
    "            new_state = gw_env.get_new_state_on_action(episode[-1], chosen_action)\n",
    "            \n",
    "            # Reward is taken from Q_learning agent -> it knows the reward function from the environment\n",
    "            ql_agent.update_Q_value(episode[-1], new_state, chosen_action)\n",
    "            \n",
    "            episode.append(new_state)\n",
    "            \n",
    "            if new_state in terminal_states:\n",
    "                terminal = True\n",
    "            else:\n",
    "                # add to state visited counter for the new state if it is not terminal\n",
    "                state_visited[new_state] -= 1\n",
    "                #if (state_visited[new_state] >= 5):\n",
    "                    #state_visited[new_state] = 0\n",
    "                    \n",
    "        episodes.append(episode)\n",
    "                    \n",
    "        # essentially works nicely, but to be used carefully. States that will not be visited by the current policy\n",
    "        # will only be visited, when the start is chosen by random choice in this state\n",
    "        # -> Fixed by: rarely visited states will be preferred for the choice of the start\n",
    "        \n",
    "        # Check if Q-function did is close to the Q-function from the last episode\n",
    "        if np.isclose( old_q_val_func, ql_agent.get_Q_function(mat_repr=True), atol=1e-08 ).all( ):\n",
    "            convergence_counter += 1\n",
    "            \n",
    "            # Comment in print statements to see how the episodes develop until convergence\n",
    "            #print(\"--------------\")\n",
    "            #print(f\"episode {n}\")\n",
    "            #print(\"convergence_counter\", convergence_counter)\n",
    "            #print(\"--------------\")\n",
    "            \n",
    "            if convergence_counter >= convergence_criterion:\n",
    "                break\n",
    "        else:\n",
    "            convergence_counter = 0\n",
    "\n",
    "        \n",
    "        \n",
    "    if verbose:\n",
    "        if n < n_episodes:\n",
    "            print(f\"It took {n} episodes to converge to the optimal Q-function\")\n",
    "        else:\n",
    "            print(f\"Did not converge to optimal Q-function in {n_episodes} episodes\")\n",
    "    \n",
    "    if verbose:\n",
    "        gw_env.display_q_function(q_func=ql_agent.get_Q_function())\n",
    "\n",
    "    ql_agent.construct_greedy_policy(gw_env.get_action_state_pairs)\n",
    "\n",
    "    if verbose:\n",
    "        gw_env.display_policy(policy=ql_agent.get_policy())\n",
    "\n",
    "    return ql_agent.get_policy()       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e0aec9e2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "actions_to_str_map = {(1, 0): \"v\", (0, 1): \">\", (-1, 0): \"^\", (0, -1): \"<\", None: \"-\"}\n",
    "\n",
    "GAMMA = 0.95\n",
    "VALUE_ITERATION_TRAINING_N = 100\n",
    "IRL_TRAINING_N = 2\n",
    "\n",
    "GW_SIZE = (3, 3)\n",
    "GW_TRAPS = []\n",
    "GW_GOALS = [(0, 2)] \n",
    "\n",
    "NUMBER_OF_STATES = GW_SIZE[0] * GW_SIZE[1]\n",
    "\n",
    "NUMBER_OF_TRAJECTORIES = NUMBER_OF_STATES * 20\n",
    "MAXIMUM_TRAJECTORY_LENGTH = NUMBER_OF_STATES * 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "d4877250",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "environment = Grid_World(size=GW_SIZE, traps=GW_TRAPS, goals=GW_GOALS, randomize_board=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "41a4e9e4",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function:\n",
      "[[0.95     1.       0.      ]\n",
      " [0.9025   0.95     1.      ]\n",
      " [0.857375 0.9025   0.95    ]]\n",
      "Policy:\n",
      "[['>' '>' 'x']\n",
      " ['>' '>' '^']\n",
      " ['>' '>' '^']]\n"
     ]
    }
   ],
   "source": [
    "vi_greedy_policy = train_value_iteration(gw_env=environment, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ab6ea97f",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 502 episodes to converge to the optimal Q-function\n",
      "Q function:\n",
      "[[[0.857375   0.95       0.9025     0.9025    ]\n",
      "  [0.9025     1.         0.95       0.9025    ]\n",
      "  [0.         0.         0.         0.        ]]\n",
      "\n",
      " [[0.81450625 0.9025     0.9025     0.857375  ]\n",
      "  [0.857375   0.95       0.95       0.857375  ]\n",
      "  [0.9025     0.95       1.         0.9025    ]]\n",
      "\n",
      " [[0.81450625 0.857375   0.857375   0.81450625]\n",
      "  [0.857375   0.9025     0.9025     0.81450625]\n",
      "  [0.9025     0.9025     0.95       0.857375  ]]]\n",
      "Policy:\n",
      "[['>' '>' 'x']\n",
      " ['>' '>' '^']\n",
      " ['>' '>' '^']]\n"
     ]
    }
   ],
   "source": [
    "ql_greedy_policy = train_q_learning(gw_env=environment, verbose=True, policy=\"eps_greedy\", eps=0.2, gamma=GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "049198c1",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 180 trajectories...\n",
      "____________________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Generating {NUMBER_OF_TRAJECTORIES} trajectories...\")\n",
    "print(\"____________________________\")\n",
    "print()\n",
    "\n",
    "greedy_policy = vi_greedy_policy\n",
    "# greedy_policy = ql_greedy_policy\n",
    "\n",
    "trajectories = environment.generate_trajectories(policy=greedy_policy,\n",
    "                                                 n_traj=NUMBER_OF_TRAJECTORIES,\n",
    "                                                 max_traj_length=MAXIMUM_TRAJECTORY_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8a05dc6e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing a few generated sample trajectories\n",
      "\n",
      "random trajectory 0\n",
      "\n",
      "[[' ' ' ' 'x']\n",
      " ['>' '>' '^']\n",
      " [' ' ' ' ' ']]\n",
      "\n",
      "-------------\n",
      "\n",
      "random trajectory 1\n",
      "\n",
      "[[' ' ' ' 'x']\n",
      " [' ' '>' '^']\n",
      " [' ' ' ' ' ']]\n",
      "\n",
      "-------------\n",
      "\n",
      "random trajectory 2\n",
      "\n",
      "[[' ' ' ' 'x']\n",
      " [' ' ' ' '^']\n",
      " [' ' ' ' ' ']]\n"
     ]
    }
   ],
   "source": [
    "print(\"showing a few generated sample trajectories\")\n",
    "print() \n",
    "\n",
    "for i in range(3):\n",
    "    print(f\"random trajectory {i}\")\n",
    "    print()\n",
    "    example_traj = np.zeros(GW_SIZE, str)\n",
    "    example_traj[:] = ' '\n",
    "\n",
    "    for state in trajectories[i]:\n",
    "        action_char = actions_to_str_map[greedy_policy[state]]\n",
    "        example_traj[state] = action_char\n",
    "\n",
    "    for goal in GW_GOALS:\n",
    "        example_traj[goal] = 'x'\n",
    "\n",
    "\n",
    "    print(example_traj)\n",
    "    if i < 2:\n",
    "        print()\n",
    "        print(\"-------------\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "50cbeec5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing IRL from samples...\n",
      "\n",
      "Reference reward function:\n",
      " [[0. 0. 1.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing IRL from samples...\")\n",
    "print()\n",
    "# restart the environment\n",
    "environment = Grid_World(size=GW_SIZE, traps=GW_TRAPS, goals=GW_GOALS, randomize_board=False)\n",
    "\n",
    "train_func = train_value_iteration\n",
    "# train_func = train_q_learning\n",
    "\n",
    "#estimated_rewards = irl_reward_estimation(env=environment, optimal_trajectories=trajectories, train_func=train_func)\n",
    "\n",
    "# ----\n",
    "#def irl_reward_estimation(env: Grid_World, optimal_trajectories: List[List[Any]], train_func: Callable):\n",
    "\n",
    "# store reference reward function\n",
    "reward_func_ref = deepcopy(environment.get_board())\n",
    "print('Reference reward function:\\n', reward_func_ref)\n",
    "\n",
    "irl_agent = IRL_from_sampled_trajectories(d=(GW_SIZE[0] * 4, GW_SIZE[1] * 4),\n",
    "                                          env_ranges=((0, GW_SIZE[0]), (0, GW_SIZE[1])),\n",
    "                                          env_discrete_size=GW_SIZE,\n",
    "                                          penalty_factor=2,\n",
    "                                          gamma=GAMMA)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "bf47bb79",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing value estimates for optimal trajectories...\n"
     ]
    }
   ],
   "source": [
    "# step 2: given optimal trajectories, compute the value estimate\n",
    "print(\"Computing value estimates for optimal trajectories...\")\n",
    "optimal_value_estimate = irl_agent.compute_value_estimate(trajs=trajectories)\n",
    "\n",
    "#print()\n",
    "#print(optimal_value_estimate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "10399da2",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with the randomly generated initial policy\n",
      "\n",
      "[['^' '<' 'x']\n",
      " ['<' 'v' '>']\n",
      " ['^' '<' '^']]\n"
     ]
    }
   ],
   "source": [
    "candidate_policies = [environment.construct_random_policy()]\n",
    "\n",
    "print(\"Starting with the randomly generated initial policy\\n\")\n",
    "print(get_display_policy(policy = candidate_policies[-1], actions_to_str_map=actions_to_str_map, size=GW_SIZE, terminal_states=GW_GOALS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "110fd515",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([(1,2),(3,4),(5,4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "634d2e73",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time 7.191521200000352\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06499207379917117,\n",
       " 0.0638007487017209,\n",
       " 0.06231172180547912,\n",
       " 0.06054729838755047,\n",
       " 0.0585302285994794,\n",
       " 0.05628949761947457,\n",
       " 0.05385709909407213,\n",
       " 0.05126235874931149,\n",
       " 0.0485400077705921,\n",
       " 0.045725432814123154,\n",
       " 0.04284857239204069,\n",
       " 0.03994343465510973,\n",
       " 0.06234001678871118,\n",
       " 0.06118994241009038,\n",
       " 0.0597548400567997,\n",
       " 0.058056187778927924,\n",
       " 0.056115863194973276,\n",
       " 0.05396171898838458,\n",
       " 0.05162447457198034,\n",
       " 0.04913226609404805,\n",
       " 0.04651840590265634,\n",
       " 0.04381681817244707,\n",
       " 0.04105618458591906,\n",
       " 0.03826908228892398,\n",
       " 0.05950590108277378,\n",
       " 0.05840104028125714,\n",
       " 0.05702461473699593,\n",
       " 0.0553972057170592,\n",
       " 0.05353974981034898,\n",
       " 0.051478881711792754,\n",
       " 0.04924395429138214,\n",
       " 0.04686183134322001,\n",
       " 0.04436430919825518,\n",
       " 0.04178375197188302,\n",
       " 0.0391475036209418,\n",
       " 0.036486618681789484,\n",
       " 0.056526833004721354,\n",
       " 0.055470536791219764,\n",
       " 0.05415675360260484,\n",
       " 0.05260511033537739,\n",
       " 0.05083554652699483,\n",
       " 0.04887340947625175,\n",
       " 0.046746611181168685,\n",
       " 0.04448067698641274,\n",
       " 0.04210580987389455,\n",
       " 0.039652736700659275,\n",
       " 0.03714740022396696,\n",
       " 0.034619260586642525,\n",
       " 0.053434847641813804,\n",
       " 0.05242991984748565,\n",
       " 0.05118204775726606,\n",
       " 0.04970985685658961,\n",
       " 0.04803224452388377,\n",
       " 0.04617321527214971,\n",
       " 0.0441591815024004,\n",
       " 0.042014278435071134,\n",
       " 0.039767055503097254,\n",
       " 0.03744654265265768,\n",
       " 0.035077232918669175,\n",
       " 0.03268693726723139,\n",
       " 0.05026698592720996,\n",
       " 0.0493155847545729,\n",
       " 0.04813607479902532,\n",
       " 0.04674604543514794,\n",
       " 0.045163319492291194,\n",
       " 0.043410519604454086,\n",
       " 0.041512517947703095,\n",
       " 0.03949202462937121,\n",
       " 0.03737589513938085,\n",
       " 0.03519142311613858,\n",
       " 0.032961620574808594,\n",
       " 0.030712614102684506,\n",
       " 0.047059497020531214,\n",
       " 0.046163130269474724,\n",
       " 0.04505361600829494,\n",
       " 0.043747485087310974,\n",
       " 0.04226146598340691,\n",
       " 0.04061677643349163,\n",
       " 0.03883672555013992,\n",
       " 0.03694257968601615,\n",
       " 0.034959479372656775,\n",
       " 0.03291296231990711,\n",
       " 0.030824544994338437,\n",
       " 0.02871865345584695,\n",
       " 0.043841278310154545,\n",
       " 0.043000914983970996,\n",
       " 0.04196235943719709,\n",
       " 0.04074107160638562,\n",
       " 0.03935267600101677,\n",
       " 0.0378169753970039,\n",
       " 0.03615570727335598,\n",
       " 0.03438868863853867,\n",
       " 0.03253933949496802,\n",
       " 0.030631437717967804,\n",
       " 0.028685002967870885,\n",
       " 0.026722759698870065,\n",
       " 0.04064494982198311,\n",
       " 0.039860943561373215,\n",
       " 0.03889354618698203,\n",
       " 0.037757145727373806,\n",
       " 0.03646626466426959,\n",
       " 0.03503929541986261,\n",
       " 0.033496411405835765,\n",
       " 0.03185598943661451,\n",
       " 0.030139740846841442,\n",
       " 0.028369697492064062,\n",
       " 0.026564395914907588,\n",
       " 0.024744874854754774,\n",
       " 0.037500596175995264,\n",
       " 0.0367727162630184,\n",
       " 0.035875956681801185,\n",
       " 0.034823643257816964,\n",
       " 0.033629209168657796,\n",
       " 0.03230965473575861,\n",
       " 0.03088361182329383,\n",
       " 0.029368039811982917,\n",
       " 0.02778296930349537,\n",
       " 0.026148716249383773,\n",
       " 0.02448236146717605,\n",
       " 0.022803289423673323,\n",
       " 0.03442982403035596,\n",
       " 0.03375739656764431,\n",
       " 0.03293021559633886,\n",
       " 0.031960562392913096,\n",
       " 0.030860801477980083,\n",
       " 0.029646569286324684,\n",
       " 0.028334989494348494,\n",
       " 0.026941636607200798,\n",
       " 0.025484899967571407,\n",
       " 0.02398342138747872,\n",
       " 0.022452863082276512,\n",
       " 0.020910997719496494,\n",
       " 0.031456861467365524,\n",
       " 0.030838714636275962,\n",
       " 0.030079448018185947,\n",
       " 0.029190324918578744,\n",
       " 0.028182670739728084,\n",
       " 0.027070797419422726,\n",
       " 0.025870365964438313,\n",
       " 0.02459560954920001,\n",
       " 0.023263328669652448,\n",
       " 0.021890545821585308,\n",
       " 0.02049155264674277,\n",
       " 0.019082564849923524]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just for backup to show compute_value_estimate in case someone is interested.\n",
    "# Cell will be skipped\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "def _approx_func(x, center):\n",
    "    return multivariate_normal.pdf(x=x, mean=center, cov=5)\n",
    "\n",
    "def compute_value_estimate(d_centers, trajs: List[List[Any]], gamma = 0.95) -> List[float]:\n",
    "    \"\"\"Given a List of trajectories, return a List with the value estimates for each approximating function.\"\"\"\n",
    "    \n",
    "    start = timer()\n",
    "    value_estimates = []\n",
    "    np.zeros(len(d_centers) * )\n",
    "    for approx_center in d_centers:\n",
    "        temp_value = 0\n",
    "\n",
    "        for traj in trajs:\n",
    "            for i, state in enumerate(traj):\n",
    "                temp_value += gamma**i * _approx_func(x=state, center=approx_center)\n",
    "\n",
    "        temp_value /= len(trajs)\n",
    "        value_estimates.append(temp_value)\n",
    "\n",
    "    end = timer()\n",
    "    print(f\"total time {end - start}\")\n",
    "\n",
    "    return value_estimates\n",
    "\n",
    "\n",
    "d_rows = 3 * 4\n",
    "d_cols = 3 * 4\n",
    "env_cols_min = 3\n",
    "env_cols_max = 3\n",
    "\n",
    "# determine the approximating funcs centers\n",
    "d_row_centers, step = np.linspace(3 - 1, 3 + 1, d_rows, endpoint=False, retstep=True)\n",
    "d_row_centers += step / 2\n",
    "# d_row_centers = np.append(d_row_centers, [min(d_row_centers) - step, max(d_row_centers) + step])\n",
    "# d_row_centers = np.sort(d_row_centers)\n",
    "d_col_centers, step = np.linspace(env_cols_min - 1, env_cols_max + 1, d_cols, endpoint=False, retstep=True)\n",
    "d_col_centers += step / 2\n",
    "# d_col_centers = np.append(d_col_centers, [min(d_col_centers) - step, max(d_col_centers) + step])\n",
    "# d_col_centers = np.sort(d_col_centers)\n",
    "# aggregate to one list\n",
    "d_centers = [(i, j) for i in d_row_centers.round(4) for j in d_col_centers.round(4)]\n",
    "d_centers\n",
    "\n",
    "greedy_policy = vi_greedy_policy\n",
    "# greedy_policy = ql_greedy_policy\n",
    "\n",
    "trajectoriesTMP = environment.generate_trajectories(policy=greedy_policy,\n",
    "                                                 n_traj=NUMBER_OF_TRAJECTORIES,\n",
    "                                                 max_traj_length=MAXIMUM_TRAJECTORY_LENGTH)\n",
    "\n",
    "compute_value_estimate(d_centers, trajectoriesTMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "9abe4766",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0...\n",
      "Generating trajectories for the candidate policy...\n",
      "total time Generating trajectories for the candidate policy... 0.03631150000001071\n",
      "Computing value estimates for candidate trajectories...\n",
      "total time Computing value estimates for candidate trajectories... 173.44049070000074\n",
      "Solving linear programming...\n",
      "total time Solving linear programming... 0.0015633000002708286\n",
      "total time construct new reward function from the alphas 0.27644750000035856\n",
      "total time find optimal policy under new reward function 0.004434200000105193\n",
      "Latest estimated reward function:\n",
      " [[-2.33182064 -2.65797864 -2.65797864]\n",
      " [-2.65797864 -3.02975724 -3.02975724]\n",
      " [-2.65797864 -3.02975724 -3.02975724]]\n",
      "Policy:\n",
      "[['>' '>' 'x']\n",
      " ['^' '^' '^']\n",
      " ['^' '^' '^']]\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "Iteration 1...\n",
      "Generating trajectories for the candidate policy...\n",
      "total time Generating trajectories for the candidate policy... 0.008505699999659555\n",
      "Computing value estimates for candidate trajectories...\n",
      "total time Computing value estimates for candidate trajectories... 15.954554399999324\n",
      "Solving linear programming...\n",
      "total time Solving linear programming... 0.0012320999994699378\n",
      "total time construct new reward function from the alphas 0.2139971999986301\n",
      "total time find optimal policy under new reward function 0.002476000001479406\n",
      "Latest estimated reward function:\n",
      " [[-2.33182064 -2.65797864 -2.65797864]\n",
      " [-2.65797864 -3.02975724 -3.02975724]\n",
      " [-2.65797864 -3.02975724 -3.02975724]]\n",
      "Policy:\n",
      "[['>' '>' 'x']\n",
      " ['^' '^' '^']\n",
      " ['^' '^' '^']]\n",
      "============================================================\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "candidate_value_estimates = []\n",
    "reward_func_estimates = []\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "start = timer()\n",
    "\n",
    "# while True:\n",
    "for i in range(IRL_TRAINING_N):\n",
    "    print(f\"Iteration {i}...\")\n",
    "\n",
    "    # step 3: generate trajectories and compute the value estimate for a random policy\n",
    "    start = timer()\n",
    "    print(\"Generating trajectories for the candidate policy...\")\n",
    "    candidate_trajectories = environment.generate_trajectories(policy=candidate_policies[-1],\n",
    "                                                       n_traj=NUMBER_OF_TRAJECTORIES,\n",
    "                                                       max_traj_length=MAXIMUM_TRAJECTORY_LENGTH)\n",
    "    end = timer()\n",
    "    print(f\"total time Generating trajectories for the candidate policy... {end - start}\")\n",
    "    \n",
    "    start = timer()\n",
    "    print(\"Computing value estimates for candidate trajectories...\")\n",
    "    candidate_value_estimates.append(irl_agent.compute_value_estimate(trajs=candidate_trajectories))\n",
    "    end = timer()\n",
    "    print(f\"total time Computing value estimates for candidate trajectories... {end - start}\")\n",
    "\n",
    "    start = timer()\n",
    "    # step 4: obtain new alphas\n",
    "    print(\"Solving linear programming...\")\n",
    "    irl_agent.solve_lp(optimal_value_estimate, candidate_value_estimates)\n",
    "    end = timer()\n",
    "    print(f\"total time Solving linear programming... {end - start}\")    \n",
    "\n",
    "    # step 5: construct new reward function from the alphas\n",
    "    start = timer()\n",
    "    reward_func = irl_agent.construct_reward_function(alphas=irl_agent.get_alphas())\n",
    "    end = timer()\n",
    "    print(f\"total time construct new reward function from the alphas {end - start}\")\n",
    "    \n",
    "    # step 6: find optimal policy under new reward function and add to 'candidate_policies' list\n",
    "    start = timer()\n",
    "    environment.set_reward_func(reward_func)\n",
    "    candidate_policies.append(train_func(gw_env=environment, verbose=False))  # train_value_iteration(gw_env=environment))\n",
    "    # store new reward function\n",
    "    reward_func_estimates.append(environment.get_board())\n",
    "    end = timer()\n",
    "    print(f\"total time find optimal policy under new reward function {end - start}\")\n",
    "\n",
    "    print(\"Latest estimated reward function:\\n\", reward_func_estimates[-1])\n",
    "    environment.display_policy(policy=candidate_policies[-1])\n",
    "    print(\"============================================================\\n\" * 2)\n",
    "\n",
    "    \n",
    "#return {'reference_reward_func': reward_func_ref, 'policy_pred': np.mean(np.array([list(pol.values()) for pol in candidate_policies]), axis=0), 'avg_predicted_reward_func': np.mean(np.array(reward_func_estimates), axis=0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82359f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def irl_reward_estimation(env: Grid_World, optimal_trajectories: List[List[Any]], train_func: Callable):\n",
    "\n",
    "    # store reference reward function\n",
    "    reward_func_ref = deepcopy(env.get_board())\n",
    "    print('Reference reward function:\\n', reward_func_ref)\n",
    "\n",
    "    irl_agent = IRL_from_sampled_trajectories(d=(GW_SIZE[0] * 4, GW_SIZE[1] * 4),\n",
    "                                              env_ranges=((0, GW_SIZE[0]), (0, GW_SIZE[1])),\n",
    "                                              env_discrete_size=GW_SIZE,\n",
    "                                              penalty_factor=2,\n",
    "                                              gamma=GAMMA)\n",
    "\n",
    "    # step 2: given optimal trajectories, compute the value estimate\n",
    "    print(\"Computing value estimates for optimal trajectories...\")\n",
    "    optimal_value_estimate = irl_agent.compute_value_estimate(trajs=optimal_trajectories)\n",
    "\n",
    "    candidate_policies = [env.construct_random_policy()]\n",
    "    candidate_value_estimates = []\n",
    "    reward_func_estimates = []\n",
    "\n",
    "    # while True:\n",
    "    for i in range(IRL_TRAINING_N):\n",
    "        print(f\"Iteration {i}...\")\n",
    "\n",
    "        # step 3: generate trajectories and compute the value estimate for a random policy\n",
    "        print(\"Generating trajectories for the candidate policy...\")\n",
    "        candidate_trajectories = env.generate_trajectories(policy=candidate_policies[-1],\n",
    "                                                           n_traj=NUMBER_OF_TRAJECTORIES,\n",
    "                                                           max_traj_length=MAXIMUM_TRAJECTORY_LENGTH)\n",
    "        print(\"Computing value estimates for condidate trajectories...\")\n",
    "        candidate_value_estimates.append(irl_agent.compute_value_estimate(trajs=candidate_trajectories))\n",
    "\n",
    "        # step 4: obtain new alphas\n",
    "        print(\"Solving linear programming...\")\n",
    "        irl_agent.solve_lp(optimal_value_estimate, candidate_value_estimates)\n",
    "\n",
    "        # step 5: construct new reward function from the alphas\n",
    "        reward_func = irl_agent.construct_reward_function(alphas=irl_agent.get_alphas())\n",
    "\n",
    "        # step 6: find optimal policy under new reward function and add to 'candidate_policies' list\n",
    "        env.set_reward_func(reward_func)\n",
    "        candidate_policies.append(train_func(gw_env=env, verbose=True))  # train_value_iteration(gw_env=env))\n",
    "        # store new reward function\n",
    "        reward_func_estimates.append(env.get_board())\n",
    "        \n",
    "        print(\"Latest estimated reward function:\\n\", reward_func_estimates[-1])\n",
    "        env.display_policy(policy=candidate_policies[-1])\n",
    "        print(\"============================================================\\n\" * 2)\n",
    "\n",
    "    return {'reference_reward_func': reward_func_ref, 'policy_pred': np.mean(np.array([list(pol.values()) for pol in candidate_policies]), axis=0), 'avg_predicted_reward_func': np.mean(np.array(reward_func_estimates), axis=0)}\n",
    "\n",
    "\n",
    "def calc_value_distance(value_estimates_ref, value_estimates_pred):\n",
    "    return np.linalg.norm(np.array(value_estimates_ref) - np.array(value_estimates_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6584a17f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayesian Inverse Reinforcement Learning (BIRL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab11bdf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hands On - BIRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "eeb62b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_value_iteration_new(gw_env: Grid_World, \n",
    "                          policy: Dict[Any, Any] = None,\n",
    "                          value_function: Dict[Any, float] = None,\n",
    "                          verbose=False,\n",
    "                          gamma=0.95,\n",
    "                          training_iterations=1000\n",
    "                         ) -> Tuple[Dict[Any, Any], Dict[Any, float]]:\n",
    "    \n",
    "    vi_agent = ValueIterationAgent(states=gw_env.get_state_space(),\n",
    "                                   terminal_states=gw_env.get_terminal_states(),\n",
    "                                   reward_function=gw_env.get_reward_func(),\n",
    "                                   actions=gw_env.get_action_space(),\n",
    "                                   gamma=gamma)\n",
    "\n",
    "    if policy and value_function:\n",
    "        raise ValueError(\"Can't pass both policy and value_function arguments at the same time.\")\n",
    "    \n",
    "    if policy:\n",
    "        if verbose:\n",
    "            print(\"Calculating value function of the given policy via value evaluation...\")\n",
    "        vi_agent.set_value_function(new_value_function=perform_value_evaluation(gw_env, policy, verbose))\n",
    "        \n",
    "    if value_function:\n",
    "        if verbose:\n",
    "            print(\"Using the given value function...\")\n",
    "        vi_agent.set_value_function(new_value_function=value_function)\n",
    "    \n",
    "    iters = 0\n",
    "    while iters < training_iterations and not vi_agent.value_converged:\n",
    "\n",
    "        for state in gw_env.get_state_space():\n",
    "\n",
    "            if state in gw_env.get_terminal_states():\n",
    "                continue\n",
    "\n",
    "            opt_act = vi_agent.get_optimal_action(action_state_pairs=gw_env.get_action_state_pairs(state=state))\n",
    "            next_state = gw_env.get_new_state_on_action(old_state=state, action=opt_act)\n",
    "            next_state_value = vi_agent.get_state_value(state=next_state)\n",
    "\n",
    "            vi_agent.set_state_value(state=state, new_value=(gw_env.get_state_reward(state=next_state) + gamma * next_state_value))\n",
    "\n",
    "        iters += 1\n",
    "\n",
    "    if verbose:\n",
    "        gw_env.display_value_function(value_func=vi_agent.get_value_function())\n",
    "\n",
    "    vi_agent.construct_greedy_policy(gw_env.get_action_state_pairs)\n",
    "\n",
    "    if verbose:\n",
    "        gw_env.display_policy(policy=vi_agent.get_policy())\n",
    "        \n",
    "    return vi_agent.get_policy(), vi_agent.get_value_function()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1127221d",
   "metadata": {},
   "source": [
    "#### Reward Space functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "5c4b9605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_reward(size: Tuple[int], r_max: float = 1.0, rounding: int = 3) -> np.ndarray:\n",
    "        \n",
    "    return np.random.uniform(low=-abs(r_max), high=abs(r_max), size=size).round(rounding)\n",
    "\n",
    "def get_reward_neighbour(reward: np.ndarray, step_size: float, r_max: float = 1.0) -> np.ndarray:\n",
    "    \n",
    "    movement = np.random.randint(low=-1, high=2, size=reward.shape) # random array of -1, 0 and +1\n",
    "    \n",
    "    return (reward + step_size * movement).clip(min=-abs(r_max), max=abs(r_max)) # new neighbour reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "1bc6c42c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1. , 0.8, 1. , 0.8, 1. , 0.8])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_reward_neighbour(np.array([1, 1, 1, 1, 1, 1]), 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aab287a",
   "metadata": {},
   "source": [
    "#### Prior, Evidence and Posterior Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "5d3909e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def improper_prior(reward):\n",
    "    return 1\n",
    "\n",
    "def compute_log_posterior(gw_env:Grid_World,\n",
    "                          observations: List[List[Tuple[Any]]],\n",
    "                          reward: np.ndarray,\n",
    "                          policy: Dict[Any, Any],\n",
    "                          prior: Callable,\n",
    "                          alpha: float,\n",
    "                          gamma: float\n",
    "                         ) -> float:\n",
    "    \n",
    "    gw_env.set_board(new_board=reward)\n",
    "    q = perform_q_function_evaluation(gw_env=gw_env, policy=policy)\n",
    "    \n",
    "    log_p = 0\n",
    "    \n",
    "    for observation in observations:\n",
    "        log_p += np.sum([alpha * q[s][a] - np.log(np.sum(np.exp(alpha * np.array(list(q[s].values()))))) for s, a in observation])\n",
    "        #### Should we use np.mean() here? our obs are not all of the same length\n",
    "        # log_p += np.mean([alpha * q[s][a] - np.log(np.sum(np.exp(alpha * np.array(list(q[s].values()))))) for s, a in observation])\n",
    "    \n",
    "    log_p += np.log(prior(reward))\n",
    "    \n",
    "    return log_p\n",
    "\n",
    "def posterior_dist_ratio(gw_env: Grid_World,\n",
    "                         observations: List[List[Tuple[Any]]],\n",
    "                         reward_next: np.ndarray,\n",
    "                         policy_next: Dict[Any, Any],\n",
    "                         reward_curr: np.ndarray,\n",
    "                         policy_curr: Dict[Any, Any]\n",
    "                        ) -> float:\n",
    "    \n",
    "    prior = improper_prior\n",
    "    alpha = 10\n",
    "    gamma = GAMMA\n",
    "    \n",
    "    log_post_next = compute_log_posterior(gw_env, observations, reward_next, policy_next, prior, alpha, gamma)\n",
    "    log_post_curr = compute_log_posterior(gw_env, observations, reward_curr, policy_curr, prior, alpha, gamma)\n",
    "    \n",
    "    return np.exp(log_post_next - log_post_curr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7f5cf1",
   "metadata": {},
   "source": [
    "#### PolicyWalk sampling algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fe8b0c",
   "metadata": {},
   "source": [
    "Credits for helping to decypher the formulation of the paper go to...\n",
    "\n",
    "- https://github.com/uidilr/bayesian_irl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "55270acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policyWalk(observations: List[List[Tuple[Any]]],\n",
    "               gw_env: Grid_World,\n",
    "               step_size: float,\n",
    "               n_steps: int,\n",
    "               samples_burn_in: int = 500,\n",
    "               samples_n_out: int = 5\n",
    "              ) -> np.ndarray:\n",
    "    \n",
    "    # step 1: pick a random reward from the reward space (as np.array)\n",
    "    reward_0 = get_random_reward(size=gw_env.get_board().shape)\n",
    "    \n",
    "    # step 2: get optimal policy given reward_0\n",
    "    gw_env.set_board(new_board=reward_0)  # board is the np.array representation of the reward function\n",
    "    policy_0, val_func_0 = train_value_iteration_new(gw_env=gw_env)\n",
    "    \n",
    "    posterior_samples = []\n",
    "    \n",
    "    # step 3: explore the reward space and sample from the posterior (GridWalk as a MCMC (Metropolis algorithm))\n",
    "    for i in range(n_steps):\n",
    "        # step 3a: pick reward_1 from reward_0 neighbours randomly (step_size distance)\n",
    "        reward_1 = get_reward_neighbour(reward=reward_0, step_size=step_size)\n",
    "        \n",
    "        # step 3b: compute the Q function of reward_1 under policy_0\n",
    "        gw_env.set_board(new_board=reward_1)\n",
    "        q_func = perform_q_function_evaluation(gw_env=gw_env,\n",
    "                                               policy=policy_0)\n",
    "        \n",
    "        # step 3c: check if ... \n",
    "        if is_policy_not_optimal(q_func=q_func, policy=policy_0,\n",
    "                                 states=gw_env.get_state_space(), actions=gw_env.get_action_space()):\n",
    "            # step 3c.i: get optimal policy given reward_1 AND policy_0 (we can use the value function of policy_0 directly)\n",
    "            #### MOTIVATION: since reward_1 is a neighbour of reward_0, policy_1 will be close to policy_0. We use this fact to speed up the computation\n",
    "            #### would be a repetition of step 3b: gw_env.set_board(new_board=reward_1)\n",
    "            policy_1, val_func_1 = train_value_iteration_new(gw_env=gw_env,\n",
    "                                                         value_function=val_func_0)\n",
    "            \n",
    "            # step 3c.ii: update reward_0 AND policy_0 with prob. according to the posterior_dist\n",
    "            if np.random.uniform() < np.minimum(1, posterior_dist_ratio(gw_env, observations, reward_1, policy_1, reward_0, policy_0)):\n",
    "                reward_0 = deepcopy(reward_1)\n",
    "                policy_0 = deepcopy(policy_1)\n",
    "                val_func_0 = deepcopy(val_func_1)\n",
    "            \n",
    "        else:\n",
    "            # step 3c.iii: update reward_0 with prob. according to the posterior_dist\n",
    "            if np.random.uniform() < np.minimum(1, posterior_dist_ratio(gw_env, observations, reward_1, policy_0, reward_0, policy_0)):\n",
    "                reward_0 = deepcopy(reward_1)\n",
    "                \n",
    "        if (i > samples_burn_in) and (samples_n_out != 0) and ((i - samples_burn_in) % samples_n_out == 0):\n",
    "            posterior_samples.append(reward_0)\n",
    "\n",
    "    # step 4: return a sample from the posterior\n",
    "    if samples_n_out != 0:\n",
    "        return posterior_samples\n",
    "    else:\n",
    "        return reward_0\n",
    "\n",
    "def is_policy_not_optimal(q_func, policy, states, actions):\n",
    "    for state in states:\n",
    "        for action in actions:\n",
    "            if q_func[state][policy[state]] < q_func[state][action]:\n",
    "                return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "d0763d0b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "GAMMA = 0.95\n",
    "\n",
    "VALUE_ITERATION_TRAINING_N = 1000\n",
    "\n",
    "GW_SIZE = (3, 3)\n",
    "GW_TRAPS = []\n",
    "GW_GOALS = [(0, 0)]\n",
    "\n",
    "NUMBER_OF_STATES = GW_SIZE[0] * GW_SIZE[1]\n",
    "\n",
    "NUMBER_OF_TRAJECTORIES    = NUMBER_OF_STATES * 20\n",
    "MAXIMUM_TRAJECTORY_LENGTH = NUMBER_OF_STATES * 4\n",
    "\n",
    "BIRL_N_OF_POSTERIOR_SAMPLES     = 2\n",
    "BIRL_POLICYWALK_STEP_SIZE       = 0.05\n",
    "BIRL_POLICYWALK_N_STEPS         = int(NUMBER_OF_STATES / BIRL_POLICYWALK_STEP_SIZE * 20)\n",
    "BIRL_POLICYWALK_SAMPLES_BURN_IN = int(BIRL_POLICYWALK_N_STEPS * 0.7)\n",
    "BIRL_POLICYWALK_SAMPLES_N_OUT   = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "a2d55d51",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function:\n",
      "[[0.       1.       0.95    ]\n",
      " [1.       0.95     0.9025  ]\n",
      " [0.95     0.9025   0.857375]]\n",
      "Policy:\n",
      "[['x' '<' '<']\n",
      " ['^' '<' '<']\n",
      " ['^' '<' '<']]\n",
      "Generating 180 trajectories...\n"
     ]
    }
   ],
   "source": [
    "environment = Grid_World(size=GW_SIZE, traps=GW_TRAPS, goals=GW_GOALS, randomize_board=False)\n",
    "\n",
    "target_reward = environment.get_board()\n",
    "\n",
    "vi_greedy_policy, vi_greedy_value_function = train_value_iteration_new(gw_env=environment, verbose=True, gamma=GAMMA, training_iterations=VALUE_ITERATION_TRAINING_N)\n",
    "\n",
    "print(f\"Generating {NUMBER_OF_TRAJECTORIES} trajectories...\")\n",
    "\n",
    "trajectories = environment.generate_trajectories(policy=vi_greedy_policy,\n",
    "                                                 n_traj=NUMBER_OF_TRAJECTORIES,\n",
    "                                                 max_traj_length=MAXIMUM_TRAJECTORY_LENGTH,\n",
    "                                                 return_state_action_pairs=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68438fc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Manually go through one iteration of policy walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "ac5338a0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate 1 sample from the posterior\n",
      "\n",
      "target reward: \n",
      "[[1. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "\n",
      "optimal policy:\n",
      "[['x' '<' '<']\n",
      " ['^' '<' '<']\n",
      " ['^' '<' '<']]\n",
      "\n",
      "randomly initialised reward: \n",
      "[[-0.354 -0.902 -0.642]\n",
      " [-0.824  0.186  0.865]\n",
      " [-0.78  -0.738 -0.741]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Generate 1 sample from the posterior\")\n",
    "print()\n",
    "\n",
    "posterior_samples = []\n",
    "\n",
    "environment = Grid_World(size=GW_SIZE, traps=GW_TRAPS, goals=GW_GOALS, randomize_board=False)\n",
    "policy_opt, val_func_opt = train_value_iteration_new(gw_env=environment)\n",
    "\n",
    "# step 1: pick a random reward from the reward space (as np.array)\n",
    "reward_0 = get_random_reward(size=environment.get_board().shape)\n",
    "\n",
    "print(\"target reward: \")\n",
    "print(environment.get_board())\n",
    "print()\n",
    "\n",
    "print(\"optimal policy:\")\n",
    "print(get_display_policy(policy = policy_opt, actions_to_str_map=actions_to_str_map, size=GW_SIZE, terminal_states=GW_GOALS))\n",
    "print()\n",
    "\n",
    "print(\"randomly initialised reward: \")\n",
    "print(reward_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "5878b79e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new policy generated by new reward function: \n",
      "[['x' 'v' 'v']\n",
      " ['>' '>' '<']\n",
      " ['>' '^' '^']]\n",
      "\n",
      "new value function generated by new reward function and policy\n",
      "[[ 0.         10.33420948 10.68249901]\n",
      " [10.33420948 10.68249901 10.33437406]\n",
      " [ 9.07949901 10.33437406 10.68265535]]\n"
     ]
    }
   ],
   "source": [
    "# step 2: get optimal policy given reward_0\n",
    "environment.set_board(new_board=reward_0)  # board is the np.array representation of the reward function\n",
    "policy_0, val_func_0 = train_value_iteration_new(gw_env=environment)\n",
    "\n",
    "print(\"new policy generated by new reward function: \")\n",
    "print(get_display_policy(policy = policy_0, actions_to_str_map=actions_to_str_map, size=GW_SIZE, terminal_states=GW_GOALS))\n",
    "\n",
    "print()\n",
    "print(\"new value function generated by new reward function and policy\")\n",
    "print(get_display_value_func(val_func_0, size=GW_SIZE, terminal_states=GW_GOALS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "a8c25878",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "posterior_samples_history = []\n",
    "\n",
    "observations=trajectories\n",
    "gw_env=environment\n",
    "step_size=BIRL_POLICYWALK_STEP_SIZE\n",
    "n_steps=BIRL_POLICYWALK_N_STEPS\n",
    "samples_burn_in=BIRL_POLICYWALK_SAMPLES_BURN_IN\n",
    "samples_n_out=BIRL_POLICYWALK_SAMPLES_N_OUT\n",
    "\n",
    "# step 3: explore the reward space and sample from the posterior (GridWalk as a MCMC (Metropolis algorithm))\n",
    "for i in range(BIRL_POLICYWALK_N_STEPS):\n",
    "    # step 3a: pick reward_1 from reward_0 neighbours randomly (step_size distance)\n",
    "    reward_1 = get_reward_neighbour(reward=reward_0, step_size=BIRL_POLICYWALK_STEP_SIZE)\n",
    "\n",
    "    # step 3b: compute the Q function of reward_1 under policy_0\n",
    "    environment.set_board(new_board=reward_1)\n",
    "    q_func = perform_q_function_evaluation(gw_env=environment,\n",
    "                                           policy=policy_0)\n",
    "\n",
    "    # step 3c: check if ... \n",
    "    if is_policy_not_optimal(q_func=q_func, policy=policy_0,\n",
    "                             states=environment.get_state_space(), actions=environment.get_action_space()):\n",
    "        # step 3c.i: get optimal policy given reward_1 AND policy_0 (we can use the value function of policy_0 directly)\n",
    "        #### MOTIVATION: since reward_1 is a neighbour of reward_0, policy_1 will be close to policy_0. We use this fact to speed up the computation\n",
    "        #### would be a repetition of step 3b: gw_env.set_board(new_board=reward_1)\n",
    "        policy_1, val_func_1 = train_value_iteration_new(gw_env=environment,\n",
    "                                                     value_function=val_func_0)\n",
    "\n",
    "        # step 3c.ii: update reward_0 AND policy_0 with prob. according to the posterior_dist\n",
    "        if np.random.uniform() < np.minimum(1, posterior_dist_ratio(environment, trajectories, reward_1, policy_1, reward_0, policy_0)):\n",
    "            reward_0 = deepcopy(reward_1)\n",
    "            policy_0 = deepcopy(policy_1)\n",
    "            val_func_0 = deepcopy(val_func_1)\n",
    "\n",
    "    else:\n",
    "        # step 3c.iii: update reward_0 with prob. according to the posterior_dist\n",
    "        if np.random.uniform() < np.minimum(1, posterior_dist_ratio(environment, trajectories, reward_1, policy_0, reward_0, policy_0)):\n",
    "            reward_0 = deepcopy(reward_1)\n",
    "\n",
    "    if (i > BIRL_POLICYWALK_SAMPLES_BURN_IN) and (BIRL_POLICYWALK_SAMPLES_N_OUT != 0) and ((i - BIRL_POLICYWALK_SAMPLES_BURN_IN) % BIRL_POLICYWALK_SAMPLES_N_OUT == 0):\n",
    "        posterior_samples.append(reward_0)\n",
    "\n",
    "# step 4: return a sample from the posterior\n",
    "if BIRL_POLICYWALK_SAMPLES_N_OUT != 0:\n",
    "    posterior_samples_history += posterior_samples\n",
    "else:\n",
    "    posterior_samples_history += reward_0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80013b42",
   "metadata": {},
   "source": [
    "#### Run multiple iterations for posterior sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "d6061a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate 10 samples from the posterior\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]C:\\Users\\NiklasKaspareit\\AppData\\Local\\Temp\\ipykernel_20360\\2626929354.py:42: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(log_post_next - log_post_curr)\n",
      "100%|| 10/10 [19:40<00:00, 118.02s/it]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Generate {BIRL_N_OF_POSTERIOR_SAMPLES} samples from the posterior\")\n",
    "\n",
    "posterior_samples = []\n",
    "\n",
    "for _ in tqdm(range(BIRL_N_OF_POSTERIOR_SAMPLES)):\n",
    "    \n",
    "    # restart the environment\n",
    "    environment = Grid_World(size=GW_SIZE, traps=GW_TRAPS, goals=GW_GOALS, randomize_board=False)\n",
    "\n",
    "    posterior_samples += policyWalk(observations=trajectories,\n",
    "                                    gw_env=environment,\n",
    "                                    step_size=BIRL_POLICYWALK_STEP_SIZE,\n",
    "                                    n_steps=BIRL_POLICYWALK_N_STEPS,\n",
    "                                    samples_burn_in=BIRL_POLICYWALK_SAMPLES_BURN_IN,\n",
    "                                    samples_n_out=BIRL_POLICYWALK_SAMPLES_N_OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338c0afc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "rise": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
