{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83aac645",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bayesian Inverse Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7758314",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Agenda\n",
    "\n",
    "1. Motivation\n",
    "2. Reference work\n",
    "    1. Inverse Reinforcement Learning (IRL)\n",
    "    2. Bayesian Inverse Reinforcement Learning (BIRL)\n",
    "3. Implementation\n",
    "4. Results\n",
    "5. Outlook\n",
    "    - Add Priors\n",
    "        - Gaussian, ...\n",
    "    - Add different Policies\n",
    "        - Boltzman\n",
    "        - Epsilon Greedy\n",
    "        - Greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1ee7b3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a17055",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The problem of Inverse Reinforcement Learning has been defined in Learning agents for uncertain environments (Russel, 1998).\n",
    "\n",
    "- \"*Determine*: reward function being optimized.\n",
    "- *Given*: 1) Measurements of an agent's behavior over time, in a variety of circumstances 2) Measurements of the sensory inputs to that agent; 3) a model of the environment.\" ( Ng and Russel, 2000 )\n",
    "\n",
    "Reproduce the results of the case for Bayesian Inverse Reinforcement Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ef8331",
   "metadata": {},
   "source": [
    "motivation for this problem arises from e.g. animal and human learning.\n",
    "example: \n",
    "Bee foraging -> \n",
    "bee searches for nectar. \n",
    "reward -> nectar in flower\n",
    "possibly multiattribute: weight nectar ingestion against flight distance, time, risk from wind and predators.\n",
    "\n",
    "Conclusion: hard to determine relative weights here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3e3386",
   "metadata": {},
   "source": [
    "### The Bee hive\n",
    "<img src=\"presentation/Bee_Hive_IRL.png\" alt=\"bee hive\" width=\"100%\"/>\n",
    "source: self made\n",
    "\n",
    "The problem here: How do we actually properly set rewards for different attributes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e737e35b",
   "metadata": {},
   "source": [
    "#### Second idea: \n",
    "Often we want to learn a policy based on observation of an expert. However it is assumed that the reward function is known, which often is not the case. So we take the policy from the expert as given and infer the reward function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3751979e",
   "metadata": {},
   "source": [
    "## 1. Inverse Reinforcement Learning (IRL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8305a3e",
   "metadata": {},
   "source": [
    "Given \n",
    "- finite State space $\\mathcal{S}$\n",
    "- a set of $k$ actions $A = \\{a_1, \\cdots , a_k\\}$\n",
    "- transition probabilities $\\{P_{sa}\\}$\n",
    "- discount factor $\\gamma$ \n",
    "- policy $\\pi$ in Markov Decision Process $(S, A, \\{P_{sa}\\}, \\gamma, R)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82359f2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
