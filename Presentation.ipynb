{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a30d0cb",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "from typing import Any, List, Callable, Dict\n",
    "\n",
    "from GridWorld_environments import Grid_World\n",
    "from RL_agents import ValueIterationAgent, QLearningAgent\n",
    "from IRL_agents import IRL_from_sampled_trajectories\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60ee84e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8f3bec",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Train Value Iteration function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66130e9e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def train_value_iteration(gw_env: Grid_World, verbose=False):\n",
    "    vi_agent = ValueIterationAgent(states=gw_env.get_state_space(),\n",
    "                                   terminal_states=gw_env.get_terminal_states(),\n",
    "                                   reward_function=gw_env.get_reward_func(),\n",
    "                                   actions=gw_env.get_action_space(),\n",
    "                                   gamma=GAMMA)\n",
    "\n",
    "    iters = 0\n",
    "    while iters < VALUE_ITERATION_TRAINING_N and not vi_agent.value_converged:\n",
    "\n",
    "        for state in gw_env.get_state_space():\n",
    "\n",
    "            if state in gw_env.get_terminal_states():\n",
    "                continue\n",
    "\n",
    "            opt_act = vi_agent.get_optimal_action(action_state_pairs=gw_env.get_action_state_pairs(state=state))\n",
    "            next_state = gw_env.get_new_state_on_action(old_state=state, action=opt_act)\n",
    "            next_state_value = vi_agent.get_state_value(state=next_state)\n",
    "\n",
    "            vi_agent.set_state_value(state=state, new_value=(gw_env.get_state_reward(state=next_state) + GAMMA * next_state_value))\n",
    "\n",
    "        iters += 1\n",
    "\n",
    "    if verbose:\n",
    "        gw_env.display_value_function(value_func=vi_agent.get_value_function())\n",
    "\n",
    "    vi_agent.construct_greedy_policy(gw_env.get_action_state_pairs)\n",
    "\n",
    "    if verbose:\n",
    "        gw_env.display_policy(policy=vi_agent.get_policy())\n",
    "\n",
    "    return vi_agent.get_policy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ff83c5",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Train Q Learning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa66a5af",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def train_q_learning(gw_env: Grid_World, n_episodes=5000, verbose=False, policy=\"eps_greedy\", eps=0.2, max_episode_len=100, gamma=0.95):\n",
    "    ql_agent = QLearningAgent(states=gw_env.get_state_space(),\n",
    "                              size=gw_env.get_board_shape(),\n",
    "                              terminal_states=gw_env.get_terminal_states(),\n",
    "                              reward_function=gw_env.get_reward_func(),\n",
    "                              actions=gw_env.get_action_space(),\n",
    "                              gamma=gamma)\n",
    "    \n",
    "    # init episodes\n",
    "    episodes = []\n",
    "    \n",
    "    # Define state_space without terminal states for getting starting position\n",
    "    state_space = deepcopy(gw_env.get_state_space()) # all states\n",
    "    \n",
    "    board_size = gw_env.get_board_shape()\n",
    "    total_states = board_size[0] * board_size[1]\n",
    "    \n",
    "    # Number 15 is empirically determined.\n",
    "    # For a 3x3 Grid the total states are 9 and we checked, that at least 100 states are required to produce reasonably reliable results\n",
    "    # So 9 * x >= 100 yields that x >= 10\n",
    "    # Now we also added a buffer and therefore chose 15\n",
    "    convergence_criterion = total_states * 30\n",
    "    \n",
    "    terminal_states = gw_env.get_terminal_states()\n",
    "    for terminal_state in terminal_states:\n",
    "        state_space.remove(terminal_state) # not non absorbing state_space\n",
    "    \n",
    "    # init state_visited_counter\n",
    "    state_visited = {state: 4 for state in state_space}\n",
    "    \n",
    "    #action_value_converged = False\n",
    "    convergence_counter = 0\n",
    "    \n",
    "    for n in range(n_episodes):\n",
    "        \n",
    "        episode = []\n",
    "        \n",
    "        # reset if every state has been visited at least 4 times (for each action)\n",
    "        if ( (np.array(list(state_visited.values())) <= 0).all() ):\n",
    "            state_visited = {state: 4 for state in state_space}\n",
    "        \n",
    "        # random starting position\n",
    "        states_not_visited = [ state for state in state_visited if state_visited[state] > 0 ]\n",
    "        if len(states_not_visited) > 0:\n",
    "            start_idx = (np.random.choice(len(states_not_visited)))\n",
    "            start = states_not_visited[start_idx]\n",
    "        else:\n",
    "            start_idx = (np.random.choice(len(state_space)))\n",
    "            start = state_space[start_idx]\n",
    "        \n",
    "        state_visited[start] -= 1\n",
    "        \n",
    "        episode.append(start)\n",
    "        \n",
    "        i = 0\n",
    "        terminal = False\n",
    "        \n",
    "        old_q_val_func = ql_agent.get_Q_function(mat_repr=True)\n",
    "        \n",
    "        while ( ( i < max_episode_len ) and ( not terminal ) ):\n",
    "            i += 1\n",
    "            \n",
    "            # Choose Action from S derived by given policy\n",
    "            if policy == \"eps_greedy\":\n",
    "                if np.random.uniform() < (1-eps):\n",
    "                    # Choose greedy action -> highest Q-Value\n",
    "                    chosen_action = ql_agent.get_greedy_action(episode[-1])\n",
    "                else:\n",
    "                    # Choose random action form action space\n",
    "                    action_space = gw_env.get_action_space()\n",
    "                    chosen_action = action_space[np.random.choice(len(action_space))]\n",
    "            \n",
    "            new_state = gw_env.get_new_state_on_action(episode[-1], chosen_action)\n",
    "            \n",
    "            # Reward is taken from Q_learning agent -> it knows the reward function from the environment\n",
    "            ql_agent.update_Q_value(episode[-1], new_state, chosen_action)\n",
    "            \n",
    "            episode.append(new_state)\n",
    "            \n",
    "            if new_state in terminal_states:\n",
    "                terminal = True\n",
    "            else:\n",
    "                # add to state visited counter for the new state if it is not terminal\n",
    "                state_visited[new_state] -= 1\n",
    "                #if (state_visited[new_state] >= 5):\n",
    "                    #state_visited[new_state] = 0\n",
    "                    \n",
    "        episodes.append(episode)\n",
    "                    \n",
    "        # essentially works nicely, but to be used carefully. States that will not be visited by the current policy\n",
    "        # will only be visited, when the start is chosen by random choice in this state\n",
    "        # -> Fixed by: rarely visited states will be preferred for the choice of the start\n",
    "        \n",
    "        # Check if Q-function did is close to the Q-function from the last episode\n",
    "        if np.isclose( old_q_val_func, ql_agent.get_Q_function(mat_repr=True), atol=1e-08 ).all( ):\n",
    "            convergence_counter += 1\n",
    "            \n",
    "            # Comment in print statements to see how the episodes develop until convergence\n",
    "            #print(\"--------------\")\n",
    "            #print(f\"episode {n}\")\n",
    "            #print(\"convergence_counter\", convergence_counter)\n",
    "            #print(\"--------------\")\n",
    "            \n",
    "            if convergence_counter >= convergence_criterion:\n",
    "                break\n",
    "        else:\n",
    "            convergence_counter = 0\n",
    "\n",
    "        \n",
    "        \n",
    "    if verbose:\n",
    "        if n < n_episodes:\n",
    "            print(f\"It took {n} episodes to converge to the optimal Q-function\")\n",
    "        else:\n",
    "            print(f\"Did not converge to optimal Q-function in {n_episodes} episodes\")\n",
    "    \n",
    "    if verbose:\n",
    "        gw_env.display_q_function(q_func=ql_agent.get_Q_function())\n",
    "\n",
    "    ql_agent.construct_greedy_policy(gw_env.get_action_state_pairs)\n",
    "\n",
    "    if verbose:\n",
    "        gw_env.display_policy(policy=ql_agent.get_policy())\n",
    "\n",
    "    return ql_agent.get_policy()       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b48255",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Perform Action-Value evaluation function (Q-Learning evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0eaffca",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def perform_value_evaluation(gw_env: Grid_World, policy: Dict[Any, Any], verbose=False):\n",
    "    \n",
    "    vi_agent = ValueIterationAgent(states=gw_env.get_state_space(),\n",
    "                                   terminal_states=gw_env.get_terminal_states(),\n",
    "                                   reward_function=gw_env.get_reward_func(),\n",
    "                                   actions=gw_env.get_action_space(),\n",
    "                                   gamma=GAMMA)\n",
    "    \n",
    "    while not vi_agent.value_converged:\n",
    "        \n",
    "        for state in gw_env.get_state_space():\n",
    "            \n",
    "            if state in gw_env.get_terminal_states():\n",
    "                continue\n",
    "                \n",
    "            policy_act = policy[state]\n",
    "            next_state = gw_env.get_new_state_on_action(old_state=state, action=policy_act)\n",
    "            next_state_value = vi_agent.get_state_value(state=next_state)\n",
    "            \n",
    "            vi_agent.set_state_value(state=state, new_value=(gw_env.get_state_reward(state=next_state) + GAMMA * next_state_value))\n",
    "        \n",
    "    if verbose:\n",
    "        gw_env.display_value_function(value_func=vi_agent.get_value_function())\n",
    "\n",
    "    return vi_agent.get_value_function()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3687cb2c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### IRL Reward estimation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bd5fd97",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def irl_reward_estimation(env: Grid_World, optimal_trajectories: List[List[Any]], train_func: Callable):\n",
    "\n",
    "    # store reference reward function\n",
    "    reward_func_ref = deepcopy(env.get_board())\n",
    "    print('Reference reward function:\\n', reward_func_ref)\n",
    "\n",
    "    irl_agent = IRL_from_sampled_trajectories(d=(GW_SIZE[0] * 4, GW_SIZE[1] * 4),\n",
    "                                              env_ranges=((0, GW_SIZE[0]), (0, GW_SIZE[1])),\n",
    "                                              env_discrete_size=GW_SIZE,\n",
    "                                              penalty_factor=2,\n",
    "                                              gamma=GAMMA)\n",
    "\n",
    "    # step 2: given optimal trajectories, compute the value estimate\n",
    "    print(\"Computing value estimates for optimal trajectories...\")\n",
    "    optimal_value_estimate = irl_agent.compute_value_estimate(trajs=optimal_trajectories)\n",
    "\n",
    "    candidate_policies = [env.construct_random_policy()]\n",
    "    candidate_value_estimates = []\n",
    "    reward_func_estimates = []\n",
    "\n",
    "    # while True:\n",
    "    for i in range(IRL_TRAINING_N):\n",
    "        print(f\"Iteration {i}...\")\n",
    "\n",
    "        # step 3: generate trajectories and compute the value estimate for a random policy\n",
    "        print(\"Generating trajectories for the candidate policy...\")\n",
    "        candidate_trajectories = env.generate_trajectories(policy=candidate_policies[-1],\n",
    "                                                           n_traj=NUMBER_OF_TRAJECTORIES,\n",
    "                                                           max_traj_length=MAXIMUM_TRAJECTORY_LENGTH)\n",
    "        print(\"Computing value estimates for condidate trajectories...\")\n",
    "        candidate_value_estimates.append(irl_agent.compute_value_estimate(trajs=candidate_trajectories))\n",
    "\n",
    "        # step 4: obtain new alphas\n",
    "        print(\"Solving linear programming...\")\n",
    "        irl_agent.solve_lp(optimal_value_estimate, candidate_value_estimates)\n",
    "\n",
    "        # step 5: construct new reward function from the alphas\n",
    "        reward_func = irl_agent.construct_reward_function(alphas=irl_agent.get_alphas())\n",
    "\n",
    "        # step 6: find optimal policy under new reward function and add to 'candidate_policies' list\n",
    "        env.set_reward_func(reward_func)\n",
    "        candidate_policies.append(train_func(gw_env=env, verbose=True))  # train_value_iteration(gw_env=env))\n",
    "        # store new reward function\n",
    "        reward_func_estimates.append(env.get_board())\n",
    "        \n",
    "        print(\"Latest estimated reward function:\\n\", reward_func_estimates[-1])\n",
    "        env.display_policy(policy=candidate_policies[-1])\n",
    "        print(\"============================================================\\n\" * 2)\n",
    "\n",
    "    return {'reference_reward_func': reward_func_ref, 'policy_pred': np.mean(np.array([list(pol.values()) for pol in candidate_policies]), axis=0), 'avg_predicted_reward_func': np.mean(np.array(reward_func_estimates), axis=0)}\n",
    "\n",
    "\n",
    "def calc_value_distance(value_estimates_ref, value_estimates_pred):\n",
    "    return np.linalg.norm(np.array(value_estimates_ref) - np.array(value_estimates_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83aac645",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bayesian Inverse Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7758314",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Agenda\n",
    "\n",
    "1. Motivation\n",
    "2. Reference work\n",
    "    1. Inverse Reinforcement Learning (IRL)\n",
    "    2. Bayesian Inverse Reinforcement Learning (BIRL)\n",
    "3. Implementation\n",
    "4. Results\n",
    "5. Outlook\n",
    "    - Add Priors\n",
    "        - Gaussian, ...\n",
    "    - Add different Policies\n",
    "        - Boltzman\n",
    "        - Epsilon Greedy\n",
    "        - Greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1ee7b3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a17055",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The problem of Inverse Reinforcement Learning has been defined in Learning agents for uncertain environments (Russel, 1998).\n",
    "\n",
    "- \"**Determine**: reward function being optimized.\n",
    "- **Given**: 1) Measurements of an agent's behavior over time, in a variety of circumstances 2) Measurements of the sensory inputs to that agent; 3) a model of the environment.\" ( Ng and Russel, 2000 )\n",
    "\n",
    "Reproduce the results of the case for Bayesian Inverse Reinforcement Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04f7bfd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "motivation for this problem arises from e.g. animal and human learning.\n",
    "example: \n",
    "Bee foraging -> \n",
    "bee searches for nectar. \n",
    "reward -> nectar in flower\n",
    "possibly multiattribute: weight nectar ingestion against flight distance, time, risk from wind and predators.\n",
    "\n",
    "Conclusion: hard to determine relative weights here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395b1c6c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Bee hive\n",
    "<img src=\"presentation/Bee_Hive_IRL.png\" alt=\"bee hive\" width=\"100%\"/>\n",
    "source: self made\n",
    "\n",
    "The problem here: How do we actually properly set rewards for different attributes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d935ea0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Second idea: \n",
    "Often we want to learn a policy based on observation of an expert. However it is assumed that the reward function is known, which often is not the case. So we take the policy from the expert as given and infer the reward function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb08513",
   "metadata": {},
   "source": [
    "### Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a856064",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.95\n",
    "VALUE_ITERATION_TRAINING_N = 25\n",
    "IRL_TRAINING_N = 10\n",
    "\n",
    "NUMBER_OF_TRAJECTORIES = 400\n",
    "MAXIMUM_TRAJECTORY_LENGTH = 10\n",
    "\n",
    "GW_SIZE = (6, 6)\n",
    "GW_SIZES = [(3, 3)]  # [(x, x) for x in np.arange(5,11, 5)]\n",
    "GW_TRAPS = [(3,3)]\n",
    "GW_GOALS = [(0, 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "007283ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = Grid_World(size=GW_SIZE, traps=GW_TRAPS, goals=GW_GOALS, randomize_board=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fc91a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function:\n",
      "[[0.         1.         0.95       0.9025     0.857375   0.81450625]\n",
      " [1.         0.95       0.9025     0.857375   0.81450625 0.77378094]\n",
      " [0.95       0.9025     0.857375   0.81450625 0.77378094 0.73509189]\n",
      " [0.9025     0.857375   0.81450625 0.         0.73509189 0.6983373 ]\n",
      " [0.857375   0.81450625 0.77378094 0.73509189 0.6983373  0.66342043]\n",
      " [0.81450625 0.77378094 0.73509189 0.6983373  0.66342043 0.63024941]]\n",
      "Policy:\n",
      "[['x' '<' '<' '<' '<' '<']\n",
      " ['^' '<' '<' '<' '<' '<']\n",
      " ['^' '<' '<' '<' '<' '<']\n",
      " ['^' '<' '<' 'x' '^' '<']\n",
      " ['^' '<' '<' '<' '<' '<']\n",
      " ['^' '<' '<' '<' '<' '<']]\n"
     ]
    }
   ],
   "source": [
    "vi_greedy_policy = train_value_iteration(gw_env=environment, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e41cc76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3751979e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Inverse Reinforcement Learning (IRL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7f3c92",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Given \n",
    "- finite State space $\\mathcal{S}$\n",
    "- a set of $k$ actions $A = \\{a_1, \\cdots , a_k\\}$\n",
    "- transition probabilities $\\{P_{sa}\\}$\n",
    "- discount factor $\\gamma$ \n",
    "- policy $\\pi$ in Markov Decision Process $(S, A, \\{P_{sa}\\}, \\gamma, R)$\n",
    "\n",
    "Constraint:\n",
    "- only finite-state MDPs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dac810",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Required \"tools\"\n",
    "- value iteration\n",
    "- q learning\n",
    "-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42815f3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Linear function approximation\n",
    "\n",
    "- For a general solution of $R$ we require all function to map $\\mathbb{R}^n \\rightarrow \\mathbb{R}$\n",
    "    - Hard to work with this, therefore we choose a linear approximation\n",
    "    \n",
    "$ \\quad R(s) = \\alpha_1 \\phi_1(s) + \\alpha_2 \\phi_2(s) + \\cdots + \\alpha_d \\phi_d(s) $\n",
    "\n",
    "with the $\\phi$s being fixed basis functions mapping from state $S$ into $R$ and the $\\alpha_i$s a are the parameters that can be fit and optimized for the reward function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea87e3a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Using linearity of expectations value function $V_i^\\pi$ for the policy $\\pi$ is given by\n",
    "\n",
    "$ \\quad V^\\pi = \\alpha_1 V_1^\\pi + \\cdots + \\alpha_d V_d^\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6232d383",
   "metadata": {},
   "source": [
    "### IRL from Sampled Trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b0a18a",
   "metadata": {},
   "source": [
    "- Access to policy $\\pi$ only given by a set of trajectories.\n",
    "- Assume that optimal policy can be found by choosing a reward function\n",
    "- No explicit model of MDP required\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21215c56",
   "metadata": {},
   "source": [
    "The idea:\n",
    "- sample trajectories\n",
    "<img src=\"presentation/IRL_trajectories.png\" width=\"50%\" height=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bd10fb",
   "metadata": {},
   "source": [
    "execute $m$ Monte Carlo trajectories under $\\pi$\n",
    "Then for each $i = 1, \\cdots , d$ define $V_i^\\pi(s_0)$ to be what the average empirical return would have been on these on these $m$ trajectories if the reward $R$ had been $\\phi_i$.\n",
    "For any setting of $\\alpha_i$s the natural estimate of $\\hat{V}^\\pi(s_0)$ is:\n",
    "\n",
    "$\\hat{V}^\\pi(s_0) = \\alpha_1 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8d402f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"IRL from samples...\")\n",
    "\n",
    "# restart the environment\n",
    "environment = Grid_World(size=GW_SIZE, traps=GW_TRAPS, goals=GW_GOALS, randomize_board=False)\n",
    "\n",
    "train_func = train_value_iteration\n",
    "# train_func = train_q_learning\n",
    "\n",
    "estimated_rewards = irl_reward_estimation(env=environment, optimal_trajectories=trajectories, train_func=train_func)\n",
    "\n",
    "# ----\n",
    "#def irl_reward_estimation(env: Grid_World, optimal_trajectories: List[List[Any]], train_func: Callable):\n",
    "\n",
    "# store reference reward function\n",
    "reward_func_ref = deepcopy(env.get_board())\n",
    "print('Reference reward function:\\n', reward_func_ref)\n",
    "\n",
    "irl_agent = IRL_from_sampled_trajectories(d=(GW_SIZE[0] * 4, GW_SIZE[1] * 4),\n",
    "                                          env_ranges=((0, GW_SIZE[0]), (0, GW_SIZE[1])),\n",
    "                                          env_discrete_size=GW_SIZE,\n",
    "                                          penalty_factor=2,\n",
    "                                          gamma=GAMMA)\n",
    "\n",
    "# step 2: given optimal trajectories, compute the value estimate\n",
    "print(\"Computing value estimates for optimal trajectories...\")\n",
    "optimal_value_estimate = irl_agent.compute_value_estimate(trajs=optimal_trajectories)\n",
    "\n",
    "candidate_policies = [env.construct_random_policy()]\n",
    "candidate_value_estimates = []\n",
    "reward_func_estimates = []\n",
    "\n",
    "# while True:\n",
    "for i in range(IRL_TRAINING_N):\n",
    "    print(f\"Iteration {i}...\")\n",
    "\n",
    "    # step 3: generate trajectories and compute the value estimate for a random policy\n",
    "    print(\"Generating trajectories for the candidate policy...\")\n",
    "    candidate_trajectories = env.generate_trajectories(policy=candidate_policies[-1],\n",
    "                                                       n_traj=NUMBER_OF_TRAJECTORIES,\n",
    "                                                       max_traj_length=MAXIMUM_TRAJECTORY_LENGTH)\n",
    "    print(\"Computing value estimates for condidate trajectories...\")\n",
    "    candidate_value_estimates.append(irl_agent.compute_value_estimate(trajs=candidate_trajectories))\n",
    "\n",
    "    # step 4: obtain new alphas\n",
    "    print(\"Solving linear programming...\")\n",
    "    irl_agent.solve_lp(optimal_value_estimate, candidate_value_estimates)\n",
    "\n",
    "    # step 5: construct new reward function from the alphas\n",
    "    reward_func = irl_agent.construct_reward_function(alphas=irl_agent.get_alphas())\n",
    "\n",
    "    # step 6: find optimal policy under new reward function and add to 'candidate_policies' list\n",
    "    env.set_reward_func(reward_func)\n",
    "    candidate_policies.append(train_func(gw_env=env, verbose=True))  # train_value_iteration(gw_env=env))\n",
    "    # store new reward function\n",
    "    reward_func_estimates.append(env.get_board())\n",
    "\n",
    "    print(\"Latest estimated reward function:\\n\", reward_func_estimates[-1])\n",
    "    env.display_policy(policy=candidate_policies[-1])\n",
    "    print(\"============================================================\\n\" * 2)\n",
    "\n",
    "#return {'reference_reward_func': reward_func_ref, 'policy_pred': np.mean(np.array([list(pol.values()) for pol in candidate_policies]), axis=0), 'avg_predicted_reward_func': np.mean(np.array(reward_func_estimates), axis=0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dd22fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82359f2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df18c84d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
