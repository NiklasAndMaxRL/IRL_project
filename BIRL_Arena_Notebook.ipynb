{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b95ef11a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "from typing import Any, List, Dict, Tuple, Callable\n",
    "from functools import partial\n",
    "\n",
    "from GridWorld_environments import Grid_World\n",
    "from RL_agents import ValueIterationAgent, QLearningAgent\n",
    "from IRL_agents import IRL_from_sampled_trajectories\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bee7b6",
   "metadata": {},
   "source": [
    "### Value Function functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd1dce1",
   "metadata": {},
   "source": [
    "#### Perform Value Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ccea3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_value_evaluation(gw_env: Grid_World, policy: Dict[Any, Any], verbose=False):\n",
    "    \n",
    "    vi_agent = ValueIterationAgent(states=gw_env.get_state_space(),\n",
    "                                   terminal_states=gw_env.get_terminal_states(),\n",
    "                                   reward_function=gw_env.get_reward_func(),\n",
    "                                   actions=gw_env.get_action_space(),\n",
    "                                   gamma=GAMMA)\n",
    "    \n",
    "    while not vi_agent.value_converged:\n",
    "        \n",
    "        for state in gw_env.get_state_space():\n",
    "            \n",
    "            if state in gw_env.get_terminal_states():\n",
    "                continue\n",
    "                \n",
    "            policy_act = policy[state]\n",
    "            next_state = gw_env.get_new_state_on_action(old_state=state, action=policy_act)\n",
    "            next_state_value = vi_agent.get_state_value(state=next_state)\n",
    "            \n",
    "            vi_agent.set_state_value(state=state, new_value=(gw_env.get_state_reward(state=next_state) + GAMMA * next_state_value))\n",
    "        \n",
    "    if verbose:\n",
    "        gw_env.display_value_function(value_func=vi_agent.get_value_function())\n",
    "\n",
    "    return vi_agent.get_value_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812e149d",
   "metadata": {},
   "source": [
    "#### Train Value Iteration function (BIRL version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c7fa1e9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def train_value_iteration(gw_env: Grid_World, \n",
    "                          policy: Dict[Any, Any] = None,\n",
    "                          value_function: Dict[Any, float] = None,\n",
    "                          verbose=False\n",
    "                         ) -> Tuple[Dict[Any, Any], Dict[Any, float]]:\n",
    "    \n",
    "    vi_agent = ValueIterationAgent(states=gw_env.get_state_space(),\n",
    "                                   terminal_states=gw_env.get_terminal_states(),\n",
    "                                   reward_function=gw_env.get_reward_func(),\n",
    "                                   actions=gw_env.get_action_space(),\n",
    "                                   gamma=GAMMA)\n",
    "\n",
    "    if policy and value_function:\n",
    "        raise ValueError(\"Can't pass both policy and value_function arguments at the same time.\")\n",
    "    \n",
    "    if policy:\n",
    "        if verbose:\n",
    "            print(\"Calculating value function of the given policy via value evaluation...\")\n",
    "        vi_agent.set_value_function(new_value_function=perform_value_evaluation(gw_env, policy, verbose))\n",
    "        \n",
    "    if value_function:\n",
    "        if verbose:\n",
    "            print(\"Using the given value function...\")\n",
    "        vi_agent.set_value_function(new_value_function=value_function)\n",
    "    \n",
    "    iters = 0\n",
    "    while iters < VALUE_ITERATION_TRAINING_N and not vi_agent.value_converged:\n",
    "\n",
    "        for state in gw_env.get_state_space():\n",
    "\n",
    "            if state in gw_env.get_terminal_states():\n",
    "                continue\n",
    "\n",
    "            opt_act = vi_agent.get_optimal_action(action_state_pairs=gw_env.get_action_state_pairs(state=state))\n",
    "            next_state = gw_env.get_new_state_on_action(old_state=state, action=opt_act)\n",
    "            next_state_value = vi_agent.get_state_value(state=next_state)\n",
    "\n",
    "            vi_agent.set_state_value(state=state, new_value=(gw_env.get_state_reward(state=next_state) + GAMMA * next_state_value))\n",
    "\n",
    "        iters += 1\n",
    "\n",
    "    if verbose:\n",
    "        gw_env.display_value_function(value_func=vi_agent.get_value_function())\n",
    "\n",
    "    vi_agent.construct_greedy_policy(gw_env.get_action_state_pairs)\n",
    "\n",
    "    if verbose:\n",
    "        gw_env.display_policy(policy=vi_agent.get_policy())\n",
    "\n",
    "    return vi_agent.get_policy(), vi_agent.get_value_function()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46f255f",
   "metadata": {},
   "source": [
    "### Q Function functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac6679d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fb1b353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_q_function_evaluation(gw_env: Grid_World, policy: Dict[Any, Any], rounding: int = 3):\n",
    "\n",
    "    value_func = perform_value_evaluation(gw_env, policy)\n",
    "    rewards = gw_env.get_reward_func()\n",
    "\n",
    "    q_function = {}\n",
    "    for state in gw_env.get_state_space():\n",
    "\n",
    "        action_values = {}\n",
    "        for action in gw_env.get_action_space():\n",
    "\n",
    "            next_state = gw_env.get_new_state_on_action(old_state=state, action=action)\n",
    "            action_values[action] = np.round(rewards[next_state] + GAMMA * value_func[next_state], rounding)\n",
    "            \n",
    "        q_function[state] = action_values\n",
    "\n",
    "    return q_function\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c569f0",
   "metadata": {},
   "source": [
    "#### Train Q Learning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cbbc175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_q_learning(gw_env: Grid_World, n_episodes=1000, verbose=False, policy=\"eps_greedy\", eps=0.2, max_episode_len=100, gamma=0.95):\n",
    "    ql_agent = QLearningAgent(states=gw_env.get_state_space(),\n",
    "                              terminal_states=gw_env.get_terminal_states(),\n",
    "                              reward_function=gw_env.get_reward_func(),\n",
    "                              actions=gw_env.get_action_space(),\n",
    "                              gamma=gamma)\n",
    "    \n",
    "    # init episodes\n",
    "    episodes = []\n",
    "    \n",
    "    # Define state_space without terminal states for getting starting position\n",
    "    state_space = deepcopy(gw_env.get_state_space()) # all states\n",
    "    terminal_states = gw_env.get_terminal_states()\n",
    "    for terminal_state in terminal_states:\n",
    "        state_space.remove(terminal_state) # not non absorbing state_space\n",
    "    \n",
    "    for n in range(n_episodes):\n",
    "        \n",
    "        episode = []\n",
    "        \n",
    "        # random starting position\n",
    "        start_idx = (np.random.choice(len(state_space)))\n",
    "        start = state_space[start_idx]\n",
    "        \n",
    "        episode.append(start)\n",
    "        \n",
    "        i = 0\n",
    "        terminal = False\n",
    "        \n",
    "        while ( ( i < max_episode_len ) and ( not terminal ) ):\n",
    "            i += 1\n",
    "            \n",
    "            # Choose Action from S derived by given policy\n",
    "            if policy == \"eps_greedy\":\n",
    "                if np.random.uniform() < (1-eps):\n",
    "                    # Choose greedy action -> highest Q-Value\n",
    "                    chosen_action = ql_agent.get_greedy_action(episode[-1])\n",
    "                else:\n",
    "                    # Choose random action form action space\n",
    "                    action_space = gw_env.get_action_space()\n",
    "                    chosen_action = action_space[np.random.choice(len(action_space))]\n",
    "            \n",
    "            new_state = gw_env.get_new_state_on_action(episode[-1], chosen_action)\n",
    "            \n",
    "            # Reward is taken from Q_learning agent -> it knows the reward function from the environment\n",
    "            ql_agent.update_Q_value(episode[-1], new_state, chosen_action)\n",
    "            \n",
    "            episode.append(new_state)\n",
    "            \n",
    "            if new_state in terminal_states:\n",
    "                terminal = True\n",
    "        \n",
    "        episodes.append(episode)\n",
    "    \n",
    "    if verbose:\n",
    "        gw_env.display_q_function(q_func=ql_agent.get_Q_function())\n",
    "\n",
    "    ql_agent.construct_greedy_policy(gw_env.get_action_state_pairs)\n",
    "\n",
    "    if verbose:\n",
    "        gw_env.display_policy(policy=ql_agent.get_policy())\n",
    "\n",
    "    return ql_agent.get_policy(), ql_agent.get_Q_function()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76312398",
   "metadata": {},
   "source": [
    "### BIRL functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c2535a",
   "metadata": {},
   "source": [
    "#### Reward Space functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78773053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_reward(size: Tuple[int], r_max: float = 1.0, rounding: int = 3) -> np.ndarray:\n",
    "        \n",
    "    return np.random.uniform(low=-abs(r_max), high=abs(r_max), size=size).round(rounding)\n",
    "\n",
    "def get_reward_neighbour(reward: np.ndarray, step_size: float, r_max: float = 1.0) -> np.ndarray:\n",
    "    \n",
    "    movement = np.random.randint(low=-1, high=2, size=reward.shape) # random array of -1, 0 and +1\n",
    "    \n",
    "    return (reward + step_size * movement).clip(min=-abs(r_max), max=abs(r_max)) # new neighbour reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61a216a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8, 1. , 1. , 1. , 1. , 0.8])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_reward_neighbour(np.array([1, 1, 1, 1, 1, 1]), 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0691e806",
   "metadata": {},
   "source": [
    "#### Prior, Evidence and Posterior Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e9dfd371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def improper_prior(reward):\n",
    "    return 1\n",
    "\n",
    "def compute_log_posterior(gw_env:Grid_World,\n",
    "                          observations: List[List[Tuple[Any]]],\n",
    "                          reward: np.ndarray,\n",
    "                          policy: Dict[Any, Any],\n",
    "                          prior: Callable,\n",
    "                          alpha: float,\n",
    "                          gamma: float\n",
    "                         ) -> float:\n",
    "    \n",
    "    gw_env.set_board(new_board=reward)\n",
    "    q = perform_q_function_evaluation(gw_env=gw_env, policy=policy)\n",
    "    \n",
    "    ln_p = 0\n",
    "    \n",
    "    for observation in observations:\n",
    "        ln_p += np.sum([alpha * q[s][a] - np.log(np.sum(np.exp(alpha * np.array(list(q[s].values()))))) for s, a in observation])\n",
    "        #### Should we use np.mean() here? our obs are not all of the same length\n",
    "        # ln_p += np.mean([alpha * q[s][a] - np.log(np.sum(np.exp(alpha * np.array(list(q[s].values()))))) for s, a in observation])\n",
    "    \n",
    "    ln_p += np.log(prior(reward))\n",
    "    \n",
    "    return ln_p\n",
    "\n",
    "def posterior_dist_ratio(gw_env: Grid_World,\n",
    "                         observations: List[List[Tuple[Any]]],\n",
    "                         reward_next: np.ndarray,\n",
    "                         policy_next: Dict[Any, Any],\n",
    "                         reward_curr: np.ndarray,\n",
    "                         policy_curr: Dict[Any, Any]\n",
    "                        ) -> float:\n",
    "    \n",
    "    prior = improper_prior\n",
    "    alpha = 10\n",
    "    gamma = GAMMA\n",
    "    \n",
    "    log_post_next = compute_log_posterior(gw_env, observations, reward_next, policy_next, prior, alpha, gamma)\n",
    "    log_post_curr = compute_log_posterior(gw_env, observations, reward_curr, policy_curr, prior, alpha, gamma)\n",
    "    \n",
    "    return np.exp(log_post_next - log_post_curr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950b1b6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d336096",
   "metadata": {},
   "source": [
    "#### PolicyWalk sampling algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e3f5d4",
   "metadata": {},
   "source": [
    "Credits for helping to decypher the confusing formulation of the paper go to...\n",
    "\n",
    "- https://github.com/uidilr/bayesian_irl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "66b19639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policyWalk(observations: List[List[Tuple[Any]]],\n",
    "               gw_env: Grid_World,\n",
    "               step_size: float,\n",
    "               n_steps: int,\n",
    "               samples_burn_in: int = 500,\n",
    "               samples_n_out: int = 5\n",
    "              ) -> np.ndarray:\n",
    "    \n",
    "    # step 1: pick a random reward from the reward space (as np.array)\n",
    "    reward_0 = get_random_reward(size=gw_env.get_board().shape)\n",
    "    \n",
    "    # step 2: get optimal policy given reward_0\n",
    "    gw_env.set_board(new_board=reward_0)  # board is the np.array representation of the reward function\n",
    "    policy_0, val_func_0 = train_value_iteration(gw_env=gw_env)\n",
    "    \n",
    "    posterior_samples = []\n",
    "    \n",
    "    # step 3: explore the reward space and sample from the posterior (GridWalk as a MCMC (Metropolis algorithm))\n",
    "    for i in range(n_steps):\n",
    "        # step 3a: pick reward_1 from reward_0 neighbours randomly (step_size distance)\n",
    "        reward_1 = get_reward_neighbour(reward=reward_0, step_size=step_size)\n",
    "        \n",
    "        # step 3b: compute the Q function of reward_1 under policy_0\n",
    "        gw_env.set_board(new_board=reward_1)\n",
    "        q_func = perform_q_function_evaluation(gw_env=gw_env,\n",
    "                                               policy=policy_0)\n",
    "        \n",
    "        # step 3c: check if ... \n",
    "        if is_policy_not_optimal(q_func=q_func, policy=policy_0,\n",
    "                                 states=gw_env.get_state_space(), actions=gw_env.get_action_space()):\n",
    "            # step 3c.i: get optimal policy given reward_1 AND policy_0 (we can use the value function of policy_0 directly)\n",
    "            #### MOTIVATION: since reward_1 is a neighbour of reward_0, policy_1 will be close to policy_0. We use this fact to speed up the computation\n",
    "            #### would be a repetition of step 3b: gw_env.set_board(new_board=reward_1)\n",
    "            policy_1, val_func_1 = train_value_iteration(gw_env=gw_env,\n",
    "                                                         value_function=val_func_0)\n",
    "            \n",
    "            # step 3c.ii: update reward_0 AND policy_0 with prob. according to the posterior_dist\n",
    "            if np.random.uniform() < np.minimum(1, posterior_dist_ratio(gw_env, observations, reward_1, policy_1, reward_0, policy_0)):\n",
    "                reward_0 = deepcopy(reward_1)\n",
    "                policy_0 = deepcopy(policy_1)\n",
    "                val_func_0 = deepcopy(val_func_1)\n",
    "            \n",
    "        else:\n",
    "            # step 3c.iii: update reward_0 with prob. according to the posterior_dist\n",
    "            if np.random.uniform() < np.minimum(1, posterior_dist_ratio(gw_env, observations, reward_1, policy_0, reward_0, policy_0)):\n",
    "                reward_0 = deepcopy(reward_1)\n",
    "                \n",
    "        if (i > samples_burn_in) and (samples_n_out != 0) and ((i - samples_burn_in) % samples_n_out == 0):\n",
    "            posterior_samples.append(reward_0)\n",
    "\n",
    "    # step 4: return a sample from the posterior\n",
    "    if samples_n_out != 0:\n",
    "        return posterior_samples\n",
    "    else:\n",
    "        return reward_0\n",
    "\n",
    "def is_policy_not_optimal(q_func, policy, states, actions):\n",
    "    for state in states:\n",
    "        for action in actions:\n",
    "            if q_func[state][policy[state]] < q_func[state][action]:\n",
    "                return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46913987",
   "metadata": {},
   "source": [
    "### Step-by-step algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "138dbca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.95\n",
    "\n",
    "VALUE_ITERATION_TRAINING_N = 50\n",
    "\n",
    "NUMBER_OF_TRAJECTORIES = 25\n",
    "MAXIMUM_TRAJECTORY_LENGTH = 25\n",
    "\n",
    "BIRL_N_OF_POSTERIOR_SAMPLES = 20\n",
    "BIRL_POLICYWALK_N_STEPS = 1000\n",
    "BIRL_POLICYWALK_STEP_SIZE = 0.05\n",
    "\n",
    "GW_SIZE = (3, 3)\n",
    "GW_SIZES = [(3, 3)]  # [(x, x) for x in np.arange(5,11, 5)]\n",
    "GW_TRAPS = []\n",
    "GW_GOALS = [(0, 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "90070f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = Grid_World(size=GW_SIZE, traps=GW_TRAPS, goals=GW_GOALS, randomize_board=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "176c8196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function:\n",
      "[[0.       1.       0.95    ]\n",
      " [1.       0.95     0.9025  ]\n",
      " [0.95     0.9025   0.857375]]\n",
      "Policy:\n",
      "[['x' '<' '<']\n",
      " ['^' '<' '<']\n",
      " ['^' '<' '<']]\n"
     ]
    }
   ],
   "source": [
    "vi_greedy_policy = train_value_iteration(gw_env=environment, verbose=True)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "39c5b1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 25 trajectories...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Generating {NUMBER_OF_TRAJECTORIES} trajectories...\")\n",
    "\n",
    "trajectories = environment.generate_trajectories(policy=vi_greedy_policy,\n",
    "                                                 n_traj=NUMBER_OF_TRAJECTORIES,\n",
    "                                                 max_traj_length=MAXIMUM_TRAJECTORY_LENGTH,\n",
    "                                                 return_state_action_pairs=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e877c596",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate 20 samples from the posterior\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [03:35<00:00, 10.80s/it]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Generate {BIRL_N_OF_POSTERIOR_SAMPLES} samples from the posterior\")\n",
    "\n",
    "posterior_samples = []\n",
    "\n",
    "for _ in tqdm(range(BIRL_N_OF_POSTERIOR_SAMPLES)):\n",
    "    \n",
    "    # restart the environment\n",
    "    environment = Grid_World(size=GW_SIZE, traps=GW_TRAPS, goals=GW_GOALS, randomize_board=False)\n",
    "\n",
    "    posterior_samples.append(policyWalk(observations=trajectories,\n",
    "                                        gw_env=environment,\n",
    "                                        step_size=BIRL_POLICYWALK_STEP_SIZE,\n",
    "                                        n_steps=BIRL_POLICYWALK_N_STEPS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "29085995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment = Grid_World(size=GW_SIZE, traps=GW_TRAPS, goals=GW_GOALS, randomize_board=False)\n",
    "environment.get_board()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7b472635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.2561 , -0.7232 , -0.68855],\n",
       "       [ 0.1192 , -0.86   , -0.73585],\n",
       "       [-0.47385, -0.6015 , -0.5952 ]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(posterior_samples, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ab180319",
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "integer division or modulo by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [63]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;241;43m10006\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: integer division or modulo by zero"
     ]
    }
   ],
   "source": [
    "10006 % 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73dcf76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
